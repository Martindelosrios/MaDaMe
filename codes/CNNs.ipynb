{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f59d6d37-d694-4143-9c99-25c9179e38e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8da5432-b9a6-48ed-bf54-d656ce188ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from scipy import stats\n",
    "import seaborn as sbn\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4685aa6-ac82-44a3-9182-499722c3130d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 15:49:26.879391: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Lambda, Input, Dense, Dropout, AveragePooling2D, Flatten, Conv2D\n",
    "from keras.layers import MaxPooling2D, BatchNormalization, Activation\n",
    "from keras.models import Model, Sequential\n",
    "from keras.losses import mse, binary_crossentropy, mean_absolute_percentage_error\n",
    "from keras.utils import plot_model,to_categorical\n",
    "from keras import backend as K\n",
    "from keras import callbacks\n",
    "#from sklearn.preprocessing import LabelEncoder,OneHotEncoder,image,label_binarize\n",
    "from keras.utils import plot_model\n",
    "from keras.optimizers import SGD, Adam\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
    "from sklearn.metrics import roc_curve,roc_auc_score,mean_squared_error\n",
    "#from keras.applications.resnet50 import ResNet50\n",
    "import tensorflow as tf\n",
    "import keras.losses as kLoss\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c4a0878-612f-4379-92cb-dbab5c724eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version: 1.22.4\n",
      "matplotlib version: 3.5.2\n",
      "tensorflow version: 2.9.1\n",
      "WARNING:tensorflow:From /tmp/ipykernel_1432502/2276095913.py:11: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "Using GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 15:49:36.475514: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-17 15:49:36.612947: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-02-17 15:49:36.819681: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-02-17 15:49:36.820155: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-02-17 15:49:39.001219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-02-17 15:49:39.001470: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-02-17 15:49:39.001654: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-02-17 15:49:39.001807: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /device:GPU:0 with 5914 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "print('numpy version:', np.__version__)\n",
    "print('matplotlib version:', mpl.__version__)\n",
    "print('tensorflow version:', tf.__version__)\n",
    "\n",
    "color_rate = \"#d55e00\"\n",
    "color_drate = 'darkblue' #\"#0072b2\"\n",
    "color_s1s2 = 'limegreen' #\"#009e73\"\n",
    "color_comb = 'limegreen'\n",
    "\n",
    "# Check if gpu is available\n",
    "if tf.test.is_gpu_available():\n",
    "    device = 'gpu'\n",
    "    print('Using GPU')\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print('Using CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f239547f-cb41-447b-ab87-4cbb0120a747",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7f80d09-00f5-466f-8b96-a0120c473520",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.load('../data/TNGgalaxies/images.npy')\n",
    "labels = np.load('../data/TNGgalaxies/labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a919a0b7-6d01-4829-a3ae-4b4d4837c8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4444 nans in images\n",
      "There are 0 nans in labels\n"
     ]
    }
   ],
   "source": [
    "print(f'There are {len(np.where(np.isnan(images) == True)[0])} nans in images')\n",
    "print(f'There are {len(np.where(np.isnan(labels) == True)[0])} nans in labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98f79a57-da09-4c34-820e-f679a7f2753c",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.nan_to_num(images, nan=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e158aa09-d9ab-4d90-bda4-581e3b444200",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.moveaxis(images, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b35b5e0-ebf4-4002-bc1c-8e2a0636f172",
   "metadata": {},
   "outputs": [],
   "source": [
    "nobs, npix, _, nch = images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf013a1f-bd8f-4045-9eb1-f6c170fced33",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(28890)\n",
    "random_ind = np.random.choice(np.arange(nobs), size = nobs, replace = False)\n",
    "\n",
    "train_size = int(0.7 * nobs)\n",
    "val_size = int(0.2 * nobs)\n",
    "test_size = nobs - train_size - val_size\n",
    "\n",
    "train_ind = random_ind[:train_size]\n",
    "val_ind = random_ind[train_size:(val_size + train_size)]\n",
    "test_ind = random_ind[-test_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f58b2156-5630-438c-a125-9b2e4857ab3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trainset = images[train_ind,:,:,:]\n",
    "x_valset = images[val_ind,:,:,:]\n",
    "x_testset = images[test_ind,:,:,:]\n",
    "\n",
    "y_trainset = np.log10(labels[train_ind,0,:])\n",
    "y_valset = np.log10(labels[val_ind,0,:])\n",
    "y_testset = np.log10(labels[test_ind,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "daf152df-634c-4ae2-ba48-d7f927e436b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_x = np.min(x_trainset)\n",
    "max_x = np.max(x_trainset)\n",
    "\n",
    "min_y = np.min(y_trainset, axis = 0)\n",
    "max_y = np.max(y_trainset, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19d6627e-ebdf-48c0-8a42-1dd2d53beeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trainset = (x_trainset - min_x) / (max_x - min_x)\n",
    "x_valset = (x_valset - min_x) / (max_x - min_x)\n",
    "x_testset = (x_testset - min_x) / (max_x - min_x)\n",
    "\n",
    "y_trainset = (y_trainset - min_y) / (max_y - min_y)\n",
    "y_valset = (y_valset - min_y) / (max_y - min_y)\n",
    "y_testset = (y_testset - min_y) / (max_y - min_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1274844f-e6ca-4f3d-b4f0-814ebf4cfaaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1218547.7897511267"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2129415b-4e7c-45d1-9442-9a9db3bdf7d7",
   "metadata": {},
   "source": [
    "# Playing with CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "061887a5-e9e4-4175-9b2c-275f944f6562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# network parameters\n",
    "input_shape = (npix, npix, nch) # Input shape (#rows, #cols, #channels)\n",
    "actFunction = 'relu'\n",
    "\n",
    "# Hidden layers dimensions\n",
    "intermediate_dim1 = 500\n",
    "intermediate_dim2 = 400\n",
    "intermediate_dim3 = 300\n",
    "intermediate_dim4 = 200\n",
    "output_dim        = y_trainset.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6bc81c6-17cf-4775-9414-84ef47889fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 128, 128, 5)       1005      \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 128, 128, 5)      20        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " activation (Activation)     (None, 128, 128, 5)       0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 64, 64, 5)        0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 64, 64, 15)        1890      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 64, 64, 15)       60        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 64, 64, 15)        0         \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 32, 32, 15)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 32, 32, 30)        11280     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 32, 32, 30)       120       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 32, 32, 30)        0         \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 16, 16, 30)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 7680)              0         \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 7680)             30720     \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense (Dense)               (None, 500)               3840500   \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 500)              2000      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 400)               200400    \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 400)              1600      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 300)               120300    \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 300)              1200      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 200)               60200     \n",
      "                                                                 \n",
      " output (Dense)              (None, 20)                4020      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,275,315\n",
      "Trainable params: 4,257,455\n",
      "Non-trainable params: 17,860\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 15:50:45.533427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-02-17 15:50:45.533977: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-02-17 15:50:45.534363: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-02-17 15:50:45.535308: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-02-17 15:50:45.535717: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-02-17 15:50:45.536096: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-02-17 15:50:45.536568: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-02-17 15:50:45.536944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-02-17 15:50:45.537205: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5914 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# build model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters = 5, kernel_size = (5,5), strides = 1 , padding = 'same', input_shape = input_shape))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(actFunction))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(filters = 15, kernel_size = (5,5), strides = 1 , padding = 'same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(actFunction))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(filters = 30, kernel_size = (5,5), strides = 1 , padding = 'same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(actFunction))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#model.add(Dropout(0.2)) \n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(intermediate_dim1, activation = actFunction))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(intermediate_dim2, activation = actFunction))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(intermediate_dim3, activation = actFunction))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(intermediate_dim4, activation = 'linear'))\n",
    "\n",
    "model.add(Dense(output_dim, name = 'output'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4b3944-c7c9-44e0-ac62-0ca503fd177d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gen = ImageDataGenerator(rotation_range = 90) # Degree range for random rotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33b9f1e4-4422-48c8-a20f-4faedc0fff5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model\n",
    "optimizer = optimizers.Adam(learning_rate = 1e-5, beta_1 = 0.9, beta_2 = 0.999, amsgrad = False)\n",
    "model.compile(optimizer = optimizer, loss = 'mse', metrics=['mae','mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f62c5b8e-1f39-4d9c-9af1-fd07eee0c2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This are some callbacks to take control of the NN\n",
    "#es = callbacks.EarlyStopping(monitor = 'val_loss', mode = min, verbose = 1, patience = 100) # Stop after \"patience\" number of steps without improving\n",
    "mc = callbacks.ModelCheckpoint('../data/best_model.h5', monitor ='val_loss', mode = 'min', verbose = 1, save_best_only = True) # Save only the best epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd892299-f6cd-410e-9579-e1ef168d5639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2972, 128, 128, 8)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d29b91b-a0dd-42d4-8d82-7df14fbb1420",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(images[20:21,:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3a4c7052-98c8-43ee-939f-7cfa6802af34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1/1 [==============================] - ETA: 0s - loss: nan - mae: nan - mse: nan\n",
      "Epoch 1: val_loss did not improve from inf\n",
      "1/1 [==============================] - 1s 685ms/step - loss: nan - mae: nan - mse: nan - val_loss: nan - val_mae: nan - val_mse: nan\n",
      "Epoch 2/25\n",
      "1/1 [==============================] - ETA: 0s - loss: nan - mae: nan - mse: nan\n",
      "Epoch 2: val_loss did not improve from inf\n",
      "1/1 [==============================] - 0s 24ms/step - loss: nan - mae: nan - mse: nan - val_loss: nan - val_mae: nan - val_mse: nan\n",
      "Epoch 3/25\n",
      "1/1 [==============================] - ETA: 0s - loss: nan - mae: nan - mse: nan\n",
      "Epoch 3: val_loss did not improve from inf\n",
      "1/1 [==============================] - 0s 23ms/step - loss: nan - mae: nan - mse: nan - val_loss: nan - val_mae: nan - val_mse: nan\n",
      "Epoch 4/25\n",
      "1/1 [==============================] - ETA: 0s - loss: nan - mae: nan - mse: nan\n",
      "Epoch 4: val_loss did not improve from inf\n",
      "1/1 [==============================] - 0s 25ms/step - loss: nan - mae: nan - mse: nan - val_loss: nan - val_mae: nan - val_mse: nan\n",
      "Epoch 5/25\n",
      "1/1 [==============================] - ETA: 0s - loss: nan - mae: nan - mse: nan\n",
      "Epoch 5: val_loss did not improve from inf\n",
      "1/1 [==============================] - 0s 25ms/step - loss: nan - mae: nan - mse: nan - val_loss: nan - val_mae: nan - val_mse: nan\n",
      "Epoch 6/25\n",
      "1/1 [==============================] - ETA: 0s - loss: nan - mae: nan - mse: nan\n",
      "Epoch 6: val_loss did not improve from inf\n",
      "1/1 [==============================] - 0s 25ms/step - loss: nan - mae: nan - mse: nan - val_loss: nan - val_mae: nan - val_mse: nan\n",
      "Epoch 7/25\n",
      "1/1 [==============================] - ETA: 0s - loss: nan - mae: nan - mse: nan\n",
      "Epoch 7: val_loss did not improve from inf\n",
      "1/1 [==============================] - 0s 24ms/step - loss: nan - mae: nan - mse: nan - val_loss: nan - val_mae: nan - val_mse: nan\n",
      "Epoch 8/25\n",
      "1/1 [==============================] - ETA: 0s - loss: nan - mae: nan - mse: nan\n",
      "Epoch 8: val_loss did not improve from inf\n",
      "1/1 [==============================] - 0s 24ms/step - loss: nan - mae: nan - mse: nan - val_loss: nan - val_mae: nan - val_mse: nan\n",
      "Epoch 9/25\n",
      "1/1 [==============================] - ETA: 0s - loss: nan - mae: nan - mse: nan\n",
      "Epoch 9: val_loss did not improve from inf\n",
      "1/1 [==============================] - 0s 25ms/step - loss: nan - mae: nan - mse: nan - val_loss: nan - val_mae: nan - val_mse: nan\n",
      "Epoch 10/25\n",
      "1/1 [==============================] - ETA: 0s - loss: nan - mae: nan - mse: nan\n",
      "Epoch 10: val_loss did not improve from inf\n",
      "1/1 [==============================] - 0s 25ms/step - loss: nan - mae: nan - mse: nan - val_loss: nan - val_mae: nan - val_mse: nan\n",
      "Epoch 11/25\n",
      "1/1 [==============================] - ETA: 0s - loss: nan - mae: nan - mse: nan\n",
      "Epoch 11: val_loss did not improve from inf\n",
      "1/1 [==============================] - 0s 24ms/step - loss: nan - mae: nan - mse: nan - val_loss: nan - val_mae: nan - val_mse: nan\n",
      "Epoch 12/25\n",
      "1/1 [==============================] - ETA: 0s - loss: nan - mae: nan - mse: nan\n",
      "Epoch 12: val_loss did not improve from inf\n",
      "1/1 [==============================] - 0s 23ms/step - loss: nan - mae: nan - mse: nan - val_loss: nan - val_mae: nan - val_mse: nan\n",
      "Epoch 13/25\n",
      "1/1 [==============================] - ETA: 0s - loss: nan - mae: nan - mse: nan\n",
      "Epoch 13: val_loss did not improve from inf\n",
      "1/1 [==============================] - 0s 24ms/step - loss: nan - mae: nan - mse: nan - val_loss: nan - val_mae: nan - val_mse: nan\n",
      "Epoch 14/25\n",
      "1/1 [==============================] - ETA: 0s - loss: nan - mae: nan - mse: nan\n",
      "Epoch 14: val_loss did not improve from inf\n",
      "1/1 [==============================] - 0s 24ms/step - loss: nan - mae: nan - mse: nan - val_loss: nan - val_mae: nan - val_mse: nan\n",
      "Epoch 15/25\n",
      "1/1 [==============================] - ETA: 0s - loss: nan - mae: nan - mse: nan\n",
      "Epoch 15: val_loss did not improve from inf\n",
      "1/1 [==============================] - 0s 24ms/step - loss: nan - mae: nan - mse: nan - val_loss: nan - val_mae: nan - val_mse: nan\n",
      "Epoch 16/25\n",
      "1/1 [==============================] - ETA: 0s - loss: nan - mae: nan - mse: nan\n",
      "Epoch 16: val_loss did not improve from inf\n",
      "1/1 [==============================] - 0s 23ms/step - loss: nan - mae: nan - mse: nan - val_loss: nan - val_mae: nan - val_mse: nan\n",
      "Epoch 17/25\n",
      "1/1 [==============================] - ETA: 0s - loss: nan - mae: nan - mse: nan\n",
      "Epoch 17: val_loss did not improve from inf\n",
      "1/1 [==============================] - 0s 25ms/step - loss: nan - mae: nan - mse: nan - val_loss: nan - val_mae: nan - val_mse: nan\n",
      "Epoch 18/25\n",
      "1/1 [==============================] - ETA: 0s - loss: nan - mae: nan - mse: nan\n",
      "Epoch 18: val_loss did not improve from inf\n",
      "1/1 [==============================] - 0s 23ms/step - loss: nan - mae: nan - mse: nan - val_loss: nan - val_mae: nan - val_mse: nan\n",
      "Epoch 19/25\n",
      "1/1 [==============================] - ETA: 0s - loss: nan - mae: nan - mse: nan\n",
      "Epoch 19: val_loss did not improve from inf\n",
      "1/1 [==============================] - 0s 23ms/step - loss: nan - mae: nan - mse: nan - val_loss: nan - val_mae: nan - val_mse: nan\n",
      "Epoch 20/25\n",
      "1/1 [==============================] - ETA: 0s - loss: nan - mae: nan - mse: nan\n",
      "Epoch 20: val_loss did not improve from inf\n",
      "1/1 [==============================] - 0s 24ms/step - loss: nan - mae: nan - mse: nan - val_loss: nan - val_mae: nan - val_mse: nan\n",
      "Epoch 21/25\n",
      "1/1 [==============================] - ETA: 0s - loss: nan - mae: nan - mse: nan\n",
      "Epoch 21: val_loss did not improve from inf\n",
      "1/1 [==============================] - 0s 24ms/step - loss: nan - mae: nan - mse: nan - val_loss: nan - val_mae: nan - val_mse: nan\n",
      "Epoch 22/25\n",
      "1/1 [==============================] - ETA: 0s - loss: nan - mae: nan - mse: nan\n",
      "Epoch 22: val_loss did not improve from inf\n",
      "1/1 [==============================] - 0s 24ms/step - loss: nan - mae: nan - mse: nan - val_loss: nan - val_mae: nan - val_mse: nan\n",
      "Epoch 23/25\n",
      "1/1 [==============================] - ETA: 0s - loss: nan - mae: nan - mse: nan\n",
      "Epoch 23: val_loss did not improve from inf\n",
      "1/1 [==============================] - 0s 23ms/step - loss: nan - mae: nan - mse: nan - val_loss: nan - val_mae: nan - val_mse: nan\n",
      "Epoch 24/25\n",
      "1/1 [==============================] - ETA: 0s - loss: nan - mae: nan - mse: nan\n",
      "Epoch 24: val_loss did not improve from inf\n",
      "1/1 [==============================] - 0s 23ms/step - loss: nan - mae: nan - mse: nan - val_loss: nan - val_mae: nan - val_mse: nan\n",
      "Epoch 25/25\n",
      "1/1 [==============================] - ETA: 0s - loss: nan - mae: nan - mse: nan\n",
      "Epoch 25: val_loss did not improve from inf\n",
      "1/1 [==============================] - 0s 24ms/step - loss: nan - mae: nan - mse: nan - val_loss: nan - val_mae: nan - val_mse: nan\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs     = 25\n",
    "history = model.fit(x_trainset[:batch_size], y_trainset[:batch_size],\n",
    "                   epochs = epochs,\n",
    "                   callbacks = [mc],\n",
    "                   validation_data = (x_trainset[:batch_size], y_trainset[:batch_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756035ff-ea8d-48e4-813a-2528bcabaac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs     = 250\n",
    "#ran_ind    = np.random.choice(range(len(x_trainset)), size = len(x_trainset), replace = True)\n",
    "#generator  = gen.flow(x_trainset[ran_ind,:,:,:], y_trainset[ran_ind,:], batch_size = batch_size)\n",
    "generator  = gen.flow(x_trainset, y_trainset, batch_size = batch_size)\n",
    "# change epochs\n",
    "history = model.fit_generator(\n",
    "                  generator,\n",
    "                  steps_per_epoch  = len(x_trainset)/batch_size,\n",
    "                  epochs           = epochs,\n",
    "                  verbose          = 1,\n",
    "                  validation_data  = (x_valset, y_valset),\n",
    "                  validation_steps = len(x_valset)/batch_size,\n",
    "                  callbacks        = [es, mc])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
