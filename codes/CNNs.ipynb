{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f59d6d37-d694-4143-9c99-25c9179e38e0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8da5432-b9a6-48ed-bf54-d656ce188ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from scipy import stats\n",
    "import seaborn as sbn\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4685aa6-ac82-44a3-9182-499722c3130d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 13:00:23.671018: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-02-28 13:00:23.743534: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-02-28 13:00:23.743979: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-28 13:00:24.508794: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Lambda, Input, Dense, Dropout, AveragePooling2D, Flatten, Conv2D\n",
    "from keras.layers import MaxPooling2D, BatchNormalization, Activation\n",
    "from keras.models import Model, Sequential\n",
    "from keras.losses import mse, binary_crossentropy, mean_absolute_percentage_error\n",
    "from keras.utils import plot_model,to_categorical\n",
    "from keras import backend as K\n",
    "from keras import callbacks\n",
    "#from sklearn.preprocessing import LabelEncoder,OneHotEncoder,image,label_binarize\n",
    "from keras.utils import plot_model\n",
    "from keras.optimizers import SGD, Adam\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
    "from sklearn.metrics import roc_curve,roc_auc_score,mean_squared_error\n",
    "#from keras.applications.resnet50 import ResNet50\n",
    "import tensorflow as tf\n",
    "import keras.losses as kLoss\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c4a0878-612f-4379-92cb-dbab5c724eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version: 1.24.4\n",
      "matplotlib version: 3.4.3\n",
      "tensorflow version: 2.13.1\n",
      "Using CPU\n"
     ]
    }
   ],
   "source": [
    "print('numpy version:', np.__version__)\n",
    "print('matplotlib version:', mpl.__version__)\n",
    "print('tensorflow version:', tf.__version__)\n",
    "\n",
    "color_rate = \"#d55e00\"\n",
    "color_drate = 'darkblue' #\"#0072b2\"\n",
    "color_s1s2 = 'limegreen' #\"#009e73\"\n",
    "color_comb = 'limegreen'\n",
    "\n",
    "# Check if gpu is available\n",
    "if tf.test.is_gpu_available():\n",
    "    device = 'gpu'\n",
    "    print('Using GPU')\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print('Using CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f239547f-cb41-447b-ab87-4cbb0120a747",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7f80d09-00f5-466f-8b96-a0120c473520",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.load('../data/TNGgalaxies_28_02/images.npy')\n",
    "labels = np.load('../data/TNGgalaxies_28_02/labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a919a0b7-6d01-4829-a3ae-4b4d4837c8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3086 nans in images\n",
      "There are 0 nans in labels\n"
     ]
    }
   ],
   "source": [
    "print(f'There are {len(np.where(np.isnan(images) == True)[0])} nans in images')\n",
    "print(f'There are {len(np.where(np.isnan(labels) == True)[0])} nans in labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98f79a57-da09-4c34-820e-f679a7f2753c",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.nan_to_num(images, nan=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e158aa09-d9ab-4d90-bda4-581e3b444200",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.moveaxis(images, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76d153fe-55bb-40e8-965e-7d8859d81d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = images[:100,:,:,:]\n",
    "labels = labels[:100,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b35b5e0-ebf4-4002-bc1c-8e2a0636f172",
   "metadata": {},
   "outputs": [],
   "source": [
    "nobs, npix, _, nch = images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf013a1f-bd8f-4045-9eb1-f6c170fced33",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(28890)\n",
    "random_ind = np.random.choice(np.arange(nobs), size = nobs, replace = False)\n",
    "\n",
    "train_size = int(0.7 * nobs)\n",
    "val_size = int(0.2 * nobs)\n",
    "test_size = nobs - train_size - val_size\n",
    "\n",
    "train_ind = random_ind[:train_size]\n",
    "val_ind = random_ind[train_size:(val_size + train_size)]\n",
    "test_ind = random_ind[-test_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f58b2156-5630-438c-a125-9b2e4857ab3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trainset = images[train_ind,:,:,:]\n",
    "x_valset = images[val_ind,:,:,:]\n",
    "x_testset = images[test_ind,:,:,:]\n",
    "\n",
    "y_trainset = np.log10(labels[train_ind,0,:])\n",
    "y_valset = np.log10(labels[val_ind,0,:])\n",
    "y_testset = np.log10(labels[test_ind,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "daf152df-634c-4ae2-ba48-d7f927e436b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_x = np.min(x_trainset)\n",
    "max_x = np.max(x_trainset)\n",
    "\n",
    "min_y = np.min(y_trainset, axis = 0)\n",
    "max_y = np.max(y_trainset, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19d6627e-ebdf-48c0-8a42-1dd2d53beeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trainset = (x_trainset - min_x) / (max_x - min_x)\n",
    "x_valset = (x_valset - min_x) / (max_x - min_x)\n",
    "x_testset = (x_testset - min_x) / (max_x - min_x)\n",
    "\n",
    "y_trainset = (y_trainset - min_y) / (max_y - min_y)\n",
    "y_valset = (y_valset - min_y) / (max_y - min_y)\n",
    "y_testset = (y_testset - min_y) / (max_y - min_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1274844f-e6ca-4f3d-b4f0-814ebf4cfaaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-73866.63006661664"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2129415b-4e7c-45d1-9442-9a9db3bdf7d7",
   "metadata": {},
   "source": [
    "# Playing with CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "061887a5-e9e4-4175-9b2c-275f944f6562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# network parameters\n",
    "input_shape = (npix, npix, nch) # Input shape (#rows, #cols, #channels)\n",
    "actFunction = 'relu'\n",
    "\n",
    "# Hidden layers dimensions\n",
    "intermediate_dim1 = 500\n",
    "intermediate_dim2 = 400\n",
    "intermediate_dim3 = 300\n",
    "intermediate_dim4 = 200\n",
    "output_dim        = y_trainset.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9681e0-8380-4670-b860-28aea01ffb65",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## First let's check that it overfit 1 batch\n",
    "\n",
    "* It seems to converge to MSE = 0.0451 (without batch normalization nor dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e6bc81c6-17cf-4775-9414-84ef47889fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_18 (Conv2D)          (None, 128, 128, 5)       1005      \n",
      "                                                                 \n",
      " batch_normalization_7 (Bat  (None, 128, 128, 5)       20        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_18 (Activation)  (None, 128, 128, 5)       0         \n",
      "                                                                 \n",
      " max_pooling2d_18 (MaxPooli  (None, 64, 64, 5)         0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_19 (Conv2D)          (None, 64, 64, 15)        1890      \n",
      "                                                                 \n",
      " batch_normalization_8 (Bat  (None, 64, 64, 15)        60        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_19 (Activation)  (None, 64, 64, 15)        0         \n",
      "                                                                 \n",
      " max_pooling2d_19 (MaxPooli  (None, 32, 32, 15)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_20 (Conv2D)          (None, 32, 32, 30)        11280     \n",
      "                                                                 \n",
      " batch_normalization_9 (Bat  (None, 32, 32, 30)        120       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_20 (Activation)  (None, 32, 32, 30)        0         \n",
      "                                                                 \n",
      " max_pooling2d_20 (MaxPooli  (None, 16, 16, 30)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16, 16, 30)        0         \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (None, 7680)              0         \n",
      "                                                                 \n",
      " batch_normalization_10 (Ba  (None, 7680)              30720     \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 500)               3840500   \n",
      "                                                                 \n",
      " batch_normalization_11 (Ba  (None, 500)               2000      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 400)               200400    \n",
      "                                                                 \n",
      " batch_normalization_12 (Ba  (None, 400)               1600      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 300)               120300    \n",
      "                                                                 \n",
      " batch_normalization_13 (Ba  (None, 300)               1200      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 200)               60200     \n",
      "                                                                 \n",
      " output (Dense)              (None, 20)                4020      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4275315 (16.31 MB)\n",
      "Trainable params: 4257455 (16.24 MB)\n",
      "Non-trainable params: 17860 (69.77 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters = 5, kernel_size = (5,5), strides = 1 , padding = 'same', input_shape = input_shape))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Activation(actFunction))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(filters = 15, kernel_size = (5,5), strides = 1 , padding = 'same'))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Activation(actFunction))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(filters = 30, kernel_size = (5,5), strides = 1 , padding = 'same'))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Activation(actFunction))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#model.add(Dropout(0.2)) \n",
    "model.add(Flatten())\n",
    "\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dense(intermediate_dim1, activation = actFunction))\n",
    "\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dense(intermediate_dim2, activation = actFunction))\n",
    "\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dense(intermediate_dim3, activation = actFunction))\n",
    "\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dense(intermediate_dim4, activation = 'linear'))\n",
    "\n",
    "model.add(Dense(output_dim, name = 'output'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "33b9f1e4-4422-48c8-a20f-4faedc0fff5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model\n",
    "optimizer = optimizers.Adam(learning_rate = 1e-5, beta_1 = 0.9, beta_2 = 0.999, amsgrad = False)\n",
    "model.compile(optimizer = optimizer, loss = 'mse', metrics=['mae','mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3a4c7052-98c8-43ee-939f-7cfa6802af34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.3322 - mae: 0.5282 - mse: 0.3322 - val_loss: 0.3210 - val_mae: 0.5184 - val_mse: 0.3210\n",
      "Epoch 2/250\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.3210 - mae: 0.5184 - mse: 0.3210 - val_loss: 0.3104 - val_mae: 0.5089 - val_mse: 0.3104\n",
      "Epoch 3/250\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.3104 - mae: 0.5089 - mse: 0.3104 - val_loss: 0.3009 - val_mae: 0.5002 - val_mse: 0.3009\n",
      "Epoch 4/250\n",
      "1/1 [==============================] - 0s 345ms/step - loss: 0.3009 - mae: 0.5002 - mse: 0.3009 - val_loss: 0.2919 - val_mae: 0.4918 - val_mse: 0.2919\n",
      "Epoch 5/250\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 0.2919 - mae: 0.4918 - mse: 0.2919 - val_loss: 0.2837 - val_mae: 0.4839 - val_mse: 0.2837\n",
      "Epoch 6/250\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 0.2837 - mae: 0.4839 - mse: 0.2837 - val_loss: 0.2761 - val_mae: 0.4765 - val_mse: 0.2761\n",
      "Epoch 7/250\n",
      "1/1 [==============================] - 0s 333ms/step - loss: 0.2761 - mae: 0.4765 - mse: 0.2761 - val_loss: 0.2689 - val_mae: 0.4692 - val_mse: 0.2689\n",
      "Epoch 8/250\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.2689 - mae: 0.4692 - mse: 0.2689 - val_loss: 0.2624 - val_mae: 0.4625 - val_mse: 0.2624\n",
      "Epoch 9/250\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.2624 - mae: 0.4625 - mse: 0.2624 - val_loss: 0.2560 - val_mae: 0.4558 - val_mse: 0.2560\n",
      "Epoch 10/250\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 0.2560 - mae: 0.4558 - mse: 0.2560 - val_loss: 0.2501 - val_mae: 0.4496 - val_mse: 0.2501\n",
      "Epoch 11/250\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.2501 - mae: 0.4496 - mse: 0.2501 - val_loss: 0.2447 - val_mae: 0.4437 - val_mse: 0.2447\n",
      "Epoch 12/250\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.2447 - mae: 0.4437 - mse: 0.2447 - val_loss: 0.2395 - val_mae: 0.4380 - val_mse: 0.2395\n",
      "Epoch 13/250\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.2395 - mae: 0.4380 - mse: 0.2395 - val_loss: 0.2346 - val_mae: 0.4325 - val_mse: 0.2346\n",
      "Epoch 14/250\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.2346 - mae: 0.4325 - mse: 0.2346 - val_loss: 0.2300 - val_mae: 0.4273 - val_mse: 0.2300\n",
      "Epoch 15/250\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.2300 - mae: 0.4273 - mse: 0.2300 - val_loss: 0.2257 - val_mae: 0.4223 - val_mse: 0.2257\n",
      "Epoch 16/250\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.2257 - mae: 0.4223 - mse: 0.2257 - val_loss: 0.2216 - val_mae: 0.4175 - val_mse: 0.2216\n",
      "Epoch 17/250\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.2216 - mae: 0.4175 - mse: 0.2216 - val_loss: 0.2177 - val_mae: 0.4128 - val_mse: 0.2177\n",
      "Epoch 18/250\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.2177 - mae: 0.4128 - mse: 0.2177 - val_loss: 0.2138 - val_mae: 0.4081 - val_mse: 0.2138\n",
      "Epoch 19/250\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 0.2138 - mae: 0.4081 - mse: 0.2138 - val_loss: 0.2099 - val_mae: 0.4036 - val_mse: 0.2099\n",
      "Epoch 20/250\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.2099 - mae: 0.4036 - mse: 0.2099 - val_loss: 0.2061 - val_mae: 0.3991 - val_mse: 0.2061\n",
      "Epoch 21/250\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.2061 - mae: 0.3991 - mse: 0.2061 - val_loss: 0.2025 - val_mae: 0.3948 - val_mse: 0.2025\n",
      "Epoch 22/250\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 0.2025 - mae: 0.3948 - mse: 0.2025 - val_loss: 0.1989 - val_mae: 0.3905 - val_mse: 0.1989\n",
      "Epoch 23/250\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.1989 - mae: 0.3905 - mse: 0.1989 - val_loss: 0.1951 - val_mae: 0.3860 - val_mse: 0.1951\n",
      "Epoch 24/250\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.1951 - mae: 0.3860 - mse: 0.1951 - val_loss: 0.1913 - val_mae: 0.3814 - val_mse: 0.1913\n",
      "Epoch 25/250\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.1913 - mae: 0.3814 - mse: 0.1913 - val_loss: 0.1875 - val_mae: 0.3768 - val_mse: 0.1875\n",
      "Epoch 26/250\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.1875 - mae: 0.3768 - mse: 0.1875 - val_loss: 0.1837 - val_mae: 0.3721 - val_mse: 0.1837\n",
      "Epoch 27/250\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 0.1837 - mae: 0.3721 - mse: 0.1837 - val_loss: 0.1799 - val_mae: 0.3674 - val_mse: 0.1799\n",
      "Epoch 28/250\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.1799 - mae: 0.3674 - mse: 0.1799 - val_loss: 0.1760 - val_mae: 0.3625 - val_mse: 0.1760\n",
      "Epoch 29/250\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.1760 - mae: 0.3625 - mse: 0.1760 - val_loss: 0.1722 - val_mae: 0.3577 - val_mse: 0.1722\n",
      "Epoch 30/250\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.1722 - mae: 0.3577 - mse: 0.1722 - val_loss: 0.1683 - val_mae: 0.3528 - val_mse: 0.1683\n",
      "Epoch 31/250\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.1683 - mae: 0.3528 - mse: 0.1683 - val_loss: 0.1645 - val_mae: 0.3479 - val_mse: 0.1645\n",
      "Epoch 32/250\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 0.1645 - mae: 0.3479 - mse: 0.1645 - val_loss: 0.1608 - val_mae: 0.3432 - val_mse: 0.1608\n",
      "Epoch 33/250\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.1608 - mae: 0.3432 - mse: 0.1608 - val_loss: 0.1572 - val_mae: 0.3384 - val_mse: 0.1572\n",
      "Epoch 34/250\n",
      "1/1 [==============================] - 0s 322ms/step - loss: 0.1572 - mae: 0.3384 - mse: 0.1572 - val_loss: 0.1536 - val_mae: 0.3337 - val_mse: 0.1536\n",
      "Epoch 35/250\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.1536 - mae: 0.3337 - mse: 0.1536 - val_loss: 0.1500 - val_mae: 0.3289 - val_mse: 0.1500\n",
      "Epoch 36/250\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.1500 - mae: 0.3289 - mse: 0.1500 - val_loss: 0.1465 - val_mae: 0.3240 - val_mse: 0.1465\n",
      "Epoch 37/250\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1465 - mae: 0.3240 - mse: 0.1465 - val_loss: 0.1430 - val_mae: 0.3193 - val_mse: 0.1430\n",
      "Epoch 38/250\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.1430 - mae: 0.3193 - mse: 0.1430 - val_loss: 0.1395 - val_mae: 0.3145 - val_mse: 0.1395\n",
      "Epoch 39/250\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1395 - mae: 0.3145 - mse: 0.1395 - val_loss: 0.1361 - val_mae: 0.3098 - val_mse: 0.1361\n",
      "Epoch 40/250\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 0.1361 - mae: 0.3098 - mse: 0.1361 - val_loss: 0.1326 - val_mae: 0.3050 - val_mse: 0.1326\n",
      "Epoch 41/250\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.1326 - mae: 0.3050 - mse: 0.1326 - val_loss: 0.1291 - val_mae: 0.3002 - val_mse: 0.1291\n",
      "Epoch 42/250\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.1291 - mae: 0.3002 - mse: 0.1291 - val_loss: 0.1257 - val_mae: 0.2955 - val_mse: 0.1257\n",
      "Epoch 43/250\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.1257 - mae: 0.2955 - mse: 0.1257 - val_loss: 0.1224 - val_mae: 0.2908 - val_mse: 0.1224\n",
      "Epoch 44/250\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.1224 - mae: 0.2908 - mse: 0.1224 - val_loss: 0.1190 - val_mae: 0.2861 - val_mse: 0.1190\n",
      "Epoch 45/250\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 0.1190 - mae: 0.2861 - mse: 0.1190 - val_loss: 0.1156 - val_mae: 0.2813 - val_mse: 0.1156\n",
      "Epoch 46/250\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.1156 - mae: 0.2813 - mse: 0.1156 - val_loss: 0.1124 - val_mae: 0.2766 - val_mse: 0.1124\n",
      "Epoch 47/250\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.1124 - mae: 0.2766 - mse: 0.1124 - val_loss: 0.1092 - val_mae: 0.2721 - val_mse: 0.1092\n",
      "Epoch 48/250\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.1092 - mae: 0.2721 - mse: 0.1092 - val_loss: 0.1060 - val_mae: 0.2675 - val_mse: 0.1060\n",
      "Epoch 49/250\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.1060 - mae: 0.2675 - mse: 0.1060 - val_loss: 0.1029 - val_mae: 0.2629 - val_mse: 0.1029\n",
      "Epoch 50/250\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.1029 - mae: 0.2629 - mse: 0.1029 - val_loss: 0.0999 - val_mae: 0.2584 - val_mse: 0.0999\n",
      "Epoch 51/250\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.0999 - mae: 0.2584 - mse: 0.0999 - val_loss: 0.0970 - val_mae: 0.2540 - val_mse: 0.0970\n",
      "Epoch 52/250\n",
      "1/1 [==============================] - 0s 327ms/step - loss: 0.0970 - mae: 0.2540 - mse: 0.0970 - val_loss: 0.0942 - val_mae: 0.2497 - val_mse: 0.0942\n",
      "Epoch 53/250\n",
      "1/1 [==============================] - 0s 334ms/step - loss: 0.0942 - mae: 0.2497 - mse: 0.0942 - val_loss: 0.0914 - val_mae: 0.2453 - val_mse: 0.0914\n",
      "Epoch 54/250\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.0914 - mae: 0.2453 - mse: 0.0914 - val_loss: 0.0887 - val_mae: 0.2410 - val_mse: 0.0887\n",
      "Epoch 55/250\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.0887 - mae: 0.2410 - mse: 0.0887 - val_loss: 0.0861 - val_mae: 0.2368 - val_mse: 0.0861\n",
      "Epoch 56/250\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 0.0861 - mae: 0.2368 - mse: 0.0861 - val_loss: 0.0836 - val_mae: 0.2327 - val_mse: 0.0836\n",
      "Epoch 57/250\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.0836 - mae: 0.2327 - mse: 0.0836 - val_loss: 0.0811 - val_mae: 0.2288 - val_mse: 0.0811\n",
      "Epoch 58/250\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 0.0811 - mae: 0.2288 - mse: 0.0811 - val_loss: 0.0788 - val_mae: 0.2250 - val_mse: 0.0788\n",
      "Epoch 59/250\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.0788 - mae: 0.2250 - mse: 0.0788 - val_loss: 0.0765 - val_mae: 0.2213 - val_mse: 0.0765\n",
      "Epoch 60/250\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.0765 - mae: 0.2213 - mse: 0.0765 - val_loss: 0.0744 - val_mae: 0.2178 - val_mse: 0.0744\n",
      "Epoch 61/250\n",
      "1/1 [==============================] - 0s 257ms/step - loss: 0.0744 - mae: 0.2178 - mse: 0.0744 - val_loss: 0.0723 - val_mae: 0.2145 - val_mse: 0.0723\n",
      "Epoch 62/250\n",
      "1/1 [==============================] - 0s 255ms/step - loss: 0.0723 - mae: 0.2145 - mse: 0.0723 - val_loss: 0.0703 - val_mae: 0.2113 - val_mse: 0.0703\n",
      "Epoch 63/250\n",
      "1/1 [==============================] - 0s 255ms/step - loss: 0.0703 - mae: 0.2113 - mse: 0.0703 - val_loss: 0.0685 - val_mae: 0.2082 - val_mse: 0.0685\n",
      "Epoch 64/250\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.0685 - mae: 0.2082 - mse: 0.0685 - val_loss: 0.0667 - val_mae: 0.2053 - val_mse: 0.0667\n",
      "Epoch 65/250\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.0667 - mae: 0.2053 - mse: 0.0667 - val_loss: 0.0650 - val_mae: 0.2025 - val_mse: 0.0650\n",
      "Epoch 66/250\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.0650 - mae: 0.2025 - mse: 0.0650 - val_loss: 0.0633 - val_mae: 0.1998 - val_mse: 0.0633\n",
      "Epoch 67/250\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.0633 - mae: 0.1998 - mse: 0.0633 - val_loss: 0.0618 - val_mae: 0.1972 - val_mse: 0.0618\n",
      "Epoch 68/250\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.0618 - mae: 0.1972 - mse: 0.0618 - val_loss: 0.0604 - val_mae: 0.1948 - val_mse: 0.0604\n",
      "Epoch 69/250\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.0604 - mae: 0.1948 - mse: 0.0604 - val_loss: 0.0591 - val_mae: 0.1925 - val_mse: 0.0591\n",
      "Epoch 70/250\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.0591 - mae: 0.1925 - mse: 0.0591 - val_loss: 0.0579 - val_mae: 0.1904 - val_mse: 0.0579\n",
      "Epoch 71/250\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.0579 - mae: 0.1904 - mse: 0.0579 - val_loss: 0.0567 - val_mae: 0.1885 - val_mse: 0.0567\n",
      "Epoch 72/250\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.0567 - mae: 0.1885 - mse: 0.0567 - val_loss: 0.0556 - val_mae: 0.1868 - val_mse: 0.0556\n",
      "Epoch 73/250\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.0556 - mae: 0.1868 - mse: 0.0556 - val_loss: 0.0546 - val_mae: 0.1853 - val_mse: 0.0546\n",
      "Epoch 74/250\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 0.0546 - mae: 0.1853 - mse: 0.0546 - val_loss: 0.0537 - val_mae: 0.1839 - val_mse: 0.0537\n",
      "Epoch 75/250\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.0537 - mae: 0.1839 - mse: 0.0537 - val_loss: 0.0529 - val_mae: 0.1828 - val_mse: 0.0529\n",
      "Epoch 76/250\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.0529 - mae: 0.1828 - mse: 0.0529 - val_loss: 0.0521 - val_mae: 0.1817 - val_mse: 0.0521\n",
      "Epoch 77/250\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.0521 - mae: 0.1817 - mse: 0.0521 - val_loss: 0.0514 - val_mae: 0.1808 - val_mse: 0.0514\n",
      "Epoch 78/250\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.0514 - mae: 0.1808 - mse: 0.0514 - val_loss: 0.0508 - val_mae: 0.1799 - val_mse: 0.0508\n",
      "Epoch 79/250\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 0.0508 - mae: 0.1799 - mse: 0.0508 - val_loss: 0.0502 - val_mae: 0.1791 - val_mse: 0.0502\n",
      "Epoch 80/250\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.0502 - mae: 0.1791 - mse: 0.0502 - val_loss: 0.0496 - val_mae: 0.1784 - val_mse: 0.0496\n",
      "Epoch 81/250\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.0496 - mae: 0.1784 - mse: 0.0496 - val_loss: 0.0491 - val_mae: 0.1777 - val_mse: 0.0491\n",
      "Epoch 82/250\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.0491 - mae: 0.1777 - mse: 0.0491 - val_loss: 0.0487 - val_mae: 0.1771 - val_mse: 0.0487\n",
      "Epoch 83/250\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 0.0487 - mae: 0.1771 - mse: 0.0487 - val_loss: 0.0483 - val_mae: 0.1766 - val_mse: 0.0483\n",
      "Epoch 84/250\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.0483 - mae: 0.1766 - mse: 0.0483 - val_loss: 0.0479 - val_mae: 0.1761 - val_mse: 0.0479\n",
      "Epoch 85/250\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.0479 - mae: 0.1761 - mse: 0.0479 - val_loss: 0.0476 - val_mae: 0.1757 - val_mse: 0.0476\n",
      "Epoch 86/250\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.0476 - mae: 0.1757 - mse: 0.0476 - val_loss: 0.0473 - val_mae: 0.1753 - val_mse: 0.0473\n",
      "Epoch 87/250\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 0.0473 - mae: 0.1753 - mse: 0.0473 - val_loss: 0.0470 - val_mae: 0.1749 - val_mse: 0.0470\n",
      "Epoch 88/250\n",
      "1/1 [==============================] - 0s 323ms/step - loss: 0.0470 - mae: 0.1749 - mse: 0.0470 - val_loss: 0.0468 - val_mae: 0.1746 - val_mse: 0.0468\n",
      "Epoch 89/250\n",
      "1/1 [==============================] - 0s 325ms/step - loss: 0.0468 - mae: 0.1746 - mse: 0.0468 - val_loss: 0.0465 - val_mae: 0.1744 - val_mse: 0.0465\n",
      "Epoch 90/250\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 0.0465 - mae: 0.1744 - mse: 0.0465 - val_loss: 0.0464 - val_mae: 0.1742 - val_mse: 0.0464\n",
      "Epoch 91/250\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.0464 - mae: 0.1742 - mse: 0.0464 - val_loss: 0.0462 - val_mae: 0.1741 - val_mse: 0.0462\n",
      "Epoch 92/250\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.0462 - mae: 0.1741 - mse: 0.0462 - val_loss: 0.0460 - val_mae: 0.1740 - val_mse: 0.0460\n",
      "Epoch 93/250\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.0460 - mae: 0.1740 - mse: 0.0460 - val_loss: 0.0459 - val_mae: 0.1739 - val_mse: 0.0459\n",
      "Epoch 94/250\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.0459 - mae: 0.1739 - mse: 0.0459 - val_loss: 0.0458 - val_mae: 0.1738 - val_mse: 0.0458\n",
      "Epoch 95/250\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.0458 - mae: 0.1738 - mse: 0.0458 - val_loss: 0.0457 - val_mae: 0.1737 - val_mse: 0.0457\n",
      "Epoch 96/250\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.0457 - mae: 0.1737 - mse: 0.0457 - val_loss: 0.0456 - val_mae: 0.1737 - val_mse: 0.0456\n",
      "Epoch 97/250\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 0.0456 - mae: 0.1737 - mse: 0.0456 - val_loss: 0.0455 - val_mae: 0.1736 - val_mse: 0.0455\n",
      "Epoch 98/250\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 0.0455 - mae: 0.1736 - mse: 0.0455 - val_loss: 0.0455 - val_mae: 0.1736 - val_mse: 0.0455\n",
      "Epoch 99/250\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 0.0455 - mae: 0.1736 - mse: 0.0455 - val_loss: 0.0454 - val_mae: 0.1736 - val_mse: 0.0454\n",
      "Epoch 100/250\n",
      "1/1 [==============================] - 0s 376ms/step - loss: 0.0454 - mae: 0.1736 - mse: 0.0454 - val_loss: 0.0454 - val_mae: 0.1736 - val_mse: 0.0454\n",
      "Epoch 101/250\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 0.0454 - mae: 0.1736 - mse: 0.0454 - val_loss: 0.0453 - val_mae: 0.1735 - val_mse: 0.0453\n",
      "Epoch 102/250\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.0453 - mae: 0.1735 - mse: 0.0453 - val_loss: 0.0453 - val_mae: 0.1735 - val_mse: 0.0453\n",
      "Epoch 103/250\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.0453 - mae: 0.1735 - mse: 0.0453 - val_loss: 0.0453 - val_mae: 0.1735 - val_mse: 0.0453\n",
      "Epoch 104/250\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 0.0453 - mae: 0.1735 - mse: 0.0453 - val_loss: 0.0452 - val_mae: 0.1735 - val_mse: 0.0452\n",
      "Epoch 105/250\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.0452 - mae: 0.1735 - mse: 0.0452 - val_loss: 0.0452 - val_mae: 0.1736 - val_mse: 0.0452\n",
      "Epoch 106/250\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.0452 - mae: 0.1736 - mse: 0.0452 - val_loss: 0.0452 - val_mae: 0.1736 - val_mse: 0.0452\n",
      "Epoch 107/250\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.0452 - mae: 0.1736 - mse: 0.0452 - val_loss: 0.0452 - val_mae: 0.1736 - val_mse: 0.0452\n",
      "Epoch 108/250\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 0.0452 - mae: 0.1736 - mse: 0.0452 - val_loss: 0.0452 - val_mae: 0.1736 - val_mse: 0.0452\n",
      "Epoch 109/250\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.0452 - mae: 0.1736 - mse: 0.0452 - val_loss: 0.0452 - val_mae: 0.1736 - val_mse: 0.0452\n",
      "Epoch 110/250\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.0452 - mae: 0.1736 - mse: 0.0452 - val_loss: 0.0452 - val_mae: 0.1735 - val_mse: 0.0452\n",
      "Epoch 111/250\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.0452 - mae: 0.1735 - mse: 0.0452 - val_loss: 0.0452 - val_mae: 0.1735 - val_mse: 0.0452\n",
      "Epoch 112/250\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.0452 - mae: 0.1735 - mse: 0.0452 - val_loss: 0.0452 - val_mae: 0.1735 - val_mse: 0.0452\n",
      "Epoch 113/250\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 0.0452 - mae: 0.1735 - mse: 0.0452 - val_loss: 0.0452 - val_mae: 0.1735 - val_mse: 0.0452\n",
      "Epoch 114/250\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.0452 - mae: 0.1735 - mse: 0.0452 - val_loss: 0.0452 - val_mae: 0.1735 - val_mse: 0.0452\n",
      "Epoch 115/250\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 0.0452 - mae: 0.1735 - mse: 0.0452 - val_loss: 0.0452 - val_mae: 0.1735 - val_mse: 0.0452\n",
      "Epoch 116/250\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 0.0452 - mae: 0.1735 - mse: 0.0452 - val_loss: 0.0452 - val_mae: 0.1735 - val_mse: 0.0452\n",
      "Epoch 117/250\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.0452 - mae: 0.1735 - mse: 0.0452 - val_loss: 0.0452 - val_mae: 0.1735 - val_mse: 0.0452\n",
      "Epoch 118/250\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.0452 - mae: 0.1735 - mse: 0.0452 - val_loss: 0.0452 - val_mae: 0.1735 - val_mse: 0.0452\n",
      "Epoch 119/250\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.0452 - mae: 0.1735 - mse: 0.0452 - val_loss: 0.0452 - val_mae: 0.1734 - val_mse: 0.0452\n",
      "Epoch 120/250\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 0.0452 - mae: 0.1734 - mse: 0.0452 - val_loss: 0.0452 - val_mae: 0.1734 - val_mse: 0.0452\n",
      "Epoch 121/250\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 0.0452 - mae: 0.1734 - mse: 0.0452 - val_loss: 0.0452 - val_mae: 0.1734 - val_mse: 0.0452\n",
      "Epoch 122/250\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.0452 - mae: 0.1734 - mse: 0.0452 - val_loss: 0.0452 - val_mae: 0.1734 - val_mse: 0.0452\n",
      "Epoch 123/250\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 0.0452 - mae: 0.1734 - mse: 0.0452 - val_loss: 0.0452 - val_mae: 0.1734 - val_mse: 0.0452\n",
      "Epoch 124/250\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 0.0452 - mae: 0.1734 - mse: 0.0452 - val_loss: 0.0452 - val_mae: 0.1734 - val_mse: 0.0452\n",
      "Epoch 125/250\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 0.0452 - mae: 0.1734 - mse: 0.0452 - val_loss: 0.0452 - val_mae: 0.1734 - val_mse: 0.0452\n",
      "Epoch 126/250\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 0.0452 - mae: 0.1734 - mse: 0.0452 - val_loss: 0.0452 - val_mae: 0.1734 - val_mse: 0.0452\n",
      "Epoch 127/250\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 0.0452 - mae: 0.1734 - mse: 0.0452 - val_loss: 0.0452 - val_mae: 0.1734 - val_mse: 0.0452\n",
      "Epoch 128/250\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.0452 - mae: 0.1734 - mse: 0.0452 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 129/250\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 130/250\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 131/250\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 132/250\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 133/250\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 134/250\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 135/250\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 136/250\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 137/250\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1733 - val_mse: 0.0451\n",
      "Epoch 138/250\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.0451 - mae: 0.1733 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1733 - val_mse: 0.0451\n",
      "Epoch 139/250\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.0451 - mae: 0.1733 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1733 - val_mse: 0.0451\n",
      "Epoch 140/250\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 0.0451 - mae: 0.1733 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1733 - val_mse: 0.0451\n",
      "Epoch 141/250\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 0.0451 - mae: 0.1733 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1733 - val_mse: 0.0451\n",
      "Epoch 142/250\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 0.0451 - mae: 0.1733 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1733 - val_mse: 0.0451\n",
      "Epoch 143/250\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.0451 - mae: 0.1733 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1733 - val_mse: 0.0451\n",
      "Epoch 144/250\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.0451 - mae: 0.1733 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1733 - val_mse: 0.0451\n",
      "Epoch 145/250\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.0451 - mae: 0.1733 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1733 - val_mse: 0.0451\n",
      "Epoch 146/250\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 0.0451 - mae: 0.1733 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1733 - val_mse: 0.0451\n",
      "Epoch 147/250\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.0451 - mae: 0.1733 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1733 - val_mse: 0.0451\n",
      "Epoch 148/250\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 0.0451 - mae: 0.1733 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1733 - val_mse: 0.0451\n",
      "Epoch 149/250\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.0451 - mae: 0.1733 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1733 - val_mse: 0.0451\n",
      "Epoch 150/250\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 0.0451 - mae: 0.1733 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1733 - val_mse: 0.0451\n",
      "Epoch 151/250\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 0.0451 - mae: 0.1733 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1733 - val_mse: 0.0451\n",
      "Epoch 152/250\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 0.0451 - mae: 0.1733 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1733 - val_mse: 0.0451\n",
      "Epoch 153/250\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 0.0451 - mae: 0.1733 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1733 - val_mse: 0.0451\n",
      "Epoch 154/250\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 0.0451 - mae: 0.1733 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1733 - val_mse: 0.0451\n",
      "Epoch 155/250\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 0.0451 - mae: 0.1733 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1733 - val_mse: 0.0451\n",
      "Epoch 156/250\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 0.0451 - mae: 0.1733 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1733 - val_mse: 0.0451\n",
      "Epoch 157/250\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 0.0451 - mae: 0.1733 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1733 - val_mse: 0.0451\n",
      "Epoch 158/250\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 0.0451 - mae: 0.1733 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1733 - val_mse: 0.0451\n",
      "Epoch 159/250\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 0.0451 - mae: 0.1733 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1733 - val_mse: 0.0451\n",
      "Epoch 160/250\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 0.0451 - mae: 0.1733 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 161/250\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 162/250\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 163/250\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 164/250\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 165/250\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 166/250\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 167/250\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 168/250\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 169/250\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 170/250\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 171/250\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 172/250\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 173/250\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 174/250\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 175/250\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 176/250\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 177/250\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 178/250\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 179/250\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 180/250\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 181/250\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 182/250\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 183/250\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 184/250\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 185/250\n",
      "1/1 [==============================] - 0s 322ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 186/250\n",
      "1/1 [==============================] - 0s 358ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 187/250\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 188/250\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 189/250\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 190/250\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 191/250\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 192/250\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 193/250\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 194/250\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 195/250\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 196/250\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 197/250\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 198/250\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 199/250\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 200/250\n",
      "1/1 [==============================] - 0s 358ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 201/250\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 202/250\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 203/250\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 204/250\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 205/250\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 206/250\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 207/250\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 208/250\n",
      "1/1 [==============================] - 0s 322ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 209/250\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 210/250\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 211/250\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 212/250\n",
      "1/1 [==============================] - 0s 295ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 213/250\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 214/250\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 215/250\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 216/250\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 217/250\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 218/250\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 219/250\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 220/250\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 221/250\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 222/250\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 223/250\n",
      "1/1 [==============================] - 0s 336ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 224/250\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 225/250\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 226/250\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 227/250\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 228/250\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 229/250\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 230/250\n",
      "1/1 [==============================] - 0s 341ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 231/250\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 232/250\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 233/250\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 234/250\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 235/250\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 236/250\n",
      "1/1 [==============================] - 0s 321ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 237/250\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 238/250\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 239/250\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 240/250\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 241/250\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 242/250\n",
      "1/1 [==============================] - 0s 449ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 243/250\n",
      "1/1 [==============================] - 0s 354ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 244/250\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 245/250\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 246/250\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 247/250\n",
      "1/1 [==============================] - 0s 319ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 248/250\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 249/250\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 250/250\n",
      "1/1 [==============================] - 0s 337ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs     = 250\n",
    "history = model.fit(x_trainset[:batch_size], y_trainset[:batch_size],\n",
    "                   epochs = epochs,\n",
    "                   validation_data = (x_trainset[:batch_size], y_trainset[:batch_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e574456b-4207-4f12-95d6-6500df31570b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.ylim(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1fb78786-e197-4d00-8df8-01650720f135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 121ms/step\n"
     ]
    }
   ],
   "source": [
    "y_predicted = model.predict(x_trainset[:batch_size]) * (max_y - min_y) + min_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "cdad5bcb-3e58-42d1-a8c8-0785e0666c8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70, 20)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_trainset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2358d20c-773a-48a7-8fa0-b9317ce7fcb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = 22\n",
    "plt.plot(10**(y_predicted[i,:]))\n",
    "plt.plot(10**(y_trainset[i,:] * (max_y - min_y) + min_y), c = 'black')\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6972a4e-0c74-445b-bd5c-e52f5ef260bd",
   "metadata": {},
   "source": [
    "## Now let's play the real game\n",
    "\n",
    "* Seems to converge to MSE = 0.054 (without batch normalization nor dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "48025db5-24d2-4bcf-9814-47b0e2c89d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_24 (Conv2D)          (None, 128, 128, 5)       1005      \n",
      "                                                                 \n",
      " batch_normalization_21 (Ba  (None, 128, 128, 5)       20        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_24 (Activation)  (None, 128, 128, 5)       0         \n",
      "                                                                 \n",
      " max_pooling2d_24 (MaxPooli  (None, 64, 64, 5)         0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_25 (Conv2D)          (None, 64, 64, 15)        1890      \n",
      "                                                                 \n",
      " batch_normalization_22 (Ba  (None, 64, 64, 15)        60        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_25 (Activation)  (None, 64, 64, 15)        0         \n",
      "                                                                 \n",
      " max_pooling2d_25 (MaxPooli  (None, 32, 32, 15)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_26 (Conv2D)          (None, 32, 32, 30)        11280     \n",
      "                                                                 \n",
      " batch_normalization_23 (Ba  (None, 32, 32, 30)        120       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_26 (Activation)  (None, 32, 32, 30)        0         \n",
      "                                                                 \n",
      " max_pooling2d_26 (MaxPooli  (None, 16, 16, 30)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 16, 16, 30)        0         \n",
      "                                                                 \n",
      " flatten_8 (Flatten)         (None, 7680)              0         \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 500)               3840500   \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 400)               200400    \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 300)               120300    \n",
      "                                                                 \n",
      " dense_35 (Dense)            (None, 200)               60200     \n",
      "                                                                 \n",
      " output (Dense)              (None, 20)                4020      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4239795 (16.17 MB)\n",
      "Trainable params: 4239695 (16.17 MB)\n",
      "Non-trainable params: 100 (400.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters = 5, kernel_size = (5,5), strides = 1 , padding = 'same', input_shape = input_shape))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(actFunction))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(filters = 15, kernel_size = (5,5), strides = 1 , padding = 'same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(actFunction))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(filters = 30, kernel_size = (5,5), strides = 1 , padding = 'same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(actFunction))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Dropout(0.2)) \n",
    "model.add(Flatten())\n",
    "\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dense(intermediate_dim1, activation = actFunction))\n",
    "\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dense(intermediate_dim2, activation = actFunction))\n",
    "\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dense(intermediate_dim3, activation = actFunction))\n",
    "\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dense(intermediate_dim4, activation = 'linear'))\n",
    "\n",
    "model.add(Dense(output_dim, name = 'output'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "cd25fed0-6299-4f95-84e3-ba1fb2a984fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model\n",
    "optimizer = optimizers.Adam(learning_rate = 1e-5, beta_1 = 0.9, beta_2 = 0.999, amsgrad = False)\n",
    "model.compile(optimizer = optimizer, loss = 'mse', metrics=['mae','mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "756035ff-ea8d-48e4-813a-2528bcabaac4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0530 - mae: 0.1808 - mse: 0.0530\n",
      "Epoch 1: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 250ms/step - loss: 0.0530 - mae: 0.1808 - mse: 0.0530 - val_loss: 0.0669 - val_mae: 0.2041 - val_mse: 0.0669\n",
      "Epoch 2/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0530 - mae: 0.1823 - mse: 0.0530\n",
      "Epoch 2: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0530 - mae: 0.1823 - mse: 0.0530 - val_loss: 0.0684 - val_mae: 0.2066 - val_mse: 0.0684\n",
      "Epoch 3/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0493 - mae: 0.1753 - mse: 0.0493\n",
      "Epoch 3: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 239ms/step - loss: 0.0493 - mae: 0.1753 - mse: 0.0493 - val_loss: 0.0696 - val_mae: 0.2086 - val_mse: 0.0696\n",
      "Epoch 4/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0516 - mae: 0.1799 - mse: 0.0516\n",
      "Epoch 4: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0516 - mae: 0.1799 - mse: 0.0516 - val_loss: 0.0703 - val_mae: 0.2099 - val_mse: 0.0703\n",
      "Epoch 5/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0538 - mae: 0.1847 - mse: 0.0538\n",
      "Epoch 5: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 249ms/step - loss: 0.0538 - mae: 0.1847 - mse: 0.0538 - val_loss: 0.0705 - val_mae: 0.2102 - val_mse: 0.0705\n",
      "Epoch 6/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0508 - mae: 0.1800 - mse: 0.0508\n",
      "Epoch 6: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 272ms/step - loss: 0.0508 - mae: 0.1800 - mse: 0.0508 - val_loss: 0.0701 - val_mae: 0.2098 - val_mse: 0.0701\n",
      "Epoch 7/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0510 - mae: 0.1810 - mse: 0.0510\n",
      "Epoch 7: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 239ms/step - loss: 0.0510 - mae: 0.1810 - mse: 0.0510 - val_loss: 0.0689 - val_mae: 0.2076 - val_mse: 0.0689\n",
      "Epoch 8/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0551 - mae: 0.1876 - mse: 0.0551\n",
      "Epoch 8: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 288ms/step - loss: 0.0551 - mae: 0.1876 - mse: 0.0551 - val_loss: 0.0673 - val_mae: 0.2051 - val_mse: 0.0673\n",
      "Epoch 9/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0528 - mae: 0.1827 - mse: 0.0528\n",
      "Epoch 9: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0528 - mae: 0.1827 - mse: 0.0528 - val_loss: 0.0662 - val_mae: 0.2030 - val_mse: 0.0662\n",
      "Epoch 10/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0495 - mae: 0.1779 - mse: 0.0495\n",
      "Epoch 10: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 241ms/step - loss: 0.0495 - mae: 0.1779 - mse: 0.0495 - val_loss: 0.0672 - val_mae: 0.2049 - val_mse: 0.0672\n",
      "Epoch 11/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0549 - mae: 0.1863 - mse: 0.0549\n",
      "Epoch 11: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 247ms/step - loss: 0.0549 - mae: 0.1863 - mse: 0.0549 - val_loss: 0.0682 - val_mae: 0.2067 - val_mse: 0.0682\n",
      "Epoch 12/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0544 - mae: 0.1838 - mse: 0.0544\n",
      "Epoch 12: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0544 - mae: 0.1838 - mse: 0.0544 - val_loss: 0.0692 - val_mae: 0.2084 - val_mse: 0.0692\n",
      "Epoch 13/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0521 - mae: 0.1810 - mse: 0.0521\n",
      "Epoch 13: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0521 - mae: 0.1810 - mse: 0.0521 - val_loss: 0.0719 - val_mae: 0.2131 - val_mse: 0.0719\n",
      "Epoch 14/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0542 - mae: 0.1847 - mse: 0.0542\n",
      "Epoch 14: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0542 - mae: 0.1847 - mse: 0.0542 - val_loss: 0.0718 - val_mae: 0.2130 - val_mse: 0.0718\n",
      "Epoch 15/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0509 - mae: 0.1788 - mse: 0.0509\n",
      "Epoch 15: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0509 - mae: 0.1788 - mse: 0.0509 - val_loss: 0.0691 - val_mae: 0.2080 - val_mse: 0.0691\n",
      "Epoch 16/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0513 - mae: 0.1785 - mse: 0.0513\n",
      "Epoch 16: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 239ms/step - loss: 0.0513 - mae: 0.1785 - mse: 0.0513 - val_loss: 0.0682 - val_mae: 0.2064 - val_mse: 0.0682\n",
      "Epoch 17/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0510 - mae: 0.1785 - mse: 0.0510\n",
      "Epoch 17: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0510 - mae: 0.1785 - mse: 0.0510 - val_loss: 0.0678 - val_mae: 0.2056 - val_mse: 0.0678\n",
      "Epoch 18/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0510 - mae: 0.1780 - mse: 0.0510\n",
      "Epoch 18: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 246ms/step - loss: 0.0510 - mae: 0.1780 - mse: 0.0510 - val_loss: 0.0677 - val_mae: 0.2053 - val_mse: 0.0677\n",
      "Epoch 19/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0531 - mae: 0.1832 - mse: 0.0531\n",
      "Epoch 19: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 243ms/step - loss: 0.0531 - mae: 0.1832 - mse: 0.0531 - val_loss: 0.0673 - val_mae: 0.2047 - val_mse: 0.0673\n",
      "Epoch 20/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0505 - mae: 0.1785 - mse: 0.0505\n",
      "Epoch 20: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 239ms/step - loss: 0.0505 - mae: 0.1785 - mse: 0.0505 - val_loss: 0.0662 - val_mae: 0.2027 - val_mse: 0.0662\n",
      "Epoch 21/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0491 - mae: 0.1743 - mse: 0.0491\n",
      "Epoch 21: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 242ms/step - loss: 0.0491 - mae: 0.1743 - mse: 0.0491 - val_loss: 0.0659 - val_mae: 0.2019 - val_mse: 0.0659\n",
      "Epoch 22/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0524 - mae: 0.1812 - mse: 0.0524\n",
      "Epoch 22: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 277ms/step - loss: 0.0524 - mae: 0.1812 - mse: 0.0524 - val_loss: 0.0641 - val_mae: 0.1983 - val_mse: 0.0641\n",
      "Epoch 23/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0538 - mae: 0.1839 - mse: 0.0538\n",
      "Epoch 23: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0538 - mae: 0.1839 - mse: 0.0538 - val_loss: 0.0627 - val_mae: 0.1956 - val_mse: 0.0627\n",
      "Epoch 24/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0506 - mae: 0.1770 - mse: 0.0506\n",
      "Epoch 24: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0506 - mae: 0.1770 - mse: 0.0506 - val_loss: 0.0638 - val_mae: 0.1974 - val_mse: 0.0638\n",
      "Epoch 25/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0498 - mae: 0.1758 - mse: 0.0498\n",
      "Epoch 25: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 260ms/step - loss: 0.0498 - mae: 0.1758 - mse: 0.0498 - val_loss: 0.0664 - val_mae: 0.2020 - val_mse: 0.0664\n",
      "Epoch 26/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0520 - mae: 0.1803 - mse: 0.0520\n",
      "Epoch 26: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 252ms/step - loss: 0.0520 - mae: 0.1803 - mse: 0.0520 - val_loss: 0.0684 - val_mae: 0.2054 - val_mse: 0.0684\n",
      "Epoch 27/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0517 - mae: 0.1801 - mse: 0.0517\n",
      "Epoch 27: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 258ms/step - loss: 0.0517 - mae: 0.1801 - mse: 0.0517 - val_loss: 0.0697 - val_mae: 0.2081 - val_mse: 0.0697\n",
      "Epoch 28/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0536 - mae: 0.1827 - mse: 0.0536\n",
      "Epoch 28: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 255ms/step - loss: 0.0536 - mae: 0.1827 - mse: 0.0536 - val_loss: 0.0697 - val_mae: 0.2084 - val_mse: 0.0697\n",
      "Epoch 29/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0509 - mae: 0.1807 - mse: 0.0509\n",
      "Epoch 29: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 260ms/step - loss: 0.0509 - mae: 0.1807 - mse: 0.0509 - val_loss: 0.0699 - val_mae: 0.2091 - val_mse: 0.0699\n",
      "Epoch 30/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0538 - mae: 0.1827 - mse: 0.0538\n",
      "Epoch 30: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 265ms/step - loss: 0.0538 - mae: 0.1827 - mse: 0.0538 - val_loss: 0.0697 - val_mae: 0.2091 - val_mse: 0.0697\n",
      "Epoch 31/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0516 - mae: 0.1821 - mse: 0.0516\n",
      "Epoch 31: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 253ms/step - loss: 0.0516 - mae: 0.1821 - mse: 0.0516 - val_loss: 0.0688 - val_mae: 0.2075 - val_mse: 0.0688\n",
      "Epoch 32/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0538 - mae: 0.1832 - mse: 0.0538\n",
      "Epoch 32: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 253ms/step - loss: 0.0538 - mae: 0.1832 - mse: 0.0538 - val_loss: 0.0686 - val_mae: 0.2071 - val_mse: 0.0686\n",
      "Epoch 33/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0510 - mae: 0.1800 - mse: 0.0510\n",
      "Epoch 33: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 246ms/step - loss: 0.0510 - mae: 0.1800 - mse: 0.0510 - val_loss: 0.0680 - val_mae: 0.2059 - val_mse: 0.0680\n",
      "Epoch 34/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0521 - mae: 0.1809 - mse: 0.0521\n",
      "Epoch 34: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 244ms/step - loss: 0.0521 - mae: 0.1809 - mse: 0.0521 - val_loss: 0.0664 - val_mae: 0.2027 - val_mse: 0.0664\n",
      "Epoch 35/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0505 - mae: 0.1796 - mse: 0.0505\n",
      "Epoch 35: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 240ms/step - loss: 0.0505 - mae: 0.1796 - mse: 0.0505 - val_loss: 0.0641 - val_mae: 0.1983 - val_mse: 0.0641\n",
      "Epoch 36/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0524 - mae: 0.1806 - mse: 0.0524\n",
      "Epoch 36: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 247ms/step - loss: 0.0524 - mae: 0.1806 - mse: 0.0524 - val_loss: 0.0628 - val_mae: 0.1960 - val_mse: 0.0628\n",
      "Epoch 37/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0510 - mae: 0.1781 - mse: 0.0510\n",
      "Epoch 37: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 254ms/step - loss: 0.0510 - mae: 0.1781 - mse: 0.0510 - val_loss: 0.0621 - val_mae: 0.1948 - val_mse: 0.0621\n",
      "Epoch 38/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0514 - mae: 0.1797 - mse: 0.0514\n",
      "Epoch 38: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 263ms/step - loss: 0.0514 - mae: 0.1797 - mse: 0.0514 - val_loss: 0.0623 - val_mae: 0.1952 - val_mse: 0.0623\n",
      "Epoch 39/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0507 - mae: 0.1793 - mse: 0.0507\n",
      "Epoch 39: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 270ms/step - loss: 0.0507 - mae: 0.1793 - mse: 0.0507 - val_loss: 0.0632 - val_mae: 0.1967 - val_mse: 0.0632\n",
      "Epoch 40/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0487 - mae: 0.1736 - mse: 0.0487\n",
      "Epoch 40: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 281ms/step - loss: 0.0487 - mae: 0.1736 - mse: 0.0487 - val_loss: 0.0649 - val_mae: 0.1997 - val_mse: 0.0649\n",
      "Epoch 41/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0515 - mae: 0.1795 - mse: 0.0515\n",
      "Epoch 41: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 266ms/step - loss: 0.0515 - mae: 0.1795 - mse: 0.0515 - val_loss: 0.0669 - val_mae: 0.2032 - val_mse: 0.0669\n",
      "Epoch 42/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0490 - mae: 0.1756 - mse: 0.0490\n",
      "Epoch 42: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 258ms/step - loss: 0.0490 - mae: 0.1756 - mse: 0.0490 - val_loss: 0.0682 - val_mae: 0.2056 - val_mse: 0.0682\n",
      "Epoch 43/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0500 - mae: 0.1750 - mse: 0.0500\n",
      "Epoch 43: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 254ms/step - loss: 0.0500 - mae: 0.1750 - mse: 0.0500 - val_loss: 0.0691 - val_mae: 0.2074 - val_mse: 0.0691\n",
      "Epoch 44/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0508 - mae: 0.1781 - mse: 0.0508\n",
      "Epoch 44: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 250ms/step - loss: 0.0508 - mae: 0.1781 - mse: 0.0508 - val_loss: 0.0678 - val_mae: 0.2053 - val_mse: 0.0678\n",
      "Epoch 45/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0504 - mae: 0.1758 - mse: 0.0504\n",
      "Epoch 45: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 254ms/step - loss: 0.0504 - mae: 0.1758 - mse: 0.0504 - val_loss: 0.0661 - val_mae: 0.2023 - val_mse: 0.0661\n",
      "Epoch 46/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0489 - mae: 0.1770 - mse: 0.0489\n",
      "Epoch 46: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 250ms/step - loss: 0.0489 - mae: 0.1770 - mse: 0.0489 - val_loss: 0.0648 - val_mae: 0.2002 - val_mse: 0.0648\n",
      "Epoch 47/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0494 - mae: 0.1765 - mse: 0.0494\n",
      "Epoch 47: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 251ms/step - loss: 0.0494 - mae: 0.1765 - mse: 0.0494 - val_loss: 0.0640 - val_mae: 0.1988 - val_mse: 0.0640\n",
      "Epoch 48/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0515 - mae: 0.1796 - mse: 0.0515\n",
      "Epoch 48: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 250ms/step - loss: 0.0515 - mae: 0.1796 - mse: 0.0515 - val_loss: 0.0634 - val_mae: 0.1977 - val_mse: 0.0634\n",
      "Epoch 49/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0484 - mae: 0.1737 - mse: 0.0484\n",
      "Epoch 49: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 260ms/step - loss: 0.0484 - mae: 0.1737 - mse: 0.0484 - val_loss: 0.0626 - val_mae: 0.1963 - val_mse: 0.0626\n",
      "Epoch 50/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0519 - mae: 0.1813 - mse: 0.0519\n",
      "Epoch 50: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 253ms/step - loss: 0.0519 - mae: 0.1813 - mse: 0.0519 - val_loss: 0.0623 - val_mae: 0.1957 - val_mse: 0.0623\n",
      "Epoch 51/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0510 - mae: 0.1802 - mse: 0.0510\n",
      "Epoch 51: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 256ms/step - loss: 0.0510 - mae: 0.1802 - mse: 0.0510 - val_loss: 0.0618 - val_mae: 0.1949 - val_mse: 0.0618\n",
      "Epoch 52/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0530 - mae: 0.1839 - mse: 0.0530\n",
      "Epoch 52: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 302ms/step - loss: 0.0530 - mae: 0.1839 - mse: 0.0530 - val_loss: 0.0618 - val_mae: 0.1944 - val_mse: 0.0618\n",
      "Epoch 53/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0494 - mae: 0.1770 - mse: 0.0494\n",
      "Epoch 53: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 269ms/step - loss: 0.0494 - mae: 0.1770 - mse: 0.0494 - val_loss: 0.0623 - val_mae: 0.1949 - val_mse: 0.0623\n",
      "Epoch 54/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0528 - mae: 0.1785 - mse: 0.0528\n",
      "Epoch 54: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 267ms/step - loss: 0.0528 - mae: 0.1785 - mse: 0.0528 - val_loss: 0.0647 - val_mae: 0.1990 - val_mse: 0.0647\n",
      "Epoch 55/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0525 - mae: 0.1818 - mse: 0.0525\n",
      "Epoch 55: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 262ms/step - loss: 0.0525 - mae: 0.1818 - mse: 0.0525 - val_loss: 0.0672 - val_mae: 0.2036 - val_mse: 0.0672\n",
      "Epoch 56/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0492 - mae: 0.1768 - mse: 0.0492\n",
      "Epoch 56: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 254ms/step - loss: 0.0492 - mae: 0.1768 - mse: 0.0492 - val_loss: 0.0682 - val_mae: 0.2055 - val_mse: 0.0682\n",
      "Epoch 57/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0487 - mae: 0.1742 - mse: 0.0487\n",
      "Epoch 57: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 248ms/step - loss: 0.0487 - mae: 0.1742 - mse: 0.0487 - val_loss: 0.0666 - val_mae: 0.2025 - val_mse: 0.0666\n",
      "Epoch 58/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0488 - mae: 0.1763 - mse: 0.0488\n",
      "Epoch 58: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 240ms/step - loss: 0.0488 - mae: 0.1763 - mse: 0.0488 - val_loss: 0.0644 - val_mae: 0.1987 - val_mse: 0.0644\n",
      "Epoch 59/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0508 - mae: 0.1789 - mse: 0.0508\n",
      "Epoch 59: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0508 - mae: 0.1789 - mse: 0.0508 - val_loss: 0.0618 - val_mae: 0.1944 - val_mse: 0.0618\n",
      "Epoch 60/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0511 - mae: 0.1773 - mse: 0.0511\n",
      "Epoch 60: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 243ms/step - loss: 0.0511 - mae: 0.1773 - mse: 0.0511 - val_loss: 0.0603 - val_mae: 0.1919 - val_mse: 0.0603\n",
      "Epoch 61/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0529 - mae: 0.1809 - mse: 0.0529\n",
      "Epoch 61: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 264ms/step - loss: 0.0529 - mae: 0.1809 - mse: 0.0529 - val_loss: 0.0602 - val_mae: 0.1917 - val_mse: 0.0602\n",
      "Epoch 62/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0494 - mae: 0.1752 - mse: 0.0494\n",
      "Epoch 62: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 260ms/step - loss: 0.0494 - mae: 0.1752 - mse: 0.0494 - val_loss: 0.0606 - val_mae: 0.1923 - val_mse: 0.0606\n",
      "Epoch 63/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0497 - mae: 0.1770 - mse: 0.0497\n",
      "Epoch 63: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 253ms/step - loss: 0.0497 - mae: 0.1770 - mse: 0.0497 - val_loss: 0.0626 - val_mae: 0.1956 - val_mse: 0.0626\n",
      "Epoch 64/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0507 - mae: 0.1778 - mse: 0.0507\n",
      "Epoch 64: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 256ms/step - loss: 0.0507 - mae: 0.1778 - mse: 0.0507 - val_loss: 0.0632 - val_mae: 0.1967 - val_mse: 0.0632\n",
      "Epoch 65/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0498 - mae: 0.1773 - mse: 0.0498\n",
      "Epoch 65: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 244ms/step - loss: 0.0498 - mae: 0.1773 - mse: 0.0498 - val_loss: 0.0642 - val_mae: 0.1984 - val_mse: 0.0642\n",
      "Epoch 66/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0480 - mae: 0.1740 - mse: 0.0480\n",
      "Epoch 66: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 292ms/step - loss: 0.0480 - mae: 0.1740 - mse: 0.0480 - val_loss: 0.0642 - val_mae: 0.1985 - val_mse: 0.0642\n",
      "Epoch 67/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0513 - mae: 0.1767 - mse: 0.0513\n",
      "Epoch 67: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 243ms/step - loss: 0.0513 - mae: 0.1767 - mse: 0.0513 - val_loss: 0.0643 - val_mae: 0.1986 - val_mse: 0.0643\n",
      "Epoch 68/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0495 - mae: 0.1772 - mse: 0.0495\n",
      "Epoch 68: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 226ms/step - loss: 0.0495 - mae: 0.1772 - mse: 0.0495 - val_loss: 0.0643 - val_mae: 0.1983 - val_mse: 0.0643\n",
      "Epoch 69/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0508 - mae: 0.1795 - mse: 0.0508\n",
      "Epoch 69: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 242ms/step - loss: 0.0508 - mae: 0.1795 - mse: 0.0508 - val_loss: 0.0638 - val_mae: 0.1974 - val_mse: 0.0638\n",
      "Epoch 70/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0482 - mae: 0.1733 - mse: 0.0482\n",
      "Epoch 70: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 245ms/step - loss: 0.0482 - mae: 0.1733 - mse: 0.0482 - val_loss: 0.0624 - val_mae: 0.1950 - val_mse: 0.0624\n",
      "Epoch 71/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0489 - mae: 0.1746 - mse: 0.0489\n",
      "Epoch 71: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0489 - mae: 0.1746 - mse: 0.0489 - val_loss: 0.0612 - val_mae: 0.1930 - val_mse: 0.0612\n",
      "Epoch 72/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0489 - mae: 0.1756 - mse: 0.0489\n",
      "Epoch 72: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0489 - mae: 0.1756 - mse: 0.0489 - val_loss: 0.0605 - val_mae: 0.1919 - val_mse: 0.0605\n",
      "Epoch 73/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0498 - mae: 0.1772 - mse: 0.0498\n",
      "Epoch 73: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0498 - mae: 0.1772 - mse: 0.0498 - val_loss: 0.0603 - val_mae: 0.1916 - val_mse: 0.0603\n",
      "Epoch 74/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0483 - mae: 0.1754 - mse: 0.0483\n",
      "Epoch 74: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0483 - mae: 0.1754 - mse: 0.0483 - val_loss: 0.0595 - val_mae: 0.1903 - val_mse: 0.0595\n",
      "Epoch 75/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0489 - mae: 0.1754 - mse: 0.0489\n",
      "Epoch 75: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 243ms/step - loss: 0.0489 - mae: 0.1754 - mse: 0.0489 - val_loss: 0.0585 - val_mae: 0.1885 - val_mse: 0.0585\n",
      "Epoch 76/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0509 - mae: 0.1784 - mse: 0.0509\n",
      "Epoch 76: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0509 - mae: 0.1784 - mse: 0.0509 - val_loss: 0.0575 - val_mae: 0.1867 - val_mse: 0.0575\n",
      "Epoch 77/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0483 - mae: 0.1764 - mse: 0.0483\n",
      "Epoch 77: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0483 - mae: 0.1764 - mse: 0.0483 - val_loss: 0.0572 - val_mae: 0.1865 - val_mse: 0.0572\n",
      "Epoch 78/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0515 - mae: 0.1787 - mse: 0.0515\n",
      "Epoch 78: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0515 - mae: 0.1787 - mse: 0.0515 - val_loss: 0.0578 - val_mae: 0.1875 - val_mse: 0.0578\n",
      "Epoch 79/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0471 - mae: 0.1712 - mse: 0.0471\n",
      "Epoch 79: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0471 - mae: 0.1712 - mse: 0.0471 - val_loss: 0.0588 - val_mae: 0.1892 - val_mse: 0.0588\n",
      "Epoch 80/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0487 - mae: 0.1770 - mse: 0.0487\n",
      "Epoch 80: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0487 - mae: 0.1770 - mse: 0.0487 - val_loss: 0.0604 - val_mae: 0.1920 - val_mse: 0.0604\n",
      "Epoch 81/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0508 - mae: 0.1784 - mse: 0.0508\n",
      "Epoch 81: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0508 - mae: 0.1784 - mse: 0.0508 - val_loss: 0.0616 - val_mae: 0.1941 - val_mse: 0.0616\n",
      "Epoch 82/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0516 - mae: 0.1815 - mse: 0.0516\n",
      "Epoch 82: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0516 - mae: 0.1815 - mse: 0.0516 - val_loss: 0.0619 - val_mae: 0.1945 - val_mse: 0.0619\n",
      "Epoch 83/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0497 - mae: 0.1784 - mse: 0.0497\n",
      "Epoch 83: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 252ms/step - loss: 0.0497 - mae: 0.1784 - mse: 0.0497 - val_loss: 0.0622 - val_mae: 0.1951 - val_mse: 0.0622\n",
      "Epoch 84/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0506 - mae: 0.1792 - mse: 0.0506\n",
      "Epoch 84: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0506 - mae: 0.1792 - mse: 0.0506 - val_loss: 0.0634 - val_mae: 0.1972 - val_mse: 0.0634\n",
      "Epoch 85/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0486 - mae: 0.1739 - mse: 0.0486\n",
      "Epoch 85: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0486 - mae: 0.1739 - mse: 0.0486 - val_loss: 0.0651 - val_mae: 0.2001 - val_mse: 0.0651\n",
      "Epoch 86/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0491 - mae: 0.1744 - mse: 0.0491\n",
      "Epoch 86: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.0491 - mae: 0.1744 - mse: 0.0491 - val_loss: 0.0654 - val_mae: 0.2008 - val_mse: 0.0654\n",
      "Epoch 87/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0475 - mae: 0.1713 - mse: 0.0475\n",
      "Epoch 87: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0475 - mae: 0.1713 - mse: 0.0475 - val_loss: 0.0638 - val_mae: 0.1980 - val_mse: 0.0638\n",
      "Epoch 88/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0508 - mae: 0.1790 - mse: 0.0508\n",
      "Epoch 88: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 240ms/step - loss: 0.0508 - mae: 0.1790 - mse: 0.0508 - val_loss: 0.0624 - val_mae: 0.1956 - val_mse: 0.0624\n",
      "Epoch 89/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0507 - mae: 0.1787 - mse: 0.0507\n",
      "Epoch 89: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 225ms/step - loss: 0.0507 - mae: 0.1787 - mse: 0.0507 - val_loss: 0.0608 - val_mae: 0.1929 - val_mse: 0.0608\n",
      "Epoch 90/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0477 - mae: 0.1738 - mse: 0.0477\n",
      "Epoch 90: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0477 - mae: 0.1738 - mse: 0.0477 - val_loss: 0.0598 - val_mae: 0.1911 - val_mse: 0.0598\n",
      "Epoch 91/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0525 - mae: 0.1821 - mse: 0.0525\n",
      "Epoch 91: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 245ms/step - loss: 0.0525 - mae: 0.1821 - mse: 0.0525 - val_loss: 0.0605 - val_mae: 0.1921 - val_mse: 0.0605\n",
      "Epoch 92/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0483 - mae: 0.1752 - mse: 0.0483\n",
      "Epoch 92: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 241ms/step - loss: 0.0483 - mae: 0.1752 - mse: 0.0483 - val_loss: 0.0623 - val_mae: 0.1949 - val_mse: 0.0623\n",
      "Epoch 93/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0483 - mae: 0.1756 - mse: 0.0483\n",
      "Epoch 93: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 226ms/step - loss: 0.0483 - mae: 0.1756 - mse: 0.0483 - val_loss: 0.0644 - val_mae: 0.1985 - val_mse: 0.0644\n",
      "Epoch 94/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0512 - mae: 0.1828 - mse: 0.0512\n",
      "Epoch 94: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 240ms/step - loss: 0.0512 - mae: 0.1828 - mse: 0.0512 - val_loss: 0.0660 - val_mae: 0.2016 - val_mse: 0.0660\n",
      "Epoch 95/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0505 - mae: 0.1799 - mse: 0.0505\n",
      "Epoch 95: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0505 - mae: 0.1799 - mse: 0.0505 - val_loss: 0.0666 - val_mae: 0.2030 - val_mse: 0.0666\n",
      "Epoch 96/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0512 - mae: 0.1791 - mse: 0.0512\n",
      "Epoch 96: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0512 - mae: 0.1791 - mse: 0.0512 - val_loss: 0.0656 - val_mae: 0.2011 - val_mse: 0.0656\n",
      "Epoch 97/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0498 - mae: 0.1784 - mse: 0.0498\n",
      "Epoch 97: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 227ms/step - loss: 0.0498 - mae: 0.1784 - mse: 0.0498 - val_loss: 0.0643 - val_mae: 0.1989 - val_mse: 0.0643\n",
      "Epoch 98/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0469 - mae: 0.1716 - mse: 0.0469\n",
      "Epoch 98: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0469 - mae: 0.1716 - mse: 0.0469 - val_loss: 0.0628 - val_mae: 0.1962 - val_mse: 0.0628\n",
      "Epoch 99/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0470 - mae: 0.1737 - mse: 0.0470\n",
      "Epoch 99: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0470 - mae: 0.1737 - mse: 0.0470 - val_loss: 0.0613 - val_mae: 0.1935 - val_mse: 0.0613\n",
      "Epoch 100/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0477 - mae: 0.1722 - mse: 0.0477\n",
      "Epoch 100: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 227ms/step - loss: 0.0477 - mae: 0.1722 - mse: 0.0477 - val_loss: 0.0603 - val_mae: 0.1917 - val_mse: 0.0603\n",
      "Epoch 101/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0493 - mae: 0.1753 - mse: 0.0493\n",
      "Epoch 101: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 239ms/step - loss: 0.0493 - mae: 0.1753 - mse: 0.0493 - val_loss: 0.0595 - val_mae: 0.1903 - val_mse: 0.0595\n",
      "Epoch 102/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0482 - mae: 0.1742 - mse: 0.0482\n",
      "Epoch 102: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 225ms/step - loss: 0.0482 - mae: 0.1742 - mse: 0.0482 - val_loss: 0.0587 - val_mae: 0.1890 - val_mse: 0.0587\n",
      "Epoch 103/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0474 - mae: 0.1730 - mse: 0.0474\n",
      "Epoch 103: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0474 - mae: 0.1730 - mse: 0.0474 - val_loss: 0.0579 - val_mae: 0.1874 - val_mse: 0.0579\n",
      "Epoch 104/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0484 - mae: 0.1733 - mse: 0.0484\n",
      "Epoch 104: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0484 - mae: 0.1733 - mse: 0.0484 - val_loss: 0.0578 - val_mae: 0.1873 - val_mse: 0.0578\n",
      "Epoch 105/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0523 - mae: 0.1822 - mse: 0.0523\n",
      "Epoch 105: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0523 - mae: 0.1822 - mse: 0.0523 - val_loss: 0.0583 - val_mae: 0.1880 - val_mse: 0.0583\n",
      "Epoch 106/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0483 - mae: 0.1725 - mse: 0.0483\n",
      "Epoch 106: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 239ms/step - loss: 0.0483 - mae: 0.1725 - mse: 0.0483 - val_loss: 0.0587 - val_mae: 0.1887 - val_mse: 0.0587\n",
      "Epoch 107/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0498 - mae: 0.1756 - mse: 0.0498\n",
      "Epoch 107: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 224ms/step - loss: 0.0498 - mae: 0.1756 - mse: 0.0498 - val_loss: 0.0584 - val_mae: 0.1883 - val_mse: 0.0584\n",
      "Epoch 108/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0492 - mae: 0.1771 - mse: 0.0492\n",
      "Epoch 108: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 239ms/step - loss: 0.0492 - mae: 0.1771 - mse: 0.0492 - val_loss: 0.0582 - val_mae: 0.1881 - val_mse: 0.0582\n",
      "Epoch 109/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0510 - mae: 0.1789 - mse: 0.0510\n",
      "Epoch 109: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 224ms/step - loss: 0.0510 - mae: 0.1789 - mse: 0.0510 - val_loss: 0.0584 - val_mae: 0.1884 - val_mse: 0.0584\n",
      "Epoch 110/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0496 - mae: 0.1752 - mse: 0.0496\n",
      "Epoch 110: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0496 - mae: 0.1752 - mse: 0.0496 - val_loss: 0.0585 - val_mae: 0.1885 - val_mse: 0.0585\n",
      "Epoch 111/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0481 - mae: 0.1747 - mse: 0.0481\n",
      "Epoch 111: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0481 - mae: 0.1747 - mse: 0.0481 - val_loss: 0.0591 - val_mae: 0.1895 - val_mse: 0.0591\n",
      "Epoch 112/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0472 - mae: 0.1724 - mse: 0.0472\n",
      "Epoch 112: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.0472 - mae: 0.1724 - mse: 0.0472 - val_loss: 0.0591 - val_mae: 0.1893 - val_mse: 0.0591\n",
      "Epoch 113/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0469 - mae: 0.1713 - mse: 0.0469\n",
      "Epoch 113: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0469 - mae: 0.1713 - mse: 0.0469 - val_loss: 0.0585 - val_mae: 0.1883 - val_mse: 0.0585\n",
      "Epoch 114/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0498 - mae: 0.1759 - mse: 0.0498\n",
      "Epoch 114: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0498 - mae: 0.1759 - mse: 0.0498 - val_loss: 0.0583 - val_mae: 0.1879 - val_mse: 0.0583\n",
      "Epoch 115/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0495 - mae: 0.1754 - mse: 0.0495\n",
      "Epoch 115: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 241ms/step - loss: 0.0495 - mae: 0.1754 - mse: 0.0495 - val_loss: 0.0584 - val_mae: 0.1882 - val_mse: 0.0584\n",
      "Epoch 116/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0486 - mae: 0.1747 - mse: 0.0486\n",
      "Epoch 116: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 227ms/step - loss: 0.0486 - mae: 0.1747 - mse: 0.0486 - val_loss: 0.0586 - val_mae: 0.1886 - val_mse: 0.0586\n",
      "Epoch 117/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0499 - mae: 0.1784 - mse: 0.0499\n",
      "Epoch 117: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0499 - mae: 0.1784 - mse: 0.0499 - val_loss: 0.0592 - val_mae: 0.1895 - val_mse: 0.0592\n",
      "Epoch 118/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0489 - mae: 0.1752 - mse: 0.0489\n",
      "Epoch 118: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 224ms/step - loss: 0.0489 - mae: 0.1752 - mse: 0.0489 - val_loss: 0.0612 - val_mae: 0.1931 - val_mse: 0.0612\n",
      "Epoch 119/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0481 - mae: 0.1747 - mse: 0.0481\n",
      "Epoch 119: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0481 - mae: 0.1747 - mse: 0.0481 - val_loss: 0.0622 - val_mae: 0.1949 - val_mse: 0.0622\n",
      "Epoch 120/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0513 - mae: 0.1807 - mse: 0.0513\n",
      "Epoch 120: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 240ms/step - loss: 0.0513 - mae: 0.1807 - mse: 0.0513 - val_loss: 0.0618 - val_mae: 0.1943 - val_mse: 0.0618\n",
      "Epoch 121/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0464 - mae: 0.1716 - mse: 0.0464\n",
      "Epoch 121: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 242ms/step - loss: 0.0464 - mae: 0.1716 - mse: 0.0464 - val_loss: 0.0615 - val_mae: 0.1937 - val_mse: 0.0615\n",
      "Epoch 122/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0476 - mae: 0.1717 - mse: 0.0476\n",
      "Epoch 122: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 245ms/step - loss: 0.0476 - mae: 0.1717 - mse: 0.0476 - val_loss: 0.0606 - val_mae: 0.1921 - val_mse: 0.0606\n",
      "Epoch 123/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0468 - mae: 0.1711 - mse: 0.0468\n",
      "Epoch 123: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0468 - mae: 0.1711 - mse: 0.0468 - val_loss: 0.0606 - val_mae: 0.1920 - val_mse: 0.0606\n",
      "Epoch 124/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0497 - mae: 0.1770 - mse: 0.0497\n",
      "Epoch 124: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 240ms/step - loss: 0.0497 - mae: 0.1770 - mse: 0.0497 - val_loss: 0.0616 - val_mae: 0.1938 - val_mse: 0.0616\n",
      "Epoch 125/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0471 - mae: 0.1735 - mse: 0.0471\n",
      "Epoch 125: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 241ms/step - loss: 0.0471 - mae: 0.1735 - mse: 0.0471 - val_loss: 0.0623 - val_mae: 0.1949 - val_mse: 0.0623\n",
      "Epoch 126/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0479 - mae: 0.1732 - mse: 0.0479\n",
      "Epoch 126: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 226ms/step - loss: 0.0479 - mae: 0.1732 - mse: 0.0479 - val_loss: 0.0632 - val_mae: 0.1967 - val_mse: 0.0632\n",
      "Epoch 127/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0487 - mae: 0.1762 - mse: 0.0487\n",
      "Epoch 127: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 268ms/step - loss: 0.0487 - mae: 0.1762 - mse: 0.0487 - val_loss: 0.0637 - val_mae: 0.1978 - val_mse: 0.0637\n",
      "Epoch 128/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0476 - mae: 0.1734 - mse: 0.0476\n",
      "Epoch 128: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0476 - mae: 0.1734 - mse: 0.0476 - val_loss: 0.0623 - val_mae: 0.1951 - val_mse: 0.0623\n",
      "Epoch 129/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0475 - mae: 0.1738 - mse: 0.0475\n",
      "Epoch 129: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 246ms/step - loss: 0.0475 - mae: 0.1738 - mse: 0.0475 - val_loss: 0.0607 - val_mae: 0.1921 - val_mse: 0.0607\n",
      "Epoch 130/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0479 - mae: 0.1734 - mse: 0.0479\n",
      "Epoch 130: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0479 - mae: 0.1734 - mse: 0.0479 - val_loss: 0.0602 - val_mae: 0.1911 - val_mse: 0.0602\n",
      "Epoch 131/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0490 - mae: 0.1757 - mse: 0.0490\n",
      "Epoch 131: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0490 - mae: 0.1757 - mse: 0.0490 - val_loss: 0.0599 - val_mae: 0.1905 - val_mse: 0.0599\n",
      "Epoch 132/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0489 - mae: 0.1757 - mse: 0.0489\n",
      "Epoch 132: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 227ms/step - loss: 0.0489 - mae: 0.1757 - mse: 0.0489 - val_loss: 0.0602 - val_mae: 0.1909 - val_mse: 0.0602\n",
      "Epoch 133/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0472 - mae: 0.1701 - mse: 0.0472\n",
      "Epoch 133: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0472 - mae: 0.1701 - mse: 0.0472 - val_loss: 0.0617 - val_mae: 0.1934 - val_mse: 0.0617\n",
      "Epoch 134/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0474 - mae: 0.1732 - mse: 0.0474\n",
      "Epoch 134: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0474 - mae: 0.1732 - mse: 0.0474 - val_loss: 0.0625 - val_mae: 0.1946 - val_mse: 0.0625\n",
      "Epoch 135/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0472 - mae: 0.1745 - mse: 0.0472\n",
      "Epoch 135: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 239ms/step - loss: 0.0472 - mae: 0.1745 - mse: 0.0472 - val_loss: 0.0626 - val_mae: 0.1947 - val_mse: 0.0626\n",
      "Epoch 136/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0485 - mae: 0.1734 - mse: 0.0485\n",
      "Epoch 136: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.0485 - mae: 0.1734 - mse: 0.0485 - val_loss: 0.0620 - val_mae: 0.1939 - val_mse: 0.0620\n",
      "Epoch 137/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0485 - mae: 0.1745 - mse: 0.0485\n",
      "Epoch 137: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 226ms/step - loss: 0.0485 - mae: 0.1745 - mse: 0.0485 - val_loss: 0.0613 - val_mae: 0.1927 - val_mse: 0.0613\n",
      "Epoch 138/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0473 - mae: 0.1723 - mse: 0.0473\n",
      "Epoch 138: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0473 - mae: 0.1723 - mse: 0.0473 - val_loss: 0.0606 - val_mae: 0.1917 - val_mse: 0.0606\n",
      "Epoch 139/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0463 - mae: 0.1696 - mse: 0.0463\n",
      "Epoch 139: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 244ms/step - loss: 0.0463 - mae: 0.1696 - mse: 0.0463 - val_loss: 0.0605 - val_mae: 0.1918 - val_mse: 0.0605\n",
      "Epoch 140/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0481 - mae: 0.1726 - mse: 0.0481\n",
      "Epoch 140: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 226ms/step - loss: 0.0481 - mae: 0.1726 - mse: 0.0481 - val_loss: 0.0616 - val_mae: 0.1936 - val_mse: 0.0616\n",
      "Epoch 141/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0474 - mae: 0.1712 - mse: 0.0474\n",
      "Epoch 141: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.0474 - mae: 0.1712 - mse: 0.0474 - val_loss: 0.0627 - val_mae: 0.1955 - val_mse: 0.0627\n",
      "Epoch 142/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0521 - mae: 0.1799 - mse: 0.0521\n",
      "Epoch 142: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0521 - mae: 0.1799 - mse: 0.0521 - val_loss: 0.0638 - val_mae: 0.1975 - val_mse: 0.0638\n",
      "Epoch 143/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0472 - mae: 0.1733 - mse: 0.0472\n",
      "Epoch 143: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 241ms/step - loss: 0.0472 - mae: 0.1733 - mse: 0.0472 - val_loss: 0.0643 - val_mae: 0.1986 - val_mse: 0.0643\n",
      "Epoch 144/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0479 - mae: 0.1718 - mse: 0.0479\n",
      "Epoch 144: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 224ms/step - loss: 0.0479 - mae: 0.1718 - mse: 0.0479 - val_loss: 0.0639 - val_mae: 0.1977 - val_mse: 0.0639\n",
      "Epoch 145/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0467 - mae: 0.1702 - mse: 0.0467\n",
      "Epoch 145: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 279ms/step - loss: 0.0467 - mae: 0.1702 - mse: 0.0467 - val_loss: 0.0622 - val_mae: 0.1948 - val_mse: 0.0622\n",
      "Epoch 146/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0486 - mae: 0.1741 - mse: 0.0486\n",
      "Epoch 146: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 225ms/step - loss: 0.0486 - mae: 0.1741 - mse: 0.0486 - val_loss: 0.0608 - val_mae: 0.1924 - val_mse: 0.0608\n",
      "Epoch 147/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0482 - mae: 0.1721 - mse: 0.0482\n",
      "Epoch 147: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.0482 - mae: 0.1721 - mse: 0.0482 - val_loss: 0.0598 - val_mae: 0.1907 - val_mse: 0.0598\n",
      "Epoch 148/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0451 - mae: 0.1687 - mse: 0.0451\n",
      "Epoch 148: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0451 - mae: 0.1687 - mse: 0.0451 - val_loss: 0.0599 - val_mae: 0.1908 - val_mse: 0.0599\n",
      "Epoch 149/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0474 - mae: 0.1733 - mse: 0.0474\n",
      "Epoch 149: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0474 - mae: 0.1733 - mse: 0.0474 - val_loss: 0.0599 - val_mae: 0.1909 - val_mse: 0.0599\n",
      "Epoch 150/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0485 - mae: 0.1734 - mse: 0.0485\n",
      "Epoch 150: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 243ms/step - loss: 0.0485 - mae: 0.1734 - mse: 0.0485 - val_loss: 0.0592 - val_mae: 0.1898 - val_mse: 0.0592\n",
      "Epoch 151/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0470 - mae: 0.1718 - mse: 0.0470\n",
      "Epoch 151: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 224ms/step - loss: 0.0470 - mae: 0.1718 - mse: 0.0470 - val_loss: 0.0585 - val_mae: 0.1887 - val_mse: 0.0585\n",
      "Epoch 152/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0488 - mae: 0.1748 - mse: 0.0488\n",
      "Epoch 152: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0488 - mae: 0.1748 - mse: 0.0488 - val_loss: 0.0584 - val_mae: 0.1885 - val_mse: 0.0584\n",
      "Epoch 153/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0500 - mae: 0.1760 - mse: 0.0500\n",
      "Epoch 153: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 241ms/step - loss: 0.0500 - mae: 0.1760 - mse: 0.0500 - val_loss: 0.0590 - val_mae: 0.1895 - val_mse: 0.0590\n",
      "Epoch 154/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0479 - mae: 0.1738 - mse: 0.0479\n",
      "Epoch 154: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0479 - mae: 0.1738 - mse: 0.0479 - val_loss: 0.0602 - val_mae: 0.1917 - val_mse: 0.0602\n",
      "Epoch 155/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0469 - mae: 0.1711 - mse: 0.0469\n",
      "Epoch 155: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0469 - mae: 0.1711 - mse: 0.0469 - val_loss: 0.0608 - val_mae: 0.1927 - val_mse: 0.0608\n",
      "Epoch 156/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0485 - mae: 0.1759 - mse: 0.0485\n",
      "Epoch 156: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0485 - mae: 0.1759 - mse: 0.0485 - val_loss: 0.0611 - val_mae: 0.1934 - val_mse: 0.0611\n",
      "Epoch 157/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0489 - mae: 0.1770 - mse: 0.0489\n",
      "Epoch 157: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0489 - mae: 0.1770 - mse: 0.0489 - val_loss: 0.0612 - val_mae: 0.1936 - val_mse: 0.0612\n",
      "Epoch 158/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0481 - mae: 0.1738 - mse: 0.0481\n",
      "Epoch 158: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 245ms/step - loss: 0.0481 - mae: 0.1738 - mse: 0.0481 - val_loss: 0.0610 - val_mae: 0.1934 - val_mse: 0.0610\n",
      "Epoch 159/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0467 - mae: 0.1713 - mse: 0.0467\n",
      "Epoch 159: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0467 - mae: 0.1713 - mse: 0.0467 - val_loss: 0.0608 - val_mae: 0.1929 - val_mse: 0.0608\n",
      "Epoch 160/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0481 - mae: 0.1721 - mse: 0.0481\n",
      "Epoch 160: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 239ms/step - loss: 0.0481 - mae: 0.1721 - mse: 0.0481 - val_loss: 0.0611 - val_mae: 0.1935 - val_mse: 0.0611\n",
      "Epoch 161/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0464 - mae: 0.1690 - mse: 0.0464\n",
      "Epoch 161: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 270ms/step - loss: 0.0464 - mae: 0.1690 - mse: 0.0464 - val_loss: 0.0617 - val_mae: 0.1946 - val_mse: 0.0617\n",
      "Epoch 162/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0475 - mae: 0.1713 - mse: 0.0475\n",
      "Epoch 162: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0475 - mae: 0.1713 - mse: 0.0475 - val_loss: 0.0622 - val_mae: 0.1953 - val_mse: 0.0622\n",
      "Epoch 163/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0447 - mae: 0.1684 - mse: 0.0447\n",
      "Epoch 163: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0447 - mae: 0.1684 - mse: 0.0447 - val_loss: 0.0614 - val_mae: 0.1938 - val_mse: 0.0614\n",
      "Epoch 164/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0478 - mae: 0.1745 - mse: 0.0478\n",
      "Epoch 164: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0478 - mae: 0.1745 - mse: 0.0478 - val_loss: 0.0613 - val_mae: 0.1935 - val_mse: 0.0613\n",
      "Epoch 165/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0479 - mae: 0.1745 - mse: 0.0479\n",
      "Epoch 165: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0479 - mae: 0.1745 - mse: 0.0479 - val_loss: 0.0609 - val_mae: 0.1928 - val_mse: 0.0609\n",
      "Epoch 166/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0485 - mae: 0.1758 - mse: 0.0485\n",
      "Epoch 166: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0485 - mae: 0.1758 - mse: 0.0485 - val_loss: 0.0618 - val_mae: 0.1943 - val_mse: 0.0618\n",
      "Epoch 167/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0457 - mae: 0.1687 - mse: 0.0457\n",
      "Epoch 167: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 240ms/step - loss: 0.0457 - mae: 0.1687 - mse: 0.0457 - val_loss: 0.0619 - val_mae: 0.1946 - val_mse: 0.0619\n",
      "Epoch 168/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0480 - mae: 0.1746 - mse: 0.0480\n",
      "Epoch 168: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 246ms/step - loss: 0.0480 - mae: 0.1746 - mse: 0.0480 - val_loss: 0.0613 - val_mae: 0.1936 - val_mse: 0.0613\n",
      "Epoch 169/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0475 - mae: 0.1723 - mse: 0.0475\n",
      "Epoch 169: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 243ms/step - loss: 0.0475 - mae: 0.1723 - mse: 0.0475 - val_loss: 0.0616 - val_mae: 0.1942 - val_mse: 0.0616\n",
      "Epoch 170/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0459 - mae: 0.1694 - mse: 0.0459\n",
      "Epoch 170: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 239ms/step - loss: 0.0459 - mae: 0.1694 - mse: 0.0459 - val_loss: 0.0611 - val_mae: 0.1934 - val_mse: 0.0611\n",
      "Epoch 171/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0468 - mae: 0.1717 - mse: 0.0468\n",
      "Epoch 171: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 246ms/step - loss: 0.0468 - mae: 0.1717 - mse: 0.0468 - val_loss: 0.0594 - val_mae: 0.1902 - val_mse: 0.0594\n",
      "Epoch 172/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0460 - mae: 0.1690 - mse: 0.0460\n",
      "Epoch 172: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 226ms/step - loss: 0.0460 - mae: 0.1690 - mse: 0.0460 - val_loss: 0.0585 - val_mae: 0.1886 - val_mse: 0.0585\n",
      "Epoch 173/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0490 - mae: 0.1747 - mse: 0.0490\n",
      "Epoch 173: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 241ms/step - loss: 0.0490 - mae: 0.1747 - mse: 0.0490 - val_loss: 0.0576 - val_mae: 0.1868 - val_mse: 0.0576\n",
      "Epoch 174/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0502 - mae: 0.1771 - mse: 0.0502\n",
      "Epoch 174: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0502 - mae: 0.1771 - mse: 0.0502 - val_loss: 0.0585 - val_mae: 0.1881 - val_mse: 0.0585\n",
      "Epoch 175/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0489 - mae: 0.1743 - mse: 0.0489\n",
      "Epoch 175: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0489 - mae: 0.1743 - mse: 0.0489 - val_loss: 0.0608 - val_mae: 0.1919 - val_mse: 0.0608\n",
      "Epoch 176/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0450 - mae: 0.1682 - mse: 0.0450\n",
      "Epoch 176: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0450 - mae: 0.1682 - mse: 0.0450 - val_loss: 0.0613 - val_mae: 0.1928 - val_mse: 0.0613\n",
      "Epoch 177/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0474 - mae: 0.1725 - mse: 0.0474\n",
      "Epoch 177: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 241ms/step - loss: 0.0474 - mae: 0.1725 - mse: 0.0474 - val_loss: 0.0621 - val_mae: 0.1944 - val_mse: 0.0621\n",
      "Epoch 178/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0476 - mae: 0.1727 - mse: 0.0476\n",
      "Epoch 178: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 215ms/step - loss: 0.0476 - mae: 0.1727 - mse: 0.0476 - val_loss: 0.0616 - val_mae: 0.1937 - val_mse: 0.0616\n",
      "Epoch 179/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0475 - mae: 0.1717 - mse: 0.0475\n",
      "Epoch 179: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 251ms/step - loss: 0.0475 - mae: 0.1717 - mse: 0.0475 - val_loss: 0.0613 - val_mae: 0.1934 - val_mse: 0.0613\n",
      "Epoch 180/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0488 - mae: 0.1742 - mse: 0.0488\n",
      "Epoch 180: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 249ms/step - loss: 0.0488 - mae: 0.1742 - mse: 0.0488 - val_loss: 0.0613 - val_mae: 0.1935 - val_mse: 0.0613\n",
      "Epoch 181/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0483 - mae: 0.1730 - mse: 0.0483\n",
      "Epoch 181: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 227ms/step - loss: 0.0483 - mae: 0.1730 - mse: 0.0483 - val_loss: 0.0613 - val_mae: 0.1937 - val_mse: 0.0613\n",
      "Epoch 182/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0472 - mae: 0.1711 - mse: 0.0472\n",
      "Epoch 182: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0472 - mae: 0.1711 - mse: 0.0472 - val_loss: 0.0617 - val_mae: 0.1943 - val_mse: 0.0617\n",
      "Epoch 183/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0480 - mae: 0.1760 - mse: 0.0480\n",
      "Epoch 183: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0480 - mae: 0.1760 - mse: 0.0480 - val_loss: 0.0616 - val_mae: 0.1940 - val_mse: 0.0616\n",
      "Epoch 184/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0494 - mae: 0.1756 - mse: 0.0494\n",
      "Epoch 184: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 242ms/step - loss: 0.0494 - mae: 0.1756 - mse: 0.0494 - val_loss: 0.0601 - val_mae: 0.1911 - val_mse: 0.0601\n",
      "Epoch 185/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0463 - mae: 0.1715 - mse: 0.0463\n",
      "Epoch 185: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 267ms/step - loss: 0.0463 - mae: 0.1715 - mse: 0.0463 - val_loss: 0.0598 - val_mae: 0.1904 - val_mse: 0.0598\n",
      "Epoch 186/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0466 - mae: 0.1713 - mse: 0.0466\n",
      "Epoch 186: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 295ms/step - loss: 0.0466 - mae: 0.1713 - mse: 0.0466 - val_loss: 0.0598 - val_mae: 0.1905 - val_mse: 0.0598\n",
      "Epoch 187/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0482 - mae: 0.1737 - mse: 0.0482\n",
      "Epoch 187: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 248ms/step - loss: 0.0482 - mae: 0.1737 - mse: 0.0482 - val_loss: 0.0606 - val_mae: 0.1921 - val_mse: 0.0606\n",
      "Epoch 188/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0478 - mae: 0.1737 - mse: 0.0478\n",
      "Epoch 188: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 278ms/step - loss: 0.0478 - mae: 0.1737 - mse: 0.0478 - val_loss: 0.0614 - val_mae: 0.1937 - val_mse: 0.0614\n",
      "Epoch 189/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0476 - mae: 0.1732 - mse: 0.0476\n",
      "Epoch 189: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 322ms/step - loss: 0.0476 - mae: 0.1732 - mse: 0.0476 - val_loss: 0.0623 - val_mae: 0.1957 - val_mse: 0.0623\n",
      "Epoch 190/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0502 - mae: 0.1776 - mse: 0.0502\n",
      "Epoch 190: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 289ms/step - loss: 0.0502 - mae: 0.1776 - mse: 0.0502 - val_loss: 0.0615 - val_mae: 0.1942 - val_mse: 0.0615\n",
      "Epoch 191/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0459 - mae: 0.1689 - mse: 0.0459\n",
      "Epoch 191: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 254ms/step - loss: 0.0459 - mae: 0.1689 - mse: 0.0459 - val_loss: 0.0605 - val_mae: 0.1924 - val_mse: 0.0605\n",
      "Epoch 192/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0469 - mae: 0.1740 - mse: 0.0469\n",
      "Epoch 192: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 242ms/step - loss: 0.0469 - mae: 0.1740 - mse: 0.0469 - val_loss: 0.0604 - val_mae: 0.1920 - val_mse: 0.0604\n",
      "Epoch 193/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0469 - mae: 0.1721 - mse: 0.0469\n",
      "Epoch 193: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 256ms/step - loss: 0.0469 - mae: 0.1721 - mse: 0.0469 - val_loss: 0.0614 - val_mae: 0.1936 - val_mse: 0.0614\n",
      "Epoch 194/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0474 - mae: 0.1706 - mse: 0.0474\n",
      "Epoch 194: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0474 - mae: 0.1706 - mse: 0.0474 - val_loss: 0.0635 - val_mae: 0.1973 - val_mse: 0.0635\n",
      "Epoch 195/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0478 - mae: 0.1744 - mse: 0.0478\n",
      "Epoch 195: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 252ms/step - loss: 0.0478 - mae: 0.1744 - mse: 0.0478 - val_loss: 0.0652 - val_mae: 0.2005 - val_mse: 0.0652\n",
      "Epoch 196/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0471 - mae: 0.1750 - mse: 0.0471\n",
      "Epoch 196: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0471 - mae: 0.1750 - mse: 0.0471 - val_loss: 0.0655 - val_mae: 0.2013 - val_mse: 0.0655\n",
      "Epoch 197/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0484 - mae: 0.1740 - mse: 0.0484\n",
      "Epoch 197: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0484 - mae: 0.1740 - mse: 0.0484 - val_loss: 0.0646 - val_mae: 0.1999 - val_mse: 0.0646\n",
      "Epoch 198/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0473 - mae: 0.1741 - mse: 0.0473\n",
      "Epoch 198: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 243ms/step - loss: 0.0473 - mae: 0.1741 - mse: 0.0473 - val_loss: 0.0630 - val_mae: 0.1971 - val_mse: 0.0630\n",
      "Epoch 199/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0476 - mae: 0.1745 - mse: 0.0476\n",
      "Epoch 199: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 240ms/step - loss: 0.0476 - mae: 0.1745 - mse: 0.0476 - val_loss: 0.0623 - val_mae: 0.1959 - val_mse: 0.0623\n",
      "Epoch 200/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0461 - mae: 0.1710 - mse: 0.0461\n",
      "Epoch 200: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0461 - mae: 0.1710 - mse: 0.0461 - val_loss: 0.0625 - val_mae: 0.1964 - val_mse: 0.0625\n",
      "Epoch 201/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0470 - mae: 0.1708 - mse: 0.0470\n",
      "Epoch 201: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0470 - mae: 0.1708 - mse: 0.0470 - val_loss: 0.0624 - val_mae: 0.1959 - val_mse: 0.0624\n",
      "Epoch 202/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0457 - mae: 0.1695 - mse: 0.0457\n",
      "Epoch 202: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 293ms/step - loss: 0.0457 - mae: 0.1695 - mse: 0.0457 - val_loss: 0.0616 - val_mae: 0.1942 - val_mse: 0.0616\n",
      "Epoch 203/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0485 - mae: 0.1750 - mse: 0.0485\n",
      "Epoch 203: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 243ms/step - loss: 0.0485 - mae: 0.1750 - mse: 0.0485 - val_loss: 0.0606 - val_mae: 0.1924 - val_mse: 0.0606\n",
      "Epoch 204/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0470 - mae: 0.1722 - mse: 0.0470\n",
      "Epoch 204: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 227ms/step - loss: 0.0470 - mae: 0.1722 - mse: 0.0470 - val_loss: 0.0601 - val_mae: 0.1913 - val_mse: 0.0601\n",
      "Epoch 205/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0462 - mae: 0.1706 - mse: 0.0462\n",
      "Epoch 205: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 247ms/step - loss: 0.0462 - mae: 0.1706 - mse: 0.0462 - val_loss: 0.0596 - val_mae: 0.1902 - val_mse: 0.0596\n",
      "Epoch 206/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0480 - mae: 0.1738 - mse: 0.0480\n",
      "Epoch 206: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 239ms/step - loss: 0.0480 - mae: 0.1738 - mse: 0.0480 - val_loss: 0.0589 - val_mae: 0.1890 - val_mse: 0.0589\n",
      "Epoch 207/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0461 - mae: 0.1698 - mse: 0.0461\n",
      "Epoch 207: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 245ms/step - loss: 0.0461 - mae: 0.1698 - mse: 0.0461 - val_loss: 0.0583 - val_mae: 0.1882 - val_mse: 0.0583\n",
      "Epoch 208/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0495 - mae: 0.1772 - mse: 0.0495\n",
      "Epoch 208: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0495 - mae: 0.1772 - mse: 0.0495 - val_loss: 0.0586 - val_mae: 0.1889 - val_mse: 0.0586\n",
      "Epoch 209/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0482 - mae: 0.1754 - mse: 0.0482\n",
      "Epoch 209: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 241ms/step - loss: 0.0482 - mae: 0.1754 - mse: 0.0482 - val_loss: 0.0587 - val_mae: 0.1892 - val_mse: 0.0587\n",
      "Epoch 210/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0465 - mae: 0.1701 - mse: 0.0465\n",
      "Epoch 210: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 260ms/step - loss: 0.0465 - mae: 0.1701 - mse: 0.0465 - val_loss: 0.0581 - val_mae: 0.1882 - val_mse: 0.0581\n",
      "Epoch 211/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0458 - mae: 0.1682 - mse: 0.0458\n",
      "Epoch 211: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.0458 - mae: 0.1682 - mse: 0.0458 - val_loss: 0.0583 - val_mae: 0.1884 - val_mse: 0.0583\n",
      "Epoch 212/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0473 - mae: 0.1740 - mse: 0.0473\n",
      "Epoch 212: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0473 - mae: 0.1740 - mse: 0.0473 - val_loss: 0.0583 - val_mae: 0.1883 - val_mse: 0.0583\n",
      "Epoch 213/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0460 - mae: 0.1715 - mse: 0.0460\n",
      "Epoch 213: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0460 - mae: 0.1715 - mse: 0.0460 - val_loss: 0.0592 - val_mae: 0.1899 - val_mse: 0.0592\n",
      "Epoch 214/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0469 - mae: 0.1709 - mse: 0.0469\n",
      "Epoch 214: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 284ms/step - loss: 0.0469 - mae: 0.1709 - mse: 0.0469 - val_loss: 0.0601 - val_mae: 0.1915 - val_mse: 0.0601\n",
      "Epoch 215/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0449 - mae: 0.1684 - mse: 0.0449\n",
      "Epoch 215: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0449 - mae: 0.1684 - mse: 0.0449 - val_loss: 0.0614 - val_mae: 0.1937 - val_mse: 0.0614\n",
      "Epoch 216/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0463 - mae: 0.1735 - mse: 0.0463\n",
      "Epoch 216: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 227ms/step - loss: 0.0463 - mae: 0.1735 - mse: 0.0463 - val_loss: 0.0634 - val_mae: 0.1973 - val_mse: 0.0634\n",
      "Epoch 217/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0471 - mae: 0.1723 - mse: 0.0471\n",
      "Epoch 217: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0471 - mae: 0.1723 - mse: 0.0471 - val_loss: 0.0640 - val_mae: 0.1985 - val_mse: 0.0640\n",
      "Epoch 218/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0483 - mae: 0.1746 - mse: 0.0483\n",
      "Epoch 218: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 261ms/step - loss: 0.0483 - mae: 0.1746 - mse: 0.0483 - val_loss: 0.0627 - val_mae: 0.1964 - val_mse: 0.0627\n",
      "Epoch 219/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0462 - mae: 0.1711 - mse: 0.0462\n",
      "Epoch 219: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 285ms/step - loss: 0.0462 - mae: 0.1711 - mse: 0.0462 - val_loss: 0.0613 - val_mae: 0.1939 - val_mse: 0.0613\n",
      "Epoch 220/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0475 - mae: 0.1733 - mse: 0.0475\n",
      "Epoch 220: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0475 - mae: 0.1733 - mse: 0.0475 - val_loss: 0.0596 - val_mae: 0.1907 - val_mse: 0.0596\n",
      "Epoch 221/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0480 - mae: 0.1750 - mse: 0.0480\n",
      "Epoch 221: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0480 - mae: 0.1750 - mse: 0.0480 - val_loss: 0.0583 - val_mae: 0.1882 - val_mse: 0.0583\n",
      "Epoch 222/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0462 - mae: 0.1688 - mse: 0.0462\n",
      "Epoch 222: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 241ms/step - loss: 0.0462 - mae: 0.1688 - mse: 0.0462 - val_loss: 0.0580 - val_mae: 0.1876 - val_mse: 0.0580\n",
      "Epoch 223/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0451 - mae: 0.1660 - mse: 0.0451\n",
      "Epoch 223: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 256ms/step - loss: 0.0451 - mae: 0.1660 - mse: 0.0451 - val_loss: 0.0588 - val_mae: 0.1889 - val_mse: 0.0588\n",
      "Epoch 224/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0449 - mae: 0.1649 - mse: 0.0449\n",
      "Epoch 224: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 246ms/step - loss: 0.0449 - mae: 0.1649 - mse: 0.0449 - val_loss: 0.0599 - val_mae: 0.1910 - val_mse: 0.0599\n",
      "Epoch 225/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0461 - mae: 0.1698 - mse: 0.0461\n",
      "Epoch 225: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0461 - mae: 0.1698 - mse: 0.0461 - val_loss: 0.0607 - val_mae: 0.1927 - val_mse: 0.0607\n",
      "Epoch 226/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0471 - mae: 0.1735 - mse: 0.0471\n",
      "Epoch 226: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 311ms/step - loss: 0.0471 - mae: 0.1735 - mse: 0.0471 - val_loss: 0.0620 - val_mae: 0.1951 - val_mse: 0.0620\n",
      "Epoch 227/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0458 - mae: 0.1692 - mse: 0.0458\n",
      "Epoch 227: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0458 - mae: 0.1692 - mse: 0.0458 - val_loss: 0.0633 - val_mae: 0.1975 - val_mse: 0.0633\n",
      "Epoch 228/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0461 - mae: 0.1722 - mse: 0.0461\n",
      "Epoch 228: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.0461 - mae: 0.1722 - mse: 0.0461 - val_loss: 0.0642 - val_mae: 0.1990 - val_mse: 0.0642\n",
      "Epoch 229/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0453 - mae: 0.1678 - mse: 0.0453\n",
      "Epoch 229: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 246ms/step - loss: 0.0453 - mae: 0.1678 - mse: 0.0453 - val_loss: 0.0657 - val_mae: 0.2020 - val_mse: 0.0657\n",
      "Epoch 230/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0483 - mae: 0.1745 - mse: 0.0483\n",
      "Epoch 230: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0483 - mae: 0.1745 - mse: 0.0483 - val_loss: 0.0664 - val_mae: 0.2033 - val_mse: 0.0664\n",
      "Epoch 231/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0482 - mae: 0.1738 - mse: 0.0482\n",
      "Epoch 231: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 244ms/step - loss: 0.0482 - mae: 0.1738 - mse: 0.0482 - val_loss: 0.0659 - val_mae: 0.2024 - val_mse: 0.0659\n",
      "Epoch 232/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0489 - mae: 0.1770 - mse: 0.0489\n",
      "Epoch 232: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.0489 - mae: 0.1770 - mse: 0.0489 - val_loss: 0.0653 - val_mae: 0.2013 - val_mse: 0.0653\n",
      "Epoch 233/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0466 - mae: 0.1709 - mse: 0.0466\n",
      "Epoch 233: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0466 - mae: 0.1709 - mse: 0.0466 - val_loss: 0.0653 - val_mae: 0.2015 - val_mse: 0.0653\n",
      "Epoch 234/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0472 - mae: 0.1713 - mse: 0.0472\n",
      "Epoch 234: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0472 - mae: 0.1713 - mse: 0.0472 - val_loss: 0.0652 - val_mae: 0.2015 - val_mse: 0.0652\n",
      "Epoch 235/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0450 - mae: 0.1695 - mse: 0.0450\n",
      "Epoch 235: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0450 - mae: 0.1695 - mse: 0.0450 - val_loss: 0.0654 - val_mae: 0.2019 - val_mse: 0.0654\n",
      "Epoch 236/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0471 - mae: 0.1735 - mse: 0.0471\n",
      "Epoch 236: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 224ms/step - loss: 0.0471 - mae: 0.1735 - mse: 0.0471 - val_loss: 0.0648 - val_mae: 0.2011 - val_mse: 0.0648\n",
      "Epoch 237/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0462 - mae: 0.1714 - mse: 0.0462\n",
      "Epoch 237: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 256ms/step - loss: 0.0462 - mae: 0.1714 - mse: 0.0462 - val_loss: 0.0636 - val_mae: 0.1987 - val_mse: 0.0636\n",
      "Epoch 238/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0446 - mae: 0.1670 - mse: 0.0446\n",
      "Epoch 238: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 226ms/step - loss: 0.0446 - mae: 0.1670 - mse: 0.0446 - val_loss: 0.0626 - val_mae: 0.1967 - val_mse: 0.0626\n",
      "Epoch 239/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0474 - mae: 0.1734 - mse: 0.0474\n",
      "Epoch 239: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0474 - mae: 0.1734 - mse: 0.0474 - val_loss: 0.0625 - val_mae: 0.1964 - val_mse: 0.0625\n",
      "Epoch 240/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0461 - mae: 0.1685 - mse: 0.0461\n",
      "Epoch 240: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 246ms/step - loss: 0.0461 - mae: 0.1685 - mse: 0.0461 - val_loss: 0.0632 - val_mae: 0.1977 - val_mse: 0.0632\n",
      "Epoch 241/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0447 - mae: 0.1677 - mse: 0.0447\n",
      "Epoch 241: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 244ms/step - loss: 0.0447 - mae: 0.1677 - mse: 0.0447 - val_loss: 0.0650 - val_mae: 0.2011 - val_mse: 0.0650\n",
      "Epoch 242/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0472 - mae: 0.1718 - mse: 0.0472\n",
      "Epoch 242: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0472 - mae: 0.1718 - mse: 0.0472 - val_loss: 0.0674 - val_mae: 0.2056 - val_mse: 0.0674\n",
      "Epoch 243/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0447 - mae: 0.1696 - mse: 0.0447\n",
      "Epoch 243: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 248ms/step - loss: 0.0447 - mae: 0.1696 - mse: 0.0447 - val_loss: 0.0678 - val_mae: 0.2064 - val_mse: 0.0678\n",
      "Epoch 244/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0448 - mae: 0.1683 - mse: 0.0448\n",
      "Epoch 244: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0448 - mae: 0.1683 - mse: 0.0448 - val_loss: 0.0656 - val_mae: 0.2025 - val_mse: 0.0656\n",
      "Epoch 245/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0472 - mae: 0.1716 - mse: 0.0472\n",
      "Epoch 245: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0472 - mae: 0.1716 - mse: 0.0472 - val_loss: 0.0626 - val_mae: 0.1969 - val_mse: 0.0626\n",
      "Epoch 246/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0458 - mae: 0.1681 - mse: 0.0458\n",
      "Epoch 246: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0458 - mae: 0.1681 - mse: 0.0458 - val_loss: 0.0610 - val_mae: 0.1938 - val_mse: 0.0610\n",
      "Epoch 247/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0463 - mae: 0.1712 - mse: 0.0463\n",
      "Epoch 247: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0463 - mae: 0.1712 - mse: 0.0463 - val_loss: 0.0597 - val_mae: 0.1913 - val_mse: 0.0597\n",
      "Epoch 248/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0471 - mae: 0.1723 - mse: 0.0471\n",
      "Epoch 248: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 227ms/step - loss: 0.0471 - mae: 0.1723 - mse: 0.0471 - val_loss: 0.0608 - val_mae: 0.1932 - val_mse: 0.0608\n",
      "Epoch 249/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0469 - mae: 0.1711 - mse: 0.0469\n",
      "Epoch 249: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 242ms/step - loss: 0.0469 - mae: 0.1711 - mse: 0.0469 - val_loss: 0.0629 - val_mae: 0.1970 - val_mse: 0.0629\n",
      "Epoch 250/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0457 - mae: 0.1683 - mse: 0.0457\n",
      "Epoch 250: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 0.0457 - mae: 0.1683 - mse: 0.0457 - val_loss: 0.0657 - val_mae: 0.2021 - val_mse: 0.0657\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs     = 250\n",
    "history = model.fit(x_trainset, y_trainset,\n",
    "                   epochs = epochs,\n",
    "                   callbacks = [mc],\n",
    "                   validation_data = (x_valset, y_valset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "1788d100-9f66-4b14-835d-5136438f405f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f51ecd969d0>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'], label = 'Tr. Loss')\n",
    "plt.plot(history.history['val_loss'], label = 'Val. Loss')\n",
    "plt.ylim(0,1)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "da817465-472d-4425-adc7-2b6f7b535eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 29ms/step\n"
     ]
    }
   ],
   "source": [
    "y_predicted = model.predict(x_testset) * (max_y - min_y) + min_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "01f58734-d799-402d-a00b-e16406935ca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = 4\n",
    "plt.plot(10**(y_predicted[i,:]))\n",
    "plt.plot(10**(y_testset[i,:] * (max_y - min_y) + min_y), c = 'black')\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "1e25d925-9e58-423c-99f4-645e717c76e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f51ed230bb0>]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAySklEQVR4nO3deXxU1fn48c8zmewbgYSQBRLCGmQNiMimVhTUCipudalbtWpta2v7/dn6/Xbx21Vr1farrbYubbXuG7QoCqK0KEuC7EsSQkIIkAQSErInM+f3RyY0xiyTZPZ53q9XXkzuvXPvk2HyzMk55z5HjDEopZQKXBZvB6CUUsq9NNErpVSA00SvlFIBThO9UkoFOE30SikV4KzeDqCrxMREk5mZ6e0wlFLKr+Tl5R03xiR1t8/nEn1mZia5ubneDkMppfyKiJT0tE+7bpRSKsBpoldKqQCniV4ppQKcJnqllApwmuiVUirAaaJXSqkAp4leKaUCnCb6AGG3G97+rIyK2iZvh6KU8jGa6ANAq83Ofa9t595XtvGr9/Z5OxyllI/RRO/nGlrauP2vubz1WRmjhkaxetcxmlpt3g5LKeVDNNH7sZMNLVz/502sz6/kF5dP4VdXTKG+xcbavRXeDk0p5UN8rtaNcs7Rmka++sxmSk408OT1OSyZnILNbhgeG84728q4ZGqKt0NUSvkIbdH7ocKKOpY/+QlHa5p4/tYzWTK5PamHWIRLp6Xy0f5KahpavRylUspXaKL3M9tKT3LVHz+hxWZ4+Y45zB2T+Ln9y6an0mKz897uo16KUCnlazTR+5H1+ZVc96eNxEaE8sZdZzM5Lf4Lx0xJi2d0YjTvbDvihQiVUr5IE72feGdbGbc+v4WMYdG8ftfZZAyL7vY4EWHptFQ+LTpBuc6pV0qhid4vPLfhIN9+eRszMxJ45etzGB4b0evxS6enYgys3K6teqWUJnqfZozhN6v389OVe7hwUjJ/uXU2cRGhfT5vTFIMU9LiWaGJXimFJnqfZbMbfvjWTv5vXSHXnjmSJ6/PISI0xOnnL5ueyo7DNRRV1rkxSqWUP9BE74OaWm3c/WIeL20u5Z7zxvLLK6ZgDenff9WXp6YigrbqlVKa6H1NbVMrNz27mdW7y/nxpZP43uIJiEi/zzMiPoI5o4exYtsRjDFuiFQp5S800fuQilNNXPvURvJKqnn82uncMm/0oM63bHoqRcfr2VVW66IIlVL+SBO9j2iz2fnK0xs5eLyeZ24+k2XT0wZ9zosmpxAaIry9rcwFESql/JUmeh+x79gpDlTW8+CyMzhnfJJLzhkfFcq5E4azcvsRbHbtvlEqWGmi9xF5JdUAzB2b2MeR/bNseioVp5rZVHTCpedVSvkPTfQ+IrekmpT4CNKGRLr0vOdPTCY6LERLIigVxDTR+4i84ipmZiS4/LyRYSEsPmMEq3YdpblNFyRRKhhpovcBR042cqSmiVluSPTQXhLhVFMbH+2vdMv5lVK+TRO9D8h19M/PyhzqlvPPH5vIsOgwVmj3jVJBSRO9D8grriIqLISJI2Ldcn5riIUvT01hzd5yTjXpgiRKBRtN9D4gt6Sa6SOH9LvMQX8snZ5Gc5ud93eXu+0aSinfpIney+qa29h7tNZt/fMdckYNIT0hkne09o1SQUcTvZdtLz2J3cBMN/XPdxARlk1PZUPhcSpPNbv1Wkop36KJ3styi6sRgRmjhrj9Wsump2GzG1bt1PVklQommui9LLekignJsU4tKDJY45NjmTgilne09o1SQUUTvRfZ7IbPDp1kVqZ7++c7WzY9ja2HTnLoRIPHrqmU8i5N9F60/9gp6prb3HJHbE8unZYCwModOiirVLDQRO9FeSVVAMzKcO9AbGfpCVGcmZnA25+V6YIkSgUJTfRelFtSzfDYcNITXFvIrC9Lp6dRUFHHvmOnPHpdpZR3aKL3otziamZlJgxoqcDBuGRKClaLaEVLpYKEJnovOVbTRNnJRmZ6sNumw9DoMBaMS2Tl9iPYdUESpQKeU4leRJaIyH4RKRSR+7vZ/10R2SMiO0RkrYhkdNkfJyKHReT/XBW4v+tYaMTdd8T2ZNn0NMpONpJ3qNor11dKeU6fiV5EQoAngIuAScBXRGRSl8M+A2YZY6YCrwMPddn/v8D6wYcbOHJLqogItTApNc4r179gUjIRoRadU69UEHCmRT8bKDTGFBljWoCXgWWdDzDGrDPGdEzM3gikd+wTkZlAMvC+a0IODHkl1UxLH0KoGwuZ9SY63MoFk0bwzx1HabXZvRKDUsoznMkyaUBpp+8PO7b15DbgXQARsQCPAN/r7QIicoeI5IpIbmVl4C+O0dDSxu4jtR69Uao7y6alUt3Qyr8KAv81VyqYubQ5KSI3ALOAhx2b7gZWGWMO9/Y8Y8zTxphZxphZSUlJrgzJJ20rPYnNbjw6f747C8cnER8ZqrNvlApwVieOKQNGdvo+3bHtc0RkEfAAcI4xpqM84tnAAhG5G4gBwkSkzhjzhQHdYJJX3D4AmjPKuy36MKuFi6ek8PZnZTS0tBEV5szbQSnlb5xp0W8BxonIaBEJA64FVnQ+QERmAE8BS40xFR3bjTHXG2NGGWMyae+++WuwJ3lov1FqfHIM8VHuL2TWl2XTU2lstfHBHl2QRKlA1WeiN8a0AfcAq4G9wKvGmN0i8qCILHUc9jDtLfbXRGSbiKzo4XRBz243bD1U7ZX5892ZnTmUEXERup6sUgHMqb/VjTGrgFVdtv2o0+NFTpzjeeD5/oUXeAoq6jjV1Oa1+fNdWSzC0umpPPvvg1TXt5AQHebtkJRSLqZ3xnpYbkchMy/PuOls6bRU2uyGVbt0QRKlApEmeg/LK64mMSaMUUOjvB3KaWekxjEmKVpn3ygVoDTRe1huSTUzMzxfyKw37evJprH5YBVHTjZ6OxyllItpovegilNNHKpq8Pr8+e4snZYKwG/e3693yioVYDTRe1DH/PmZPtQ/3yEzMZp7zhvLm1vLuPm5zdQ0tHo7JKWUi2ii96C8kmrCrRYmp8Z7O5RufW/xBB6+ciqbD1Zx2ZMbOFBZ5+2QlFIuoIneg3IdhczCrL77sl81ayQv3T6H2sZWLn9ig9bBUSoA+G7GCTBNrTZ2H6nxyW6brmZlDuXtb8wjdUgkNz+3hb9+WuztkJRSg6CJ3kO2l56k1WaY6eX6Ns4aOTSK1++ay3kThvOjd3bz32/v1EFapfyUJnoPyXWsKDXTR+6IdUZMuJWnbpzJneeM4YWNh7jp2c2cbGjxdlhKqX7SRO8heSXVjEmK9rsSAyEW4f6LJvLIVdPILa7msic2UFihg7RK+RNN9A4tbXZu+PMmPs53/eCj3W7IK6n2yfnzzlo+M52X7jiLuuY2Ln9yA+vd8DoppdxDE71D0fE6/l14nJ+u2E2bi/uiD1TWUdPY6hcDsb2ZmdE+SJs2JJKbn9vMcxsOYozxdlhKqT5ooncoKG/vjig6Xs+bW127YHaeo3/eVypWDkZ6QhRv3DWXRdnJ/HTlHn741i4dpFXKx2midyisqEOkvcDX42sLaG6zuezcuSXVDI0OY3RitMvO6U3R4Vb+eMNM7j53DC9tPsSNz2yiul4HaZXyVZroHQor6xiZEMUPLsqm7GQjL2065LJz55VUkzPKtwqZDZbFIvzXkok8es00th46yWVPbqCw4pS3w1JKdUMTvUNheR1jh8cwb+wwzs4axv+tK6ShpW3Q5z1e18zB4/U+VX/elS6fkc7Ld8yhvtnG5U98wmeHqr0dklKqC030QJvNzsHj9YwdHoOI8L3FEzhe18JzG4oHfe5A6p/vSc6oBN65Zx4Wi/DSZtf9JaSUcg1N9EBpdSMtNjtjh8cA7Tc1LcoezlMfH6CmcXBVHPNKqgkLsTA5zTcLmblK2pBIpqTFs/eodt8o5Ws00cPpG4A6Ej3Ady+YQG1TG39aXzSoc+cWVzElPZ6I0JBBnccfZKfEsr/8lMunpyqlBkcTPVDgGETsnOgnpcZx6bRUnt1wkMpTzQM6b1OrjV1ltQHdbdNZdkocLW3t3WBKKd+hiZ72Fn1yXDhxEaGf2/6dReNobrPz5EeFAzrvrrIaWmx2v6pvMxjZKXEA7D2m3TdK+RJN9MCBirrPteY7ZCXFcNXMdF7ceIiyAayl2lHILCdIEv2YpBhCQ4S9R2u9HYpSqpOgT/TGGAor6hib9MVED/DN88cB8Pu1Bf0+d25xNaMTo0mMCR9UjP4izGphTFKMJnqlfEzQJ/qjNU3Ut9i6bdFD+2yS6+eM4rW8wxT1Y2k9YwxbD1UHTbdNh0kpcZrolfIxQZ/o/zPjJrbHY+4+dyzhVguPrnG+VV90vJ6q+pagGYjtkJ0SR3ltM1VaEkEpn6GJvpuplV0lxYZz67zRrNx+hD1HnGut5hU7bpQK0DtiezIxpf0Dc5+26pXyGUGf6Asq6oiPDCUxpvcFQW5fmEVchJVH3t/v1HlzS6oYEhVKVmLPHyCBqGPmzR5N9Er5jKBP9B0zbvoqOBYfGcrXzxnD2n0Vp8sa9Ca3pJqZoxKwWAKnkJkzEmPCSYoN1ztklfIhQZ/oCyvrGNdLt01nt8zLJDEmjIdX7+t1wY2q+haKKuuDZlplV9k6IKuUTwnqRH+irn3QsLf++c6iwqzcc95YNhZVsaHwRI/HbQ2CQma9yR4RS2FFnS5IopSPCOpE3zEQO8bJRA/wlbNGkTYkstdWfW5JNaEhwrSRQ1wRpt/JTomjxWanqFJLISjlC4I70TvmxTvbdQMQbg3h2+ePY/vhGt7fU97tMXklVZyRGhyFzLpzuhSCdt8o5ROCO9FX1BEZGkJqfGS/nndFThpZidH89v18bPbPt+qb22xsP1wTtN02AFlJ0YSFWDTRK+Ujgj7Rjxke3e+ZMdYQC9+9cDz7y0+xcvuRz+3bVVZLS5s96ObPdxYaYmFccoxOsVTKRwR9ou+pxk1fLp6cwqSUOH77Qf7nBh3zSqqA4Clk1pOJI+LYp1UslfIJQZvo65rbOFrTxLjknksf9MZiEb6/eAKHqhp4Nbf09Pbc4mpGDY1ieGyEq0L1S9kpsVSeauZ43cBq+SulXCdoE/2Bjhk3A2zRA5w7IYmZGQn8fm0hTa2204XMgrl/vsMkHZBVymc4lehFZImI7BeRQhG5v5v93xWRPSKyQ0TWikiGY/t0EflURHY79l3j6h9goAqcqHHTF5H2Vv2x2iZe2FhCyYkGjte1MDOI++c7TNREr5TPsPZ1gIiEAE8AFwCHgS0issIYs6fTYZ8Bs4wxDSJyF/AQcA3QAHzVGFMgIqlAnoisNsacdPUP0l+FFXWEhggZw6IGdZ45WcNYMC6RJ9YVEhrS/rk5K2OoK0L0a0Ojw0iOC2eflkJQyuucadHPBgqNMUXGmBbgZWBZ5wOMMeuMMQ2ObzcC6Y7t+caYAsfjI0AFkOSq4AejsKKOzGHRp5PzYHx/8QSqG1p56L19xEVY+zUvP5Blp8TpzBulfIAzWS4NKO30/WHHtp7cBrzbdaOIzAbCgAPd7LtDRHJFJLeystKJkAbvQGX3ywcOxNT0ISw+I5n6Fhs5GcFXyKwn2SlxHKiso6VNSyEo5U0uHYwVkRuAWcDDXbanAH8DbjHGfOG33hjztDFmljFmVlKS+xv8Ta02Sk7UuyzRA9x34QRCLMKcrGEuO6e/y06Jo9VmTpeaUEp5R5999EAZMLLT9+mObZ8jIouAB4BzjDHNnbbHAf8EHjDGbBxcuK5RfKIeuxncQGxX45Nj+eA7C0lL6N9dtoEse0T71NW9R2uZlBrn5WiUCl7OtOi3AONEZLSIhAHXAis6HyAiM4CngKXGmIpO28OAt4C/GmNed13Yg+PMqlIDkZUUQ7g1OOvbdGd0YjRhVgv7jmk/vVLe1GeiN8a0AfcAq4G9wKvGmN0i8qCILHUc9jAQA7wmIttEpOOD4GpgIXCzY/s2EZnu8p+inwor6hAZ3Bx61TdriIUJybG6CIlSXuZM1w3GmFXAqi7bftTp8aIenvcC8MJgAnSHgoo6RiZEBW11SU/KToll7d4KjDF9ruKllHKPoLwztmP5QOV+E0fEcaK+hcpTWgpBKW8JukRvsxuKjrt2xo3q2ena9FrgTCmvCbpEX1rVQEubfcBVK1X/aM0bpbwv6BL96Ro3yZroPSE+KpTU+AhN9Ep5UdAlendNrVQ9m5gSp4leKS8KykQ/PDacuIhQb4cSNLJTYjlQWU9Tq83boSgVlIIw0Z/S1ryHZafEYbNrKQSlvCWoEr0xhgOV9Vpd0sOydUBWKa8KqkR/rLaJuuY2bdF7WOawaCJCLXqHrFJeElSJvqPrYIwmeo8KsYijFIK26JXyhqBK9AXl7Yl+3PCBLQiuBi47JY59x2oxxng7FKWCTlAl+sLKOuIjQ0mMCfN2KEEnOyWO6oZWymu1FIJSnhZcid5R40aLa3meDsgq5T1BlegPVNRp6QMvmeBYhETXkFXK84Im0VfVt3CivoVxWvrAK+IjQ0kbEqkteqW8IGgSvc648b72AVmdYqmUpwVdoteuG++ZlBJLUWWdlkJQysOCJtEXVJwiMjSEtCG6eLe3TEyJw24gv1xb9Up5UtAk+sKKOsYMj8Zi0Rk33qIzb5TyjqBJ9DrjxvsyhkYRFRaipRCU8rCgSPR1zW0cqWnSGjdeZrEIE0ZoKQSlPC0oEv2B04uNaOkDb5s4on0REi2FoJTnBEWi11WlfMeklFhqm9r/wlJKeUZwJPrKOqwWIWNYlLdDCXodA7L7tPtGKY8JjkRfUUdmYjShIUHx4/q0jlII2k+vlOcEReYrrKjTVaV8RGxEKCOHRurMG6U8KOATfXObjZIT9do/70OyHQOySinPCPhEX3y8AbvRgVhfkp0Sx8ET9TS0tHk7FKWCQsAnep1x43uyU+IwBvIdK34ppdwr4BN9QcUpRGCM3hXrMyZpKQSlPCrgE31hRR3pCZFEhIZ4OxTlkJ4QSXRYiCZ6pTwkKBK91rjxLRaLMDFFB2SV8pSATvQ2u6HoeD3jkrX0ga/JToll39FTWgpBKQ8I6ERfWtVAS5tdW/Q+KDsljlPNbRyubvR2KEoFvIBO9Lp8oO+aOEIHZJXylMBO9JU6tdJXTRwRiwh6h6xSHhDQib6gvI7hseHER4Z6OxTVRXS4lYyhUdqiV8oDAjrRF1bWaWveh2WnxLHvmCZ6pdwtYBO9MaZ9+UBN9D5r4og4SqoaqG/WUghKuZNTiV5ElojIfhEpFJH7u9n/XRHZIyI7RGStiGR02neTiBQ4vm5yZfC9Ka9tpq65TatW+rDslFiMgX3HtJ9eKXfqM9GLSAjwBHARMAn4iohM6nLYZ8AsY8xU4HXgIcdzhwI/Bs4CZgM/FpEE14Xfs4KK9uShM258V7aWQlDKI5xp0c8GCo0xRcaYFuBlYFnnA4wx64wxDY5vNwLpjseLgQ+MMVXGmGrgA2CJa0LvnRYz833pCZHEhlu1n14pN3Mm0acBpZ2+P+zY1pPbgHf781wRuUNEckUkt7Ky0omQ+lZYUUdchJWkmHCXnE+5nogwMSVWp1gq5WYuHYwVkRuAWcDD/XmeMeZpY8wsY8yspKQkl8RSUFHHuORYRMQl51PukZ0Sx76jtdjtWgpBKXdxJtGXASM7fZ/u2PY5IrIIeABYaoxp7s9z3eGAFjPzC9kpcdS32Citbuj7YKXUgDiT6LcA40RktIiEAdcCKzofICIzgKdoT/IVnXatBi4UkQTHIOyFjm1uVV3fwon6Fu2f9wM6IKuU+/WZ6I0xbcA9tCfovcCrxpjdIvKgiCx1HPYwEAO8JiLbRGSF47lVwP/S/mGxBXjQsc2tTpc+SNZE7+vGJ8doKQSl3MzqzEHGmFXAqi7bftTp8aJenvss8OxAAxyIAscSddp14/uiwqyMHhatLXql3Cgg74wtrKgjMjSEtCGR3g5FOSE7JY69OsVSKbcJzERfWUdWUjQWi8648QfZKbGUVjVyqqnV26EoFZACMtEfqKjT0gd+pGNAdr+WQlDKLQIu0dc3t1F2slFn3PiRiQEw86bsZCNXPLmBp9cf8HYoSn2BU4Ox/uSALjbid1LjI4iLsLLHT2fe7DlSyy3Pb6a8tplDVQ3cMm80oSEB14ZSfizg3o3/qXGjC4L7CxFpH5D1wxb9hsLjXP3UpwjC9xdP4HhdC+vzXVPGQylXCchEb7UIGcOivB2K6ofslDj2HzuFzY9KIbyzrYybn9tM2pBI3vrGXO5YmMWw6DDe2HrY26Ep9TkBl+gLKurITIzWP539THZKLI2tNg5V+X4pBGMMf/z4AN9+eRszMxJ49c6zSYmPJDTEwtLpqazZU8HJhhZvh6nUaQGXDbXGjX/yl1IINrvhpyv38Kt39/HlqSn85dbZn1uTeHlOOi02Oyt3HPVilEp9XkAl+uY2GyVVDYzT0gd+Z3xyLBaBHYdrvB1Kj5pabXzjxa08/0kxty8Yze+unUG4NeRzx5yRGsfEEbG8nqfdN8p3BFSiLz7egM1udMaNH4oIDWHe2ETe/qyMNpvd2+F8wcmGFm58ZhOr9xzjf748iQcumdTtDXkiwvKcdLaXnjw9MUApbwuoRN/xizVGu2780o1zMjhW28SavRV9H+xBh6sbuPKPn7K9tIbff2UGt80f3evxy2akEmIRHZRVPiPgEr2IJnp/9aWJw0mNj+CFjSXeDuW0PUdqueLJTyivbeKvt83my1NT+3zO8NgIzhmfxFtby/xqFpEKXIGV6CvrSE+IJDIspO+Dlc+xhli47qxR/Lvw+Okb37ypY458iEV4/c65zMka5vRzl+ekc6y2iU8OHHdjhEo5J6ASfUH5KZ1x4+euPnMkoSHCixsPeTWOtz/7zxz5N++ey4QR/bsB7/zs4cRFWHnDg4Oyf/mkmLV7yz12PeU/AibR2+yGouP1OhDr54bHRrBkcgqv55XS2GLz+PU75sjf+8rn58j3V0RoCJdOS+W93cc8UpVz37FafrxiN9986TPKTja6/XrKvwRMoq841YRFYJyWPvB7N5w1itqmNlZuP+LR63aeI3/ptNQvzJHvr+Uz02lqtfPuzmMujLJ7j68pICbcijHwP2/vwhgdG1D/ETCJPiU+kj0/XcJlM9K8HYoapNmjhzI+OYa/biz2aMK679VtPP9JMXcszOLxa6Z/YY58f80YOYSsxGi3z6nfc6SWd3cd49Z5mdx34Xg+3FfBKg98uCj/ETCJHsBiEcKsAfUjBSUR4cY5Gewqq2W7h26g2lB4nLe3HeFbXxrLDy/OdsmiNSLC8pnpbC6u4tAJ95V2eHxtPrERVm6bn8XNczOZkhbPj1fspqZBF3JR7TQrKp902Yw0osNC+Nun7p9qaYzhodX7SY2P4O7zxrr03JfPSEMEt82p332khtW7y7l13mjio0Kxhlj45RVTqG5o4Vfv7XXLNZX/0USvfFJsRCiX56SxcscRquvdWyDs/T3lbC89yb2LxhMR6tqpualDIpk3JpE3PzuM3Q1z6h9fU0BshJVbO93ENTktntvmj+alzaVsLDrh8msq/6OJXvmsG+Zk0NJm57W8Urddw2Y3PPL+frISo7kixz3jO8tnplFa1ciW4iqXnndXWQ3v7ynntvmjvzBofO+icYwcGskP39pJU6vnZy8p36KJXvmsiSPimJ05lBc3HXJLaxhgxfYy8svr+O6F47G6qbT14jNGEB0W4vLum8fWFBDXpTXfISrMys8um0JRZT1PfqTLGwY7TfTKp10/ZxQlJxr4V6Hr7zBtabPz2w/yOSM1josnp7j8/B2iwqxcPCWFVTuP0dDS5pJz7jxcw5q95XxtQRZxEd1PAT1nfBKXTU/lDx8Vkl/un8s0KtfQRK982pLJI0iMCXPLoOwruaWUVjXyvcUTXDLLpjfLZ6ZT19zG6t2umfb42Jp84iNDuWVeZq/H/c+XJxEdbuUHb+50219FgaK0qoHD1b6/8M1AaKJXPi3cGsI1Z47kw33lLv0lbGyx8fu1BZyZmcC545Ncdt6ezM4cSnpCJG/klQ36XNtLT7J2XwW3LxhNbA+t+Q7DYsL570smkVdSzd83u6+shN1u+PO/ilz2QeZpTa02rnnqU7767OaA/EDURK983ldmjwLgJRcmqr98WkzFqWa+v3giIu5tzUP7PR7Lc9LZcOA4RwZZouCxNfkMiQrlprmZTh2/PCeNeWOH8et393GspmlQ1+5OS5ud77y6jZ/9cy/3vbqdE3XNLr+Guz274SBHapooqqxn7T7fKpPtCprolc9LT4jiSxOTeWVLKS1tg1+UpLaplT98dIBzJyQxe/RQF0TonOU56RgDb3028Fb9ttKTrNtfye0LsvpszXcQEX5+2RRabHZ+smL3gK/dndqmVm55fjPvbDvCzXMzaWy18diaApdew91O1DXz5LoDnDchibQhkfzpX0XeDsnlNNErv3Dj2Rkcr2vhPRd0DfxpfRE1ja1878IJLojMeaOGRTE7cyhvbD084NIOj63JJ6EfrfkOmYnR3LtoPO/tPuay7pXy2iau/uOnbCqq4pGrpvGTpWdw3exR/H3zIb9aXet3awtobLXxwCXZ3Dp/NJsPVrG99KS3w3IpTfTKLywYm0jGsCheGOSg7PG6Zp7590EumZrC5LR4F0XnvOUz0yiqrGfbABLJ1kPVfLS/ktsXZhETbu3387+2YDQTR8Tyo3d2DbqiZkH5KS5/YgOlVQ08e/OZLJ+ZDrTP348MDeFX7/rHXbkHKut4cdMhrj1zJGOHx3LNmSOJjbAGXKteE73yCxaLcP1Zo9hcXMW+Y7UDPs+T6w7Q3GbnuxeMd2F0zrt4SgoRoZYBzal/bE0BQ6PDuOnszAFdOzTEwq+WT6XiVDMPr94/oHMAbD5YxfI/fEKr3fDK189mYafB7GEx4dx93hjW7K3wi0VXfv3uPsKtFu5d1P5+iAm3ct1Zo1i18yilVYEzA0cTvfIbV80cSZjVMuClBstONvLCxhKuzEn32nKTsRGhLD5jBCu3H+3XHat5JdWsz6/kjoVZRA+gNd9h+sgh3Dw3k79tLCGvpLrfz1+18yg3PLOJxNhw3rxrbrd/Fd06bzRpQyL5xaq9Pj2DZfPBKt7fU85d544hKTb89Pab52ZiEeG5DcXeC87FNNErv5EQHcalU1N5a2vZgLoefucYJPzWonGuDq1fluekU9PYytp+LIL+2Jp8hkWH8dWzMwZ9/fsunEBKXAQ/eHNHvwa3n9twkG/8fStT0uJ54865jBwa1e1xEaEhfH/xBHaV1fL2tsFPJ3UHu93w83/uYURcBLfNz/rcvpT4SJZOS+WVLYeoaQyMCqCa6JVfufHsDOpbbLzdz5krByrreH3rYa6fM4q0If1fMcqV5o1NZERchNPdN7nFVfyr4DhfPyeLqLCBt+Y7xIRb+dnlk8kvr+Pp9X2XR7DbDb9ctZefrtzDBdnJvPi1s0iIDuv1OUunpTI1PZ6HV+/3ykphffnHzqNsP1zDfReO73aN6a8tyKK+xebSKb3epIle+ZVp6fFMSYvnbxtL+jVz5bcf5BNutfANF5chHogQi3B5Thof51dSearvOeePrSkgMSaMG+YMvjXf4UsTk7lkagq/+7CQol4WYm9us3HvK9t4an0RN87J4A83zHSqwqfFIjxwcTZHa5p4dsNBl8XtCs1tNh56bx/ZKXFckZPe7TGTUuOYPzaR5zYcdMmUXm/TRK/8SseiJPnldWwpdq6PeVdZDf/ccZTb5o8mMSa87yd4wPKcdGx2wzt9dG1sKa7i34XH+frCMS5pzXf240snEWG18IM3d3b7oVnb1MrNz25hxfYj/L8lE3lw2RmE9KNUxFlZw7hwUjJPrit06gPNU/76SQmHqxt54OLsXn+ery0YTXltM//Y4dklLd1BE73yO5dOSyUuwsrfnByUfeT9/cRHhvK1BVl9H+whY4fHMG3kkD6XGXz0g3wSY8Jd2prvMDw2gh9enM2mg1W8lvv5OI7VtM+R31JcxaPXTOOuc8cM6A7i+y+aSHObnUfX5Lsq7EGprm/h9x8WcM74JOaPS+z12HPGJzEhOZan1xf5/Rq8muiV34kMC+HKmSN5b9dRKk71fkv/luIq1u2v5K5zxwxqoW93uDInjX3HTrH7SPfLJW4qOsEnB05w5zlZ3fYju8LVs0Yye/RQfr5q7+lWd375Ka54cgOHqxt57pYzuXxG990bzshKiuGGORm8vPkQBT5QQfP3HxZS19zGDy/O7vNYEeG2BaPZd+wUGwr9ewEXTfTKL10/ZxStNsOrW3pelMQYw0Pv7WN4bPiA556706XTUgkLsfRY6OzRNfkkxbqnNd/BYhF+ecUUGltsPPiPPWwqOsGVp+fIz2HBuMEXfPvW+eOIDrfyi1XevYmq5EQ9f9tYzNWzRjJhRKxTz1k2PZWk2HCe9vMbqJxK9CKyRET2i0ihiNzfzf6FIrJVRNpE5Mou+x4Skd0isldEfieeqCClAt6YpBjmj03k75sO0WbrfrDs4/xKthRX883zx7mtRTwYQ6LCOD97OO9sK6O1y8/w6YETbCyq4q5zxrh8ecOuxiTFcM+XxrJy+xFueGYTSY458mekuubO4aHRYdxz3ljW7a/k3wXeu4nqoff2Y7VY+nWzXLg1hJvnZrI+v5L9x9z7F8knB47z6QH3/OXQZ6IXkRDgCeAiYBLwFRGZ1OWwQ8DNwN+7PHcuMA+YCkwGzgTOGXTUStG+1OCRmiY+7KbaoN1ueHj1fkYOjeSaWSO9EJ1zrpyZzon6Fj7aX3l6mzGGR9fkMzw2nOvOGuWROO48ZwxT0+PJGZXAG3f1PEd+oG6am0l6QiQ/X7UXmxduosorqeafO49yx8IshsdF9Ou51581isjQELeWRSg72cg9f/+MB/+xxy03mTnTop8NFBpjiowxLcDLwLLOBxhjio0xO4CuTSsDRABhQDgQCpQPOmqlgEXZwxkRF8ELm7441/ndXcfYfaSW7ywaT5jVd3soF45PIjEmjDc6Dcp+WnSCzQeruPtc97fmO4RZLbx99zxe+frZDInqfY78QESEhvBfSyay92gtb7p4ScW+GGP4xaq9JMWGc8fC/g/ID4kK4+pZ6byzrYyKWteXeW5us3H3C3m0tNl54roZblkEx5nfgDSgc0foYce2PhljPgXWAUcdX6uNMV/oqBORO0QkV0RyKysru+5WqlvWEAvXnTWK9fmVFB+vP729zWbnkQ/2Mz45hmXT3bPgt6uEhlhYNj2NtfvKqa5vwRjDYx8UkBwXzrWzPdOa7+DuVbYunZrC9JFD+M37+122pKIz3tt1jLySar57wfgBl4+4df5o2uyG5z8pdm1wwIMr97D9cA2/uWoaWW4qzeHWpo6IjAWygXTaPxy+JCILuh5njHnaGDPLGDMrKcn9q/2owHHtmSOxWoQXN/1nquWbW8soqqznvgsn9Gvet7csz0mn1WZYueMInxw4webiKu4+d6zHWvOeIiL89yXZlNc28+d/eeYmqpY2O796bx/jk2O4aubAZw9lDItmyRkjeHHTIeqbXfch9XreYV7cdIivn5PFkskjXHberpxJ9GVA507OdMc2Z1wObDTG1Blj6oB3gbP7F6JSPRseF8HiM0bwau5hmlptNLfZeGxNPtNGDuHCScneDs8pk1LjyE6J4428wzz6QT4j4iK45kzfHVcYjFmZQ7lo8gj++PEBt3SDdPXiphJKTjTwg4uzsYYMrl37tQVZ1DS28lpuzzO9+mP3kRoeeGsnZ2cN4/tuXhvBmZ98CzBOREaLSBhwLbDCyfMfAs4REauIhNI+EOsfhaqV37hhTgY1ja2s3H6EFzce4khNE/+1eIJHlgh0leU5aWw/XENuSTXfOM9zffPe8P+WTKTV5v6bqGoaW3l8bQHzxya6ZF3gmRkJzMxI4JkNBwc9oFzT0MqdL+SREBXG76+bMegPob70eXZjTBtwD7Ca9iT9qjFmt4g8KCJLAUTkTBE5DFwFPCUiHeuVvQ4cAHYC24HtxpiVbvg5VBCbkzWUscNjeHZDMU+sK2TumGHMG9v7XY++Ztn0NEIsQmp8BFcHaGu+Q2ZiNDfOyeSVLaVunbL45LpCahpb+cHFrlsX+PYFoymtahzUKl12u+E7r27jWE0TT1yf45GyHE6NTBhjVgGrumz7UafHW2jv0un6PBvw9UHGqFSvOurf/NixHur3Fnt2iUBXSIoN53+XTSZjWBTh1sBtzXf41vljeT2vlF+s2stfbp3t8vOXVjXw3CfFXDEj3WX3AwBcMGkEGcOieHp9ERdNHjGgD5D/W1fIh/sqeHDZGczMSHBZbL3x3XlnSvXD5TlpxIRbuWBSMjmjPPPL42rXnTXK7/4SGaghUWF86/xxfJxfyfp818+0+837+xHge4tdu5JYiEW4bf5otpWeHNDCLR/nV/Lomnwum57KjW6847krTfQqIMRFhPKPb87nt1dP83Yoykk3np3BqKFR/MLFN1HtOHySd7Yd4WsLRpMS7/q1B66cmc6QqNB+30BVWtXAt1/+jAnJsfziiikeHUPSRK8CRmZiNLERvlW4TPUs3BrC/RdNZN+xU7ye55qZLMYYfv7PvQyLDuPOc8a45JxdRYVZueGsDN7fU87BTvdv9Kap1cbdL27FZjP84YaZLi853RdN9Eopr7lo8ghmZiTwm/fzXTI/fc3eCjYdrOLeC8a79UP/q3MzCLVYePbfzt0P8JMVu9lZVsMjV09jdGK02+LqiSZ6pZTXiAgPXJJN5almnl4/uFoyrTY7v3x3L1lJ0Vzr5plLw2MjuGxGKq/llVJd39Lrsa9sOcTLW0q5+9wxXHiG+26K6o1n/35QSqkuckYlcMnUFJ5eX8R1Z40i2VF0zGY3tLTZaW6z0dxmp7m10+M2m+N7++nvPzt0kqLKev701VmEunleOrTfQPVq7mFe2FjCN8/vfsH5nYdr+J93djN/bCL3ufmmqN5ooldKed39Sybywe5yvvSbjxARmttstNr6P0B77oQkFmUPd0OEXzQ+OZZzJyTxl0+LuX1h1hducjvZ0MJdL+aRGB3G49dO92o5Dk30SimvGzk0ioevmkpucTVhVgvhVgvh1hDCQzs9tlr+sy80xLH9P8eFhVhIHRLp0dksdyzI4ro/b+KdbWVcc+Z/itDZ7YZvv7yNitpmXr3zbIZ5ea1iTfRKKZ+wbHqaz1cb7ersMcOYlBLHn/51kKtmjjxdAfTxtQV8nF/Jzy6bzPSRQ7wbJDoYq5RSAyYi3LEwi8KKOj523Pi1bl8Fv/uwgOU56VzvoYVj+qKJXimlBuGSqSmkxEfw9PoiSqsauPeVbUwcEcfPLpvsM4X1NNErpdQghIZYuGVeJp8WneD6P2/Cbgx/vCHHp9Yp1kSvlFKDdO3sUcSEWzlU1cBj10wnY5jnb4rqjQ7GKqXUIMVFhPLQlVNparVxfrbvLXijiV4ppVzg4ikp3g6hR9p1o5RSAU4TvVJKBThN9EopFeA00SulVIDTRK+UUgFOE71SSgU4TfRKKRXgNNErpVSAE2Nct/q6K4hIJVAyiFMkAsddFI47aHyDo/ENjsY3OL4cX4YxJqm7HT6X6AdLRHKNMbO8HUdPNL7B0fgGR+MbHF+PryfadaOUUgFOE71SSgW4QEz0T3s7gD5ofIOj8Q2Oxjc4vh5ftwKuj14ppdTnBWKLXimlVCea6JVSKsD5ZaIXkSUisl9ECkXk/m72h4vIK479m0Qk04OxjRSRdSKyR0R2i8i3uznmXBGpEZFtjq8feSq+TjEUi8hOx/Vzu9kvIvI7x2u4Q0RyPBjbhE6vzTYRqRWRe7sc49HXUESeFZEKEdnVadtQEflARAoc/yb08NybHMcUiMhNHozvYRHZ5/j/e0tEhvTw3F7fC26M7yciUtbp//DiHp7b6++7G+N7pVNsxSKyrYfnuv31GzRjjF99ASHAASALCAO2A5O6HHM38EfH42uBVzwYXwqQ43gcC+R3E9+5wD+8/DoWA4m97L8YeBcQYA6wyYv/38dovxnEa68hsBDIAXZ12vYQcL/j8f3Ar7t53lCgyPFvguNxgofiuxCwOh7/urv4nHkvuDG+nwDfc+L/v9ffd3fF12X/I8CPvPX6DfbLH1v0s4FCY0yRMaYFeBlY1uWYZcBfHI9fB84XEfFEcMaYo8aYrY7Hp4C9QJonru1iy4C/mnYbgSEi4o210s4HDhhjBnO39KAZY9YDVV02d36f/QW4rJunLgY+MMZUGWOqgQ+AJZ6IzxjzvjGmzfHtRiDd1dd1Vg+vnzOc+X0ftN7ic+SOq4GXXH1dT/HHRJ8GlHb6/jBfTKSnj3G80WuAYR6JrhNHl9EMYFM3u88Wke0i8q6InOHZyAAwwPsikicid3Sz35nX2ROupedfMG+/hsnGmKOOx8eA7laF9pXX8Vba/0LrTl/vBXe6x9G19GwPXV++8PotAMqNMQU97Pfm6+cUf0z0fkFEYoA3gHuNMbVddm+lvStiGvB74G0Phwcw3xiTA1wEfENEFnohhl6JSBiwFHitm92+8BqeZtr/hvfJucoi8gDQBrzYwyHeei/8ARgDTAeO0t494ou+Qu+teZ//XfLHRF8GjOz0fbpjW7fHiIgViAdOeCS69muG0p7kXzTGvNl1vzGm1hhT53i8CggVkURPxee4bpnj3wrgLdr/RO7MmdfZ3S4Cthpjyrvu8IXXECjv6M5y/FvRzTFefR1F5Gbgy8D1jg+jL3DiveAWxphyY4zNGGMH/tTDdb39+lmBK4BXejrGW69ff/hjot8CjBOR0Y4W37XAii7HrAA6ZjdcCXzY05vc1Rz9ec8Ae40xv+3hmBEdYwYiMpv2/wdPfhBFi0hsx2PaB+12dTlsBfBVx+ybOUBNp24KT+mxJeXt19Ch8/vsJuCdbo5ZDVwoIgmOrokLHdvcTkSWAP8FLDXGNPRwjDPvBXfF13nM5/IeruvM77s7LQL2GWMOd7fTm69fv3h7NHggX7TPCMmnfTT+Ace2B2l/QwNE0P7nfiGwGcjyYGzzaf8TfgewzfF1MXAncKfjmHuA3bTPINgIzPXw65fluPZ2Rxwdr2HnGAV4wvEa7wRmeTjGaNoTd3ynbV57DWn/wDkKtNLeT3wb7eM+a4ECYA0w1HHsLODPnZ57q+O9WAjc4sH4Cmnv3+54H3bMREsFVvX2XvBQfH9zvLd20J68U7rG5/j+C7/vnojPsf35jvdcp2M9/voN9ktLICilVIDzx64bpZRS/aCJXimlApwmeqWUCnCa6JVSKsBpoldKqQCniV4ppQKcJnqllApw/x98HlRPrF+vpgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.sqrt(np.sum((y_testset * (max_y - min_y) + min_y - y_predicted)**2, axis = 0) / len(y_testset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80170f62-ae06-46de-9a87-5460372520c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
