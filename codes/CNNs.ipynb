{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f59d6d37-d694-4143-9c99-25c9179e38e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8da5432-b9a6-48ed-bf54-d656ce188ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from scipy import stats\n",
    "import seaborn as sbn\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a0a6a63-1a8a-4dc8-acc7-7c9fdfbf1c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4685aa6-ac82-44a3-9182-499722c3130d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 13:00:23.671018: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-02-28 13:00:23.743534: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-02-28 13:00:23.743979: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-28 13:00:24.508794: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Lambda, Input, Dense, Dropout, AveragePooling2D, Flatten, Conv2D\n",
    "from keras.layers import MaxPooling2D, BatchNormalization, Activation\n",
    "from keras.models import Model, Sequential\n",
    "from keras.losses import mse, binary_crossentropy, mean_absolute_percentage_error\n",
    "from keras.utils import plot_model,to_categorical\n",
    "from keras import backend as K\n",
    "from keras import callbacks\n",
    "#from sklearn.preprocessing import LabelEncoder,OneHotEncoder,image,label_binarize\n",
    "from keras.utils import plot_model\n",
    "from keras.optimizers import SGD, Adam\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
    "from sklearn.metrics import roc_curve,roc_auc_score,mean_squared_error\n",
    "#from keras.applications.resnet50 import ResNet50\n",
    "import tensorflow as tf\n",
    "import keras.losses as kLoss\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c4a0878-612f-4379-92cb-dbab5c724eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version: 1.24.4\n",
      "matplotlib version: 3.4.3\n",
      "tensorflow version: 2.13.1\n",
      "Using CPU\n"
     ]
    }
   ],
   "source": [
    "print('numpy version:', np.__version__)\n",
    "print('matplotlib version:', mpl.__version__)\n",
    "print('tensorflow version:', tf.__version__)\n",
    "\n",
    "# Check if gpu is available\n",
    "if tf.test.is_gpu_available():\n",
    "    device = 'gpu'\n",
    "    print('Using GPU')\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print('Using CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2966440-2b60-4952-a983-ef18b38922a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version: 1.24.3\n",
      "matplotlib version: 3.4.3\n",
      "torch version: 2.4.1+cu121\n",
      "Using CPU\n"
     ]
    }
   ],
   "source": [
    "print('numpy version:', np.__version__)\n",
    "print('matplotlib version:', mpl.__version__)\n",
    "print('torch version:', torch.__version__)\n",
    "\n",
    "# Check if gpu is available\n",
    "if torch.cuda.is_available():\n",
    "    device = 'gpu'\n",
    "    print('Using GPU')\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print('Using CPU')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f239547f-cb41-447b-ab87-4cbb0120a747",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d7f80d09-00f5-466f-8b96-a0120c473520",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.load('../data/TNGgalaxies_28_02/images.npy')\n",
    "labels = np.load('../data/TNGgalaxies_28_02/labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a919a0b7-6d01-4829-a3ae-4b4d4837c8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3086 nans in images\n",
      "There are 0 nans in labels\n"
     ]
    }
   ],
   "source": [
    "print(f'There are {len(np.where(np.isnan(images) == True)[0])} nans in images')\n",
    "print(f'There are {len(np.where(np.isnan(labels) == True)[0])} nans in labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "98f79a57-da09-4c34-820e-f679a7f2753c",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.nan_to_num(images, nan=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e158aa09-d9ab-4d90-bda4-581e3b444200",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.moveaxis(images, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "76d153fe-55bb-40e8-965e-7d8859d81d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = images[:100,:,:,:]\n",
    "labels = labels[:100,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6b35b5e0-ebf4-4002-bc1c-8e2a0636f172",
   "metadata": {},
   "outputs": [],
   "source": [
    "nobs,nch, npix, _ = images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bf013a1f-bd8f-4045-9eb1-f6c170fced33",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(28890)\n",
    "random_ind = np.random.choice(np.arange(nobs), size = nobs, replace = False)\n",
    "\n",
    "train_size = int(0.7 * nobs)\n",
    "val_size = int(0.2 * nobs)\n",
    "test_size = nobs - train_size - val_size\n",
    "\n",
    "train_ind = random_ind[:train_size]\n",
    "val_ind = random_ind[train_size:(val_size + train_size)]\n",
    "test_ind = random_ind[-test_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f58b2156-5630-438c-a125-9b2e4857ab3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trainset = images[train_ind,:,:,:]\n",
    "x_valset = images[val_ind,:,:,:]\n",
    "x_testset = images[test_ind,:,:,:]\n",
    "\n",
    "y_trainset = np.log10(labels[train_ind,0,:])\n",
    "y_valset = np.log10(labels[val_ind,0,:])\n",
    "y_testset = np.log10(labels[test_ind,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "daf152df-634c-4ae2-ba48-d7f927e436b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_x = np.min(x_trainset)\n",
    "max_x = np.max(x_trainset)\n",
    "\n",
    "min_y = np.min(y_trainset, axis = 0)\n",
    "max_y = np.max(y_trainset, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "19d6627e-ebdf-48c0-8a42-1dd2d53beeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trainset = (x_trainset - min_x) / (max_x - min_x)\n",
    "x_valset = (x_valset - min_x) / (max_x - min_x)\n",
    "x_testset = (x_testset - min_x) / (max_x - min_x)\n",
    "\n",
    "y_trainset = (y_trainset - min_y) / (max_y - min_y)\n",
    "y_valset = (y_valset - min_y) / (max_y - min_y)\n",
    "y_testset = (y_testset - min_y) / (max_y - min_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1274844f-e6ca-4f3d-b4f0-814ebf4cfaaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-73866.63006661664"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2129415b-4e7c-45d1-9442-9a9db3bdf7d7",
   "metadata": {},
   "source": [
    "# Playing with CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "061887a5-e9e4-4175-9b2c-275f944f6562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# network parameters\n",
    "input_shape = (nch,npix, npix) # Input shape (#rows, #cols, #channels)\n",
    "actFunction = 'relu'\n",
    "\n",
    "# Hidden layers dimensions\n",
    "intermediate_dim1 = 500\n",
    "intermediate_dim2 = 400\n",
    "intermediate_dim3 = 300\n",
    "intermediate_dim4 = 200\n",
    "output_dim        = y_trainset.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9681e0-8380-4670-b860-28aea01ffb65",
   "metadata": {},
   "source": [
    "## First let's check that it overfit 1 batch\n",
    "\n",
    "* It seems to converge to MSE = 0.0451 (without batch normalization nor dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b9ad880c-34a1-4609-a586-34d4eebf01b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, input_shape, intermediate_dim1, intermediate_dim2, intermediate_dim3, intermediate_dim4, output_dim, actFunction):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        # Cálculo de la cantidad de canales de entrada (esto depende de input_shape)\n",
    "        self.input_channels = input_shape[0]  # Ejemplo: (3, 64, 64), sería 3 canales de entrada\n",
    "        \n",
    "        # Definición de las capas de la red\n",
    "        self.conv1 = nn.Conv2d(self.input_channels, 5, kernel_size=5, stride=1, padding=2)\n",
    "        self.conv2 = nn.Conv2d(5, 15, kernel_size=5, stride=1, padding=2)\n",
    "        self.conv3 = nn.Conv2d(15, 30, kernel_size=5, stride=1, padding=2)\n",
    "        \n",
    "        # Capa de Flatten\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # Capas completamente conectadas (fully connected)\n",
    "        self.fc1 = nn.Linear(30 * input_shape[1] // 8 * input_shape[2] // 8, intermediate_dim1)  # Tamaño de salida después de 3 MaxPooling\n",
    "        self.fc2 = nn.Linear(intermediate_dim1, intermediate_dim2)\n",
    "        self.fc3 = nn.Linear(intermediate_dim2, intermediate_dim3)\n",
    "        self.fc4 = nn.Linear(intermediate_dim3, intermediate_dim4)\n",
    "        self.fc5 = nn.Linear(intermediate_dim4, output_dim)\n",
    "        \n",
    "        # Función de activación\n",
    "        self.actFunction = actFunction\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Primera capa convolucional\n",
    "        x = self.conv1(x)\n",
    "        x = self.actFunction(x)\n",
    "        x = F.max_pool2d(x, 2)  # MaxPooling (2x2)\n",
    "        \n",
    "        # Segunda capa convolucional\n",
    "        x = self.conv2(x)\n",
    "        x = self.actFunction(x)\n",
    "        x = F.max_pool2d(x, 2)  # MaxPooling (2x2)\n",
    "        \n",
    "        # Tercera capa convolucional\n",
    "        x = self.conv3(x)\n",
    "        x = self.actFunction(x)\n",
    "        x = F.max_pool2d(x, 2)  # MaxPooling (2x2)\n",
    "        \n",
    "        # Aplanamiento\n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        # Capas completamente conectadas\n",
    "        x = self.actFunction(self.fc1(x))\n",
    "        x = self.actFunction(self.fc2(x))\n",
    "        x = self.actFunction(self.fc3(x))\n",
    "        x = self.fc4(x)  # Capa final antes de la salida (sin activación, o con 'linear' como en el código original)\n",
    "        x = self.fc5(x)  # Capa de salida\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "37b809f8-7299-48bf-9ee7-7f51e4fcf2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet(\n",
      "  (conv1): Conv2d(8, 5, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(5, 15, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(15, 30, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc1): Linear(in_features=7680, out_features=500, bias=True)\n",
      "  (fc2): Linear(in_features=500, out_features=400, bias=True)\n",
      "  (fc3): Linear(in_features=400, out_features=300, bias=True)\n",
      "  (fc4): Linear(in_features=300, out_features=200, bias=True)\n",
      "  (fc5): Linear(in_features=200, out_features=20, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "actFunction = torch.relu  # Función de activación ReLU\n",
    "\n",
    "# Crear el modelo\n",
    "model = ConvNet(input_shape, intermediate_dim1, intermediate_dim2, intermediate_dim3, intermediate_dim4, output_dim, actFunction)\n",
    "\n",
    "# Mostrar la estructura del modelo\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "410d3c20-376b-4623-983d-77d6888c9637",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x_data, y_data):\n",
    "        self.x_data = x_data\n",
    "        self.y_data = y_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Convertir las imágenes y etiquetas a tensores\n",
    "        image = torch.tensor(self.x_data[idx], dtype=torch.float32)\n",
    "        label = torch.tensor(self.y_data[idx], dtype=torch.float32)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9ed11798-dcdb-4ecc-a806-52e9d0da53bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear los DataLoaders para entrenamiento y validación\n",
    "train_dataset = CustomDataset(x_trainset, y_trainset)\n",
    "val_dataset = CustomDataset(x_valset, y_valset)\n",
    "test_dataset = CustomDataset(x_testset, y_testset)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5e4b8a6f-1182-4bda-825e-6370e45a1670",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "894aa8a7-364a-4a5f-8233-1702f968246b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Definir la función de pérdida y el optimizador\n",
    "criterion = nn.MSELoss()  # Para un problema de regresión o imagen de salida continua\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3ed815e2-7ba8-415d-9f8b-d22102c38a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = iter(dataloader)  # Crear un iterador del DataLoader\n",
    "x_batch, y_batch = next(data_iter) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "86933575-88a5-4743-a86a-3e80963b1f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train Loss: 0.3067, Validation Loss: 0.2896\n",
      "Epoch [2/100], Train Loss: 0.1682, Validation Loss: 0.0575\n",
      "Epoch [3/100], Train Loss: 0.0638, Validation Loss: 0.1365\n",
      "Epoch [4/100], Train Loss: 0.0999, Validation Loss: 0.0930\n",
      "Epoch [5/100], Train Loss: 0.0617, Validation Loss: 0.0593\n",
      "Epoch [6/100], Train Loss: 0.0484, Validation Loss: 0.0699\n",
      "Epoch [7/100], Train Loss: 0.0547, Validation Loss: 0.0825\n",
      "Epoch [8/100], Train Loss: 0.0443, Validation Loss: 0.0583\n",
      "Epoch [9/100], Train Loss: 0.0548, Validation Loss: 0.0601\n",
      "Epoch [10/100], Train Loss: 0.0476, Validation Loss: 0.0803\n",
      "Epoch [11/100], Train Loss: 0.0577, Validation Loss: 0.0564\n",
      "Epoch [12/100], Train Loss: 0.0459, Validation Loss: 0.0500\n",
      "Epoch [13/100], Train Loss: 0.0427, Validation Loss: 0.0710\n",
      "Epoch [14/100], Train Loss: 0.0410, Validation Loss: 0.0793\n",
      "Epoch [15/100], Train Loss: 0.0492, Validation Loss: 0.0590\n",
      "Epoch [16/100], Train Loss: 0.0452, Validation Loss: 0.0492\n",
      "Epoch [17/100], Train Loss: 0.0463, Validation Loss: 0.0637\n",
      "Epoch [18/100], Train Loss: 0.0444, Validation Loss: 0.0758\n",
      "Epoch [19/100], Train Loss: 0.0491, Validation Loss: 0.0532\n",
      "Epoch [20/100], Train Loss: 0.0472, Validation Loss: 0.0494\n",
      "Epoch [21/100], Train Loss: 0.0437, Validation Loss: 0.0629\n",
      "Epoch [22/100], Train Loss: 0.0442, Validation Loss: 0.0710\n",
      "Epoch [23/100], Train Loss: 0.0428, Validation Loss: 0.0595\n",
      "Epoch [24/100], Train Loss: 0.0534, Validation Loss: 0.0489\n",
      "Epoch [25/100], Train Loss: 0.0479, Validation Loss: 0.0546\n",
      "Epoch [26/100], Train Loss: 0.0359, Validation Loss: 0.0799\n",
      "Epoch [27/100], Train Loss: 0.0447, Validation Loss: 0.0728\n",
      "Epoch [28/100], Train Loss: 0.0480, Validation Loss: 0.0485\n",
      "Epoch [29/100], Train Loss: 0.0732, Validation Loss: 0.0526\n",
      "Epoch [30/100], Train Loss: 0.0506, Validation Loss: 0.1015\n",
      "Epoch [31/100], Train Loss: 0.0541, Validation Loss: 0.0854\n",
      "Epoch [32/100], Train Loss: 0.0637, Validation Loss: 0.0502\n",
      "Epoch [33/100], Train Loss: 0.0467, Validation Loss: 0.0516\n",
      "Epoch [34/100], Train Loss: 0.0353, Validation Loss: 0.0674\n",
      "Epoch [35/100], Train Loss: 0.0387, Validation Loss: 0.0620\n",
      "Epoch [36/100], Train Loss: 0.0457, Validation Loss: 0.0498\n",
      "Epoch [37/100], Train Loss: 0.0448, Validation Loss: 0.0516\n",
      "Epoch [38/100], Train Loss: 0.0491, Validation Loss: 0.0653\n",
      "Epoch [39/100], Train Loss: 0.0416, Validation Loss: 0.0567\n",
      "Epoch [40/100], Train Loss: 0.0471, Validation Loss: 0.0520\n",
      "Epoch [41/100], Train Loss: 0.0424, Validation Loss: 0.0531\n",
      "Epoch [42/100], Train Loss: 0.0437, Validation Loss: 0.0518\n",
      "Epoch [43/100], Train Loss: 0.0571, Validation Loss: 0.0501\n",
      "Epoch [44/100], Train Loss: 0.0449, Validation Loss: 0.0513\n",
      "Epoch [45/100], Train Loss: 0.0383, Validation Loss: 0.0736\n",
      "Epoch [46/100], Train Loss: 0.0400, Validation Loss: 0.0645\n",
      "Epoch [47/100], Train Loss: 0.0404, Validation Loss: 0.0511\n",
      "Epoch [48/100], Train Loss: 0.0473, Validation Loss: 0.0499\n",
      "Epoch [49/100], Train Loss: 0.0508, Validation Loss: 0.0553\n",
      "Epoch [50/100], Train Loss: 0.0406, Validation Loss: 0.0673\n",
      "Epoch [51/100], Train Loss: 0.0470, Validation Loss: 0.0596\n",
      "Epoch [52/100], Train Loss: 0.0399, Validation Loss: 0.0513\n",
      "Epoch [53/100], Train Loss: 0.0405, Validation Loss: 0.0641\n",
      "Epoch [54/100], Train Loss: 0.0506, Validation Loss: 0.0770\n",
      "Epoch [55/100], Train Loss: 0.0494, Validation Loss: 0.0611\n",
      "Epoch [56/100], Train Loss: 0.0387, Validation Loss: 0.0484\n",
      "Epoch [57/100], Train Loss: 0.0417, Validation Loss: 0.0511\n",
      "Epoch [58/100], Train Loss: 0.0404, Validation Loss: 0.0681\n",
      "Epoch [59/100], Train Loss: 0.0482, Validation Loss: 0.0648\n",
      "Epoch [60/100], Train Loss: 0.0542, Validation Loss: 0.0526\n",
      "Epoch [61/100], Train Loss: 0.0539, Validation Loss: 0.0580\n",
      "Epoch [62/100], Train Loss: 0.0468, Validation Loss: 0.0641\n",
      "Epoch [63/100], Train Loss: 0.0452, Validation Loss: 0.0575\n",
      "Epoch [64/100], Train Loss: 0.0457, Validation Loss: 0.0517\n",
      "Epoch [65/100], Train Loss: 0.0422, Validation Loss: 0.0500\n",
      "Epoch [66/100], Train Loss: 0.0543, Validation Loss: 0.0588\n",
      "Epoch [67/100], Train Loss: 0.0430, Validation Loss: 0.0700\n",
      "Epoch [68/100], Train Loss: 0.0384, Validation Loss: 0.0628\n",
      "Epoch [69/100], Train Loss: 0.0482, Validation Loss: 0.0493\n",
      "Epoch [70/100], Train Loss: 0.0411, Validation Loss: 0.0490\n",
      "Epoch [71/100], Train Loss: 0.0461, Validation Loss: 0.0686\n",
      "Epoch [72/100], Train Loss: 0.0542, Validation Loss: 0.0776\n",
      "Epoch [73/100], Train Loss: 0.0519, Validation Loss: 0.0553\n",
      "Epoch [74/100], Train Loss: 0.0422, Validation Loss: 0.0484\n",
      "Epoch [75/100], Train Loss: 0.0420, Validation Loss: 0.0566\n",
      "Epoch [76/100], Train Loss: 0.0466, Validation Loss: 0.0645\n",
      "Epoch [77/100], Train Loss: 0.0476, Validation Loss: 0.0571\n",
      "Epoch [78/100], Train Loss: 0.0442, Validation Loss: 0.0572\n",
      "Epoch [79/100], Train Loss: 0.0401, Validation Loss: 0.0551\n",
      "Epoch [80/100], Train Loss: 0.0445, Validation Loss: 0.0572\n",
      "Epoch [81/100], Train Loss: 0.0369, Validation Loss: 0.0513\n",
      "Epoch [82/100], Train Loss: 0.0371, Validation Loss: 0.0523\n",
      "Epoch [83/100], Train Loss: 0.0406, Validation Loss: 0.0590\n",
      "Epoch [84/100], Train Loss: 0.0443, Validation Loss: 0.0594\n",
      "Epoch [85/100], Train Loss: 0.0431, Validation Loss: 0.0524\n",
      "Epoch [86/100], Train Loss: 0.0403, Validation Loss: 0.0502\n",
      "Epoch [87/100], Train Loss: 0.0521, Validation Loss: 0.0610\n",
      "Epoch [88/100], Train Loss: 0.0445, Validation Loss: 0.0688\n",
      "Epoch [89/100], Train Loss: 0.0500, Validation Loss: 0.0607\n",
      "Epoch [90/100], Train Loss: 0.0567, Validation Loss: 0.0499\n",
      "Epoch [91/100], Train Loss: 0.0473, Validation Loss: 0.0548\n",
      "Epoch [92/100], Train Loss: 0.0363, Validation Loss: 0.0556\n",
      "Epoch [93/100], Train Loss: 0.0511, Validation Loss: 0.0559\n",
      "Epoch [94/100], Train Loss: 0.0390, Validation Loss: 0.0595\n",
      "Epoch [95/100], Train Loss: 0.0440, Validation Loss: 0.0538\n",
      "Epoch [96/100], Train Loss: 0.0482, Validation Loss: 0.0507\n",
      "Epoch [97/100], Train Loss: 0.0412, Validation Loss: 0.0587\n",
      "Epoch [98/100], Train Loss: 0.0475, Validation Loss: 0.0648\n",
      "Epoch [99/100], Train Loss: 0.0362, Validation Loss: 0.0527\n",
      "Epoch [100/100], Train Loss: 0.0379, Validation Loss: 0.0504\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "for epoch in range(num_epochs):\n",
    "    # Entrenamiento\n",
    "    model.train()  # Configura el modelo en modo entrenamiento\n",
    "    running_train_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        # Zero gradientes\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Paso hacia adelante\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Calcular la pérdida\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Paso hacia atrás (backpropagation)\n",
    "        loss.backward()\n",
    "\n",
    "        # Actualizar los pesos\n",
    "        optimizer.step()\n",
    "\n",
    "        # Sumar la pérdida del lote\n",
    "        running_train_loss += loss.item()\n",
    "\n",
    "    # Promedio de la pérdida de entrenamiento\n",
    "    avg_train_loss = running_train_loss / len(train_loader)\n",
    "    train_loss.append(avg_train_loss)\n",
    "    \n",
    "    # Validación\n",
    "    model.eval()  # Configura el modelo en modo evaluación\n",
    "    running_val_loss = 0.0\n",
    "    with torch.no_grad():  # No calculamos gradientes en validación\n",
    "        for inputs, labels in val_loader:\n",
    "\n",
    "            # Paso hacia adelante (sin gradientes)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Calcular la pérdida\n",
    "            loss = criterion(outputs, labels)\n",
    "            # Sumar la pérdida de validación\n",
    "            running_val_loss += loss.item()\n",
    "\n",
    "    # Promedio de la pérdida de validación\n",
    "    avg_val_loss = running_val_loss / len(val_loader)\n",
    "    val_loss.append(avg_val_loss)\n",
    "    \n",
    "    # Imprimir estadísticas de la época\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "          f\"Validation Loss: {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "307c876d-0e37-46b5-b3d3-725fde322b81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f234ee70e80>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss, label = 'Tr. Loss')\n",
    "plt.plot(val_loss, label = 'Val. Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3b8ac656-8e74-4489-b0ab-da8d5209ab64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 20)\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # Asegúrate de que el modelo esté en modo evaluación\n",
    "y_predicted = []  # Lista para almacenar las predicciones\n",
    "\n",
    "# Desactivamos el cálculo de gradientes durante la predicción\n",
    "with torch.no_grad():\n",
    "    for inputs, _ in test_loader:  # Usamos _ porque no necesitamos las etiquetas en x_testset\n",
    "\n",
    "        # Hacer predicciones\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Almacenar las predicciones (por ejemplo, como numpy arrays)\n",
    "        y_predicted.append(outputs.numpy())  # Si necesitas los resultados como numpy\n",
    "\n",
    "# Convertir la lista de predicciones en un solo array\n",
    "y_predicted = np.concatenate(y_predicted, axis=0)\n",
    "\n",
    "print(y_predicted.shape)  # Esto te dará la forma de las predicciones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1cb0aef7-125e-4242-a9e2-34c18f339f02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i= 5\n",
    "plt.plot(10**(y_predicted[i,:] * (max_y - min_y) + min_y))\n",
    "plt.plot(10**(y_testset[i,:] * (max_y - min_y) + min_y))\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e6bc81c6-17cf-4775-9414-84ef47889fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_18 (Conv2D)          (None, 128, 128, 5)       1005      \n",
      "                                                                 \n",
      " batch_normalization_7 (Bat  (None, 128, 128, 5)       20        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_18 (Activation)  (None, 128, 128, 5)       0         \n",
      "                                                                 \n",
      " max_pooling2d_18 (MaxPooli  (None, 64, 64, 5)         0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_19 (Conv2D)          (None, 64, 64, 15)        1890      \n",
      "                                                                 \n",
      " batch_normalization_8 (Bat  (None, 64, 64, 15)        60        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_19 (Activation)  (None, 64, 64, 15)        0         \n",
      "                                                                 \n",
      " max_pooling2d_19 (MaxPooli  (None, 32, 32, 15)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_20 (Conv2D)          (None, 32, 32, 30)        11280     \n",
      "                                                                 \n",
      " batch_normalization_9 (Bat  (None, 32, 32, 30)        120       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_20 (Activation)  (None, 32, 32, 30)        0         \n",
      "                                                                 \n",
      " max_pooling2d_20 (MaxPooli  (None, 16, 16, 30)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16, 16, 30)        0         \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (None, 7680)              0         \n",
      "                                                                 \n",
      " batch_normalization_10 (Ba  (None, 7680)              30720     \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 500)               3840500   \n",
      "                                                                 \n",
      " batch_normalization_11 (Ba  (None, 500)               2000      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 400)               200400    \n",
      "                                                                 \n",
      " batch_normalization_12 (Ba  (None, 400)               1600      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 300)               120300    \n",
      "                                                                 \n",
      " batch_normalization_13 (Ba  (None, 300)               1200      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 200)               60200     \n",
      "                                                                 \n",
      " output (Dense)              (None, 20)                4020      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4275315 (16.31 MB)\n",
      "Trainable params: 4257455 (16.24 MB)\n",
      "Non-trainable params: 17860 (69.77 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters = 5, kernel_size = (5,5), strides = 1 , padding = 'same', input_shape = input_shape))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Activation(actFunction))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(filters = 15, kernel_size = (5,5), strides = 1 , padding = 'same'))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Activation(actFunction))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(filters = 30, kernel_size = (5,5), strides = 1 , padding = 'same'))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Activation(actFunction))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#model.add(Dropout(0.2)) \n",
    "model.add(Flatten())\n",
    "\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dense(intermediate_dim1, activation = actFunction))\n",
    "\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dense(intermediate_dim2, activation = actFunction))\n",
    "\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dense(intermediate_dim3, activation = actFunction))\n",
    "\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dense(intermediate_dim4, activation = 'linear'))\n",
    "\n",
    "model.add(Dense(output_dim, name = 'output'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "33b9f1e4-4422-48c8-a20f-4faedc0fff5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model\n",
    "optimizer = optimizers.Adam(learning_rate = 1e-5, beta_1 = 0.9, beta_2 = 0.999, amsgrad = False)\n",
    "model.compile(optimizer = optimizer, loss = 'mse', metrics=['mae','mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3a4c7052-98c8-43ee-939f-7cfa6802af34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.3322 - mae: 0.5282 - mse: 0.3322 - val_loss: 0.3210 - val_mae: 0.5184 - val_mse: 0.3210\n",
      "Epoch 2/250\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.3210 - mae: 0.5184 - mse: 0.3210 - val_loss: 0.3104 - val_mae: 0.5089 - val_mse: 0.3104\n",
      "Epoch 3/250\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.3104 - mae: 0.5089 - mse: 0.3104 - val_loss: 0.3009 - val_mae: 0.5002 - val_mse: 0.3009\n",
      "Epoch 4/250\n",
      "1/1 [==============================] - 0s 345ms/step - loss: 0.3009 - mae: 0.5002 - mse: 0.3009 - val_loss: 0.2919 - val_mae: 0.4918 - val_mse: 0.2919\n",
      "Epoch 5/250\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 0.2919 - mae: 0.4918 - mse: 0.2919 - val_loss: 0.2837 - val_mae: 0.4839 - val_mse: 0.2837\n",
      "Epoch 6/250\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 0.2837 - mae: 0.4839 - mse: 0.2837 - val_loss: 0.2761 - val_mae: 0.4765 - val_mse: 0.2761\n",
      "Epoch 7/250\n",
      "1/1 [==============================] - 0s 333ms/step - loss: 0.2761 - mae: 0.4765 - mse: 0.2761 - val_loss: 0.2689 - val_mae: 0.4692 - val_mse: 0.2689\n",
      "Epoch 8/250\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.2689 - mae: 0.4692 - mse: 0.2689 - val_loss: 0.2624 - val_mae: 0.4625 - val_mse: 0.2624\n",
      "Epoch 9/250\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.2624 - mae: 0.4625 - mse: 0.2624 - val_loss: 0.2560 - val_mae: 0.4558 - val_mse: 0.2560\n",
      "Epoch 10/250\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 0.2560 - mae: 0.4558 - mse: 0.2560 - val_loss: 0.2501 - val_mae: 0.4496 - val_mse: 0.2501\n",
      "Epoch 11/250\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.2501 - mae: 0.4496 - mse: 0.2501 - val_loss: 0.2447 - val_mae: 0.4437 - val_mse: 0.2447\n",
      "Epoch 12/250\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.2447 - mae: 0.4437 - mse: 0.2447 - val_loss: 0.2395 - val_mae: 0.4380 - val_mse: 0.2395\n",
      "Epoch 13/250\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.2395 - mae: 0.4380 - mse: 0.2395 - val_loss: 0.2346 - val_mae: 0.4325 - val_mse: 0.2346\n",
      "Epoch 14/250\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.2346 - mae: 0.4325 - mse: 0.2346 - val_loss: 0.2300 - val_mae: 0.4273 - val_mse: 0.2300\n",
      "Epoch 15/250\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.2300 - mae: 0.4273 - mse: 0.2300 - val_loss: 0.2257 - val_mae: 0.4223 - val_mse: 0.2257\n",
      "Epoch 16/250\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.2257 - mae: 0.4223 - mse: 0.2257 - val_loss: 0.2216 - val_mae: 0.4175 - val_mse: 0.2216\n",
      "Epoch 17/250\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.2216 - mae: 0.4175 - mse: 0.2216 - val_loss: 0.2177 - val_mae: 0.4128 - val_mse: 0.2177\n",
      "Epoch 18/250\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.2177 - mae: 0.4128 - mse: 0.2177 - val_loss: 0.2138 - val_mae: 0.4081 - val_mse: 0.2138\n",
      "Epoch 19/250\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 0.2138 - mae: 0.4081 - mse: 0.2138 - val_loss: 0.2099 - val_mae: 0.4036 - val_mse: 0.2099\n",
      "Epoch 20/250\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.2099 - mae: 0.4036 - mse: 0.2099 - val_loss: 0.2061 - val_mae: 0.3991 - val_mse: 0.2061\n",
      "Epoch 21/250\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.2061 - mae: 0.3991 - mse: 0.2061 - val_loss: 0.2025 - val_mae: 0.3948 - val_mse: 0.2025\n",
      "Epoch 22/250\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 0.2025 - mae: 0.3948 - mse: 0.2025 - val_loss: 0.1989 - val_mae: 0.3905 - val_mse: 0.1989\n",
      "Epoch 23/250\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.1989 - mae: 0.3905 - mse: 0.1989 - val_loss: 0.1951 - val_mae: 0.3860 - val_mse: 0.1951\n",
      "Epoch 24/250\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.1951 - mae: 0.3860 - mse: 0.1951 - val_loss: 0.1913 - val_mae: 0.3814 - val_mse: 0.1913\n",
      "Epoch 25/250\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.1913 - mae: 0.3814 - mse: 0.1913 - val_loss: 0.1875 - val_mae: 0.3768 - val_mse: 0.1875\n",
      "Epoch 26/250\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.1875 - mae: 0.3768 - mse: 0.1875 - val_loss: 0.1837 - val_mae: 0.3721 - val_mse: 0.1837\n",
      "Epoch 27/250\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 0.1837 - mae: 0.3721 - mse: 0.1837 - val_loss: 0.1799 - val_mae: 0.3674 - val_mse: 0.1799\n",
      "Epoch 28/250\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.1799 - mae: 0.3674 - mse: 0.1799 - val_loss: 0.1760 - val_mae: 0.3625 - val_mse: 0.1760\n",
      "Epoch 29/250\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.1760 - mae: 0.3625 - mse: 0.1760 - val_loss: 0.1722 - val_mae: 0.3577 - val_mse: 0.1722\n",
      "Epoch 30/250\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.1722 - mae: 0.3577 - mse: 0.1722 - val_loss: 0.1683 - val_mae: 0.3528 - val_mse: 0.1683\n",
      "Epoch 31/250\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.1683 - mae: 0.3528 - mse: 0.1683 - val_loss: 0.1645 - val_mae: 0.3479 - val_mse: 0.1645\n",
      "Epoch 32/250\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 0.1645 - mae: 0.3479 - mse: 0.1645 - val_loss: 0.1608 - val_mae: 0.3432 - val_mse: 0.1608\n",
      "Epoch 33/250\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.1608 - mae: 0.3432 - mse: 0.1608 - val_loss: 0.1572 - val_mae: 0.3384 - val_mse: 0.1572\n",
      "Epoch 34/250\n",
      "1/1 [==============================] - 0s 322ms/step - loss: 0.1572 - mae: 0.3384 - mse: 0.1572 - val_loss: 0.1536 - val_mae: 0.3337 - val_mse: 0.1536\n",
      "Epoch 35/250\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.1536 - mae: 0.3337 - mse: 0.1536 - val_loss: 0.1500 - val_mae: 0.3289 - val_mse: 0.1500\n",
      "Epoch 36/250\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.1500 - mae: 0.3289 - mse: 0.1500 - val_loss: 0.1465 - val_mae: 0.3240 - val_mse: 0.1465\n",
      "Epoch 37/250\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1465 - mae: 0.3240 - mse: 0.1465 - val_loss: 0.1430 - val_mae: 0.3193 - val_mse: 0.1430\n",
      "Epoch 38/250\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.1430 - mae: 0.3193 - mse: 0.1430 - val_loss: 0.1395 - val_mae: 0.3145 - val_mse: 0.1395\n",
      "Epoch 39/250\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1395 - mae: 0.3145 - mse: 0.1395 - val_loss: 0.1361 - val_mae: 0.3098 - val_mse: 0.1361\n",
      "Epoch 40/250\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 0.1361 - mae: 0.3098 - mse: 0.1361 - val_loss: 0.1326 - val_mae: 0.3050 - val_mse: 0.1326\n",
      "Epoch 41/250\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.1326 - mae: 0.3050 - mse: 0.1326 - val_loss: 0.1291 - val_mae: 0.3002 - val_mse: 0.1291\n",
      "Epoch 42/250\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.1291 - mae: 0.3002 - mse: 0.1291 - val_loss: 0.1257 - val_mae: 0.2955 - val_mse: 0.1257\n",
      "Epoch 43/250\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.1257 - mae: 0.2955 - mse: 0.1257 - val_loss: 0.1224 - val_mae: 0.2908 - val_mse: 0.1224\n",
      "Epoch 44/250\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.1224 - mae: 0.2908 - mse: 0.1224 - val_loss: 0.1190 - val_mae: 0.2861 - val_mse: 0.1190\n",
      "Epoch 45/250\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 0.1190 - mae: 0.2861 - mse: 0.1190 - val_loss: 0.1156 - val_mae: 0.2813 - val_mse: 0.1156\n",
      "Epoch 46/250\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.1156 - mae: 0.2813 - mse: 0.1156 - val_loss: 0.1124 - val_mae: 0.2766 - val_mse: 0.1124\n",
      "Epoch 47/250\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.1124 - mae: 0.2766 - mse: 0.1124 - val_loss: 0.1092 - val_mae: 0.2721 - val_mse: 0.1092\n",
      "Epoch 48/250\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.1092 - mae: 0.2721 - mse: 0.1092 - val_loss: 0.1060 - val_mae: 0.2675 - val_mse: 0.1060\n",
      "Epoch 49/250\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.1060 - mae: 0.2675 - mse: 0.1060 - val_loss: 0.1029 - val_mae: 0.2629 - val_mse: 0.1029\n",
      "Epoch 50/250\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.1029 - mae: 0.2629 - mse: 0.1029 - val_loss: 0.0999 - val_mae: 0.2584 - val_mse: 0.0999\n",
      "Epoch 51/250\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.0999 - mae: 0.2584 - mse: 0.0999 - val_loss: 0.0970 - val_mae: 0.2540 - val_mse: 0.0970\n",
      "Epoch 52/250\n",
      "1/1 [==============================] - 0s 327ms/step - loss: 0.0970 - mae: 0.2540 - mse: 0.0970 - val_loss: 0.0942 - val_mae: 0.2497 - val_mse: 0.0942\n",
      "Epoch 53/250\n",
      "1/1 [==============================] - 0s 334ms/step - loss: 0.0942 - mae: 0.2497 - mse: 0.0942 - val_loss: 0.0914 - val_mae: 0.2453 - val_mse: 0.0914\n",
      "Epoch 54/250\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.0914 - mae: 0.2453 - mse: 0.0914 - val_loss: 0.0887 - val_mae: 0.2410 - val_mse: 0.0887\n",
      "Epoch 55/250\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.0887 - mae: 0.2410 - mse: 0.0887 - val_loss: 0.0861 - val_mae: 0.2368 - val_mse: 0.0861\n",
      "Epoch 56/250\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 0.0861 - mae: 0.2368 - mse: 0.0861 - val_loss: 0.0836 - val_mae: 0.2327 - val_mse: 0.0836\n",
      "Epoch 57/250\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.0836 - mae: 0.2327 - mse: 0.0836 - val_loss: 0.0811 - val_mae: 0.2288 - val_mse: 0.0811\n",
      "Epoch 58/250\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 0.0811 - mae: 0.2288 - mse: 0.0811 - val_loss: 0.0788 - val_mae: 0.2250 - val_mse: 0.0788\n",
      "Epoch 59/250\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.0788 - mae: 0.2250 - mse: 0.0788 - val_loss: 0.0765 - val_mae: 0.2213 - val_mse: 0.0765\n",
      "Epoch 60/250\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.0765 - mae: 0.2213 - mse: 0.0765 - val_loss: 0.0744 - val_mae: 0.2178 - val_mse: 0.0744\n",
      "Epoch 61/250\n",
      "1/1 [==============================] - 0s 257ms/step - loss: 0.0744 - mae: 0.2178 - mse: 0.0744 - val_loss: 0.0723 - val_mae: 0.2145 - val_mse: 0.0723\n",
      "Epoch 62/250\n",
      "1/1 [==============================] - 0s 255ms/step - loss: 0.0723 - mae: 0.2145 - mse: 0.0723 - val_loss: 0.0703 - val_mae: 0.2113 - val_mse: 0.0703\n",
      "Epoch 63/250\n",
      "1/1 [==============================] - 0s 255ms/step - loss: 0.0703 - mae: 0.2113 - mse: 0.0703 - val_loss: 0.0685 - val_mae: 0.2082 - val_mse: 0.0685\n",
      "Epoch 64/250\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.0685 - mae: 0.2082 - mse: 0.0685 - val_loss: 0.0667 - val_mae: 0.2053 - val_mse: 0.0667\n",
      "Epoch 65/250\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.0667 - mae: 0.2053 - mse: 0.0667 - val_loss: 0.0650 - val_mae: 0.2025 - val_mse: 0.0650\n",
      "Epoch 66/250\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.0650 - mae: 0.2025 - mse: 0.0650 - val_loss: 0.0633 - val_mae: 0.1998 - val_mse: 0.0633\n",
      "Epoch 67/250\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.0633 - mae: 0.1998 - mse: 0.0633 - val_loss: 0.0618 - val_mae: 0.1972 - val_mse: 0.0618\n",
      "Epoch 68/250\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.0618 - mae: 0.1972 - mse: 0.0618 - val_loss: 0.0604 - val_mae: 0.1948 - val_mse: 0.0604\n",
      "Epoch 69/250\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.0604 - mae: 0.1948 - mse: 0.0604 - val_loss: 0.0591 - val_mae: 0.1925 - val_mse: 0.0591\n",
      "Epoch 70/250\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.0591 - mae: 0.1925 - mse: 0.0591 - val_loss: 0.0579 - val_mae: 0.1904 - val_mse: 0.0579\n",
      "Epoch 71/250\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.0579 - mae: 0.1904 - mse: 0.0579 - val_loss: 0.0567 - val_mae: 0.1885 - val_mse: 0.0567\n",
      "Epoch 72/250\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.0567 - mae: 0.1885 - mse: 0.0567 - val_loss: 0.0556 - val_mae: 0.1868 - val_mse: 0.0556\n",
      "Epoch 73/250\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.0556 - mae: 0.1868 - mse: 0.0556 - val_loss: 0.0546 - val_mae: 0.1853 - val_mse: 0.0546\n",
      "Epoch 74/250\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 0.0546 - mae: 0.1853 - mse: 0.0546 - val_loss: 0.0537 - val_mae: 0.1839 - val_mse: 0.0537\n",
      "Epoch 75/250\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.0537 - mae: 0.1839 - mse: 0.0537 - val_loss: 0.0529 - val_mae: 0.1828 - val_mse: 0.0529\n",
      "Epoch 76/250\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.0529 - mae: 0.1828 - mse: 0.0529 - val_loss: 0.0521 - val_mae: 0.1817 - val_mse: 0.0521\n",
      "Epoch 77/250\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.0521 - mae: 0.1817 - mse: 0.0521 - val_loss: 0.0514 - val_mae: 0.1808 - val_mse: 0.0514\n",
      "Epoch 78/250\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.0514 - mae: 0.1808 - mse: 0.0514 - val_loss: 0.0508 - val_mae: 0.1799 - val_mse: 0.0508\n",
      "Epoch 79/250\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 0.0508 - mae: 0.1799 - mse: 0.0508 - val_loss: 0.0502 - val_mae: 0.1791 - val_mse: 0.0502\n",
      "Epoch 80/250\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.0502 - mae: 0.1791 - mse: 0.0502 - val_loss: 0.0496 - val_mae: 0.1784 - val_mse: 0.0496\n",
      "Epoch 81/250\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.0496 - mae: 0.1784 - mse: 0.0496 - val_loss: 0.0491 - val_mae: 0.1777 - val_mse: 0.0491\n",
      "Epoch 82/250\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.0491 - mae: 0.1777 - mse: 0.0491 - val_loss: 0.0487 - val_mae: 0.1771 - val_mse: 0.0487\n",
      "Epoch 83/250\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 0.0487 - mae: 0.1771 - mse: 0.0487 - val_loss: 0.0483 - val_mae: 0.1766 - val_mse: 0.0483\n",
      "Epoch 84/250\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.0483 - mae: 0.1766 - mse: 0.0483 - val_loss: 0.0479 - val_mae: 0.1761 - val_mse: 0.0479\n",
      "Epoch 85/250\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.0479 - mae: 0.1761 - mse: 0.0479 - val_loss: 0.0476 - val_mae: 0.1757 - val_mse: 0.0476\n",
      "Epoch 86/250\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.0476 - mae: 0.1757 - mse: 0.0476 - val_loss: 0.0473 - val_mae: 0.1753 - val_mse: 0.0473\n",
      "Epoch 87/250\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 0.0473 - mae: 0.1753 - mse: 0.0473 - val_loss: 0.0470 - val_mae: 0.1749 - val_mse: 0.0470\n",
      "Epoch 88/250\n",
      "1/1 [==============================] - 0s 323ms/step - loss: 0.0470 - mae: 0.1749 - mse: 0.0470 - val_loss: 0.0468 - val_mae: 0.1746 - val_mse: 0.0468\n",
      "Epoch 89/250\n",
      "1/1 [==============================] - 0s 325ms/step - loss: 0.0468 - mae: 0.1746 - mse: 0.0468 - val_loss: 0.0465 - val_mae: 0.1744 - val_mse: 0.0465\n",
      "Epoch 90/250\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 0.0465 - mae: 0.1744 - mse: 0.0465 - val_loss: 0.0464 - val_mae: 0.1742 - val_mse: 0.0464\n",
      "Epoch 91/250\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.0464 - mae: 0.1742 - mse: 0.0464 - val_loss: 0.0462 - val_mae: 0.1741 - val_mse: 0.0462\n",
      "Epoch 92/250\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.0462 - mae: 0.1741 - mse: 0.0462 - val_loss: 0.0460 - val_mae: 0.1740 - val_mse: 0.0460\n",
      "Epoch 93/250\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.0460 - mae: 0.1740 - mse: 0.0460 - val_loss: 0.0459 - val_mae: 0.1739 - val_mse: 0.0459\n",
      "Epoch 94/250\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.0459 - mae: 0.1739 - mse: 0.0459 - val_loss: 0.0458 - val_mae: 0.1738 - val_mse: 0.0458\n",
      "Epoch 95/250\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.0458 - mae: 0.1738 - mse: 0.0458 - val_loss: 0.0457 - val_mae: 0.1737 - val_mse: 0.0457\n",
      "Epoch 96/250\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.0457 - mae: 0.1737 - mse: 0.0457 - val_loss: 0.0456 - val_mae: 0.1737 - val_mse: 0.0456\n",
      "Epoch 97/250\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 0.0456 - mae: 0.1737 - mse: 0.0456 - val_loss: 0.0455 - val_mae: 0.1736 - val_mse: 0.0455\n",
      "Epoch 98/250\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 0.0455 - mae: 0.1736 - mse: 0.0455 - val_loss: 0.0455 - val_mae: 0.1736 - val_mse: 0.0455\n",
      "Epoch 99/250\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 0.0455 - mae: 0.1736 - mse: 0.0455 - val_loss: 0.0454 - val_mae: 0.1736 - val_mse: 0.0454\n",
      "Epoch 100/250\n",
      "1/1 [==============================] - 0s 376ms/step - loss: 0.0454 - mae: 0.1736 - mse: 0.0454 - val_loss: 0.0454 - val_mae: 0.1736 - val_mse: 0.0454\n",
      "Epoch 101/250\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 0.0454 - mae: 0.1736 - mse: 0.0454 - val_loss: 0.0453 - val_mae: 0.1735 - val_mse: 0.0453\n",
      "Epoch 102/250\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.0453 - mae: 0.1735 - mse: 0.0453 - val_loss: 0.0453 - val_mae: 0.1735 - val_mse: 0.0453\n",
      "Epoch 103/250\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.0453 - mae: 0.1735 - mse: 0.0453 - val_loss: 0.0453 - val_mae: 0.1735 - val_mse: 0.0453\n",
      "Epoch 104/250\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 0.0453 - mae: 0.1735 - mse: 0.0453 - val_loss: 0.0452 - val_mae: 0.1735 - val_mse: 0.0452\n",
      "Epoch 105/250\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.0452 - mae: 0.1735 - mse: 0.0452 - val_loss: 0.0452 - val_mae: 0.1736 - val_mse: 0.0452\n",
      "Epoch 106/250\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.0452 - mae: 0.1736 - mse: 0.0452 - val_loss: 0.0452 - val_mae: 0.1736 - val_mse: 0.0452\n",
      "Epoch 107/250\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.0452 - mae: 0.1736 - mse: 0.0452 - val_loss: 0.0452 - val_mae: 0.1736 - val_mse: 0.0452\n",
      "Epoch 108/250\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 0.0452 - mae: 0.1736 - mse: 0.0452 - val_loss: 0.0452 - val_mae: 0.1736 - val_mse: 0.0452\n",
      "Epoch 109/250\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.0452 - mae: 0.1736 - mse: 0.0452 - val_loss: 0.0452 - val_mae: 0.1736 - val_mse: 0.0452\n",
      "Epoch 110/250\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.0452 - mae: 0.1736 - mse: 0.0452 - val_loss: 0.0452 - val_mae: 0.1735 - val_mse: 0.0452\n",
      "Epoch 111/250\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.0452 - mae: 0.1735 - mse: 0.0452 - val_loss: 0.0452 - val_mae: 0.1735 - val_mse: 0.0452\n",
      "Epoch 112/250\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.0452 - mae: 0.1735 - mse: 0.0452 - val_loss: 0.0452 - val_mae: 0.1735 - val_mse: 0.0452\n",
      "Epoch 113/250\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 0.0452 - mae: 0.1735 - mse: 0.0452 - val_loss: 0.0452 - val_mae: 0.1735 - val_mse: 0.0452\n",
      "Epoch 114/250\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.0452 - mae: 0.1735 - mse: 0.0452 - val_loss: 0.0452 - val_mae: 0.1735 - val_mse: 0.0452\n",
      "Epoch 115/250\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 0.0452 - mae: 0.1735 - mse: 0.0452 - val_loss: 0.0452 - val_mae: 0.1735 - val_mse: 0.0452\n",
      "Epoch 116/250\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 0.0452 - mae: 0.1735 - mse: 0.0452 - val_loss: 0.0452 - val_mae: 0.1735 - val_mse: 0.0452\n",
      "Epoch 117/250\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.0452 - mae: 0.1735 - mse: 0.0452 - val_loss: 0.0452 - val_mae: 0.1735 - val_mse: 0.0452\n",
      "Epoch 118/250\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.0452 - mae: 0.1735 - mse: 0.0452 - val_loss: 0.0452 - val_mae: 0.1735 - val_mse: 0.0452\n",
      "Epoch 119/250\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.0452 - mae: 0.1735 - mse: 0.0452 - val_loss: 0.0452 - val_mae: 0.1734 - val_mse: 0.0452\n",
      "Epoch 120/250\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 0.0452 - mae: 0.1734 - mse: 0.0452 - val_loss: 0.0452 - val_mae: 0.1734 - val_mse: 0.0452\n",
      "Epoch 121/250\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 0.0452 - mae: 0.1734 - mse: 0.0452 - val_loss: 0.0452 - val_mae: 0.1734 - val_mse: 0.0452\n",
      "Epoch 122/250\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.0452 - mae: 0.1734 - mse: 0.0452 - val_loss: 0.0452 - val_mae: 0.1734 - val_mse: 0.0452\n",
      "Epoch 123/250\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 0.0452 - mae: 0.1734 - mse: 0.0452 - val_loss: 0.0452 - val_mae: 0.1734 - val_mse: 0.0452\n",
      "Epoch 124/250\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 0.0452 - mae: 0.1734 - mse: 0.0452 - val_loss: 0.0452 - val_mae: 0.1734 - val_mse: 0.0452\n",
      "Epoch 125/250\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 0.0452 - mae: 0.1734 - mse: 0.0452 - val_loss: 0.0452 - val_mae: 0.1734 - val_mse: 0.0452\n",
      "Epoch 126/250\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 0.0452 - mae: 0.1734 - mse: 0.0452 - val_loss: 0.0452 - val_mae: 0.1734 - val_mse: 0.0452\n",
      "Epoch 127/250\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 0.0452 - mae: 0.1734 - mse: 0.0452 - val_loss: 0.0452 - val_mae: 0.1734 - val_mse: 0.0452\n",
      "Epoch 128/250\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.0452 - mae: 0.1734 - mse: 0.0452 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 129/250\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 130/250\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 131/250\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 132/250\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 133/250\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 134/250\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 135/250\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 136/250\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 137/250\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1733 - val_mse: 0.0451\n",
      "Epoch 138/250\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.0451 - mae: 0.1733 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1733 - val_mse: 0.0451\n",
      "Epoch 139/250\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.0451 - mae: 0.1733 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1733 - val_mse: 0.0451\n",
      "Epoch 140/250\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 0.0451 - mae: 0.1733 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1733 - val_mse: 0.0451\n",
      "Epoch 141/250\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 0.0451 - mae: 0.1733 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1733 - val_mse: 0.0451\n",
      "Epoch 142/250\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 0.0451 - mae: 0.1733 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1733 - val_mse: 0.0451\n",
      "Epoch 143/250\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.0451 - mae: 0.1733 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1733 - val_mse: 0.0451\n",
      "Epoch 144/250\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.0451 - mae: 0.1733 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1733 - val_mse: 0.0451\n",
      "Epoch 145/250\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.0451 - mae: 0.1733 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1733 - val_mse: 0.0451\n",
      "Epoch 146/250\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 0.0451 - mae: 0.1733 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1733 - val_mse: 0.0451\n",
      "Epoch 147/250\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.0451 - mae: 0.1733 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1733 - val_mse: 0.0451\n",
      "Epoch 148/250\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 0.0451 - mae: 0.1733 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1733 - val_mse: 0.0451\n",
      "Epoch 149/250\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.0451 - mae: 0.1733 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1733 - val_mse: 0.0451\n",
      "Epoch 150/250\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 0.0451 - mae: 0.1733 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1733 - val_mse: 0.0451\n",
      "Epoch 151/250\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 0.0451 - mae: 0.1733 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1733 - val_mse: 0.0451\n",
      "Epoch 152/250\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 0.0451 - mae: 0.1733 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1733 - val_mse: 0.0451\n",
      "Epoch 153/250\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 0.0451 - mae: 0.1733 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1733 - val_mse: 0.0451\n",
      "Epoch 154/250\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 0.0451 - mae: 0.1733 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1733 - val_mse: 0.0451\n",
      "Epoch 155/250\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 0.0451 - mae: 0.1733 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1733 - val_mse: 0.0451\n",
      "Epoch 156/250\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 0.0451 - mae: 0.1733 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1733 - val_mse: 0.0451\n",
      "Epoch 157/250\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 0.0451 - mae: 0.1733 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1733 - val_mse: 0.0451\n",
      "Epoch 158/250\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 0.0451 - mae: 0.1733 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1733 - val_mse: 0.0451\n",
      "Epoch 159/250\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 0.0451 - mae: 0.1733 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1733 - val_mse: 0.0451\n",
      "Epoch 160/250\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 0.0451 - mae: 0.1733 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 161/250\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 162/250\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 163/250\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 164/250\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 165/250\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 166/250\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 167/250\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 168/250\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 169/250\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 170/250\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 171/250\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 172/250\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 173/250\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 174/250\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 175/250\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 176/250\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 177/250\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 178/250\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 179/250\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 180/250\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 181/250\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 182/250\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 183/250\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 184/250\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 185/250\n",
      "1/1 [==============================] - 0s 322ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 186/250\n",
      "1/1 [==============================] - 0s 358ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 187/250\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 188/250\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 189/250\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 190/250\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 191/250\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 192/250\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 193/250\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 194/250\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 195/250\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 196/250\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 197/250\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 198/250\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 199/250\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 200/250\n",
      "1/1 [==============================] - 0s 358ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 201/250\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 202/250\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 203/250\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 204/250\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 205/250\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 206/250\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 207/250\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 208/250\n",
      "1/1 [==============================] - 0s 322ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 209/250\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 210/250\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 211/250\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 212/250\n",
      "1/1 [==============================] - 0s 295ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 213/250\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 214/250\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 215/250\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 216/250\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 217/250\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 218/250\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 219/250\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 220/250\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 221/250\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 222/250\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 223/250\n",
      "1/1 [==============================] - 0s 336ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 224/250\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 225/250\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 226/250\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 227/250\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 228/250\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 229/250\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 230/250\n",
      "1/1 [==============================] - 0s 341ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 231/250\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 232/250\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 233/250\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 234/250\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 235/250\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 236/250\n",
      "1/1 [==============================] - 0s 321ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 237/250\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 238/250\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 239/250\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 240/250\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 241/250\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 242/250\n",
      "1/1 [==============================] - 0s 449ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 243/250\n",
      "1/1 [==============================] - 0s 354ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 244/250\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 245/250\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 246/250\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 247/250\n",
      "1/1 [==============================] - 0s 319ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 248/250\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 249/250\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n",
      "Epoch 250/250\n",
      "1/1 [==============================] - 0s 337ms/step - loss: 0.0451 - mae: 0.1734 - mse: 0.0451 - val_loss: 0.0451 - val_mae: 0.1734 - val_mse: 0.0451\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs     = 250\n",
    "history = model.fit(x_trainset[:batch_size], y_trainset[:batch_size],\n",
    "                   epochs = epochs,\n",
    "                   validation_data = (x_trainset[:batch_size], y_trainset[:batch_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e574456b-4207-4f12-95d6-6500df31570b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.ylim(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1fb78786-e197-4d00-8df8-01650720f135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 121ms/step\n"
     ]
    }
   ],
   "source": [
    "y_predicted = model.predict(x_trainset[:batch_size]) * (max_y - min_y) + min_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "cdad5bcb-3e58-42d1-a8c8-0785e0666c8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70, 20)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_trainset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2358d20c-773a-48a7-8fa0-b9317ce7fcb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = 22\n",
    "plt.plot(10**(y_predicted[i,:]))\n",
    "plt.plot(10**(y_trainset[i,:] * (max_y - min_y) + min_y), c = 'black')\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6972a4e-0c74-445b-bd5c-e52f5ef260bd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Now let's play the real game\n",
    "\n",
    "* Seems to converge to MSE = 0.054 (without batch normalization nor dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "48025db5-24d2-4bcf-9814-47b0e2c89d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_24 (Conv2D)          (None, 128, 128, 5)       1005      \n",
      "                                                                 \n",
      " batch_normalization_21 (Ba  (None, 128, 128, 5)       20        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_24 (Activation)  (None, 128, 128, 5)       0         \n",
      "                                                                 \n",
      " max_pooling2d_24 (MaxPooli  (None, 64, 64, 5)         0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_25 (Conv2D)          (None, 64, 64, 15)        1890      \n",
      "                                                                 \n",
      " batch_normalization_22 (Ba  (None, 64, 64, 15)        60        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_25 (Activation)  (None, 64, 64, 15)        0         \n",
      "                                                                 \n",
      " max_pooling2d_25 (MaxPooli  (None, 32, 32, 15)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_26 (Conv2D)          (None, 32, 32, 30)        11280     \n",
      "                                                                 \n",
      " batch_normalization_23 (Ba  (None, 32, 32, 30)        120       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_26 (Activation)  (None, 32, 32, 30)        0         \n",
      "                                                                 \n",
      " max_pooling2d_26 (MaxPooli  (None, 16, 16, 30)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 16, 16, 30)        0         \n",
      "                                                                 \n",
      " flatten_8 (Flatten)         (None, 7680)              0         \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 500)               3840500   \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 400)               200400    \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 300)               120300    \n",
      "                                                                 \n",
      " dense_35 (Dense)            (None, 200)               60200     \n",
      "                                                                 \n",
      " output (Dense)              (None, 20)                4020      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4239795 (16.17 MB)\n",
      "Trainable params: 4239695 (16.17 MB)\n",
      "Non-trainable params: 100 (400.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters = 5, kernel_size = (5,5), strides = 1 , padding = 'same', input_shape = input_shape))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(actFunction))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(filters = 15, kernel_size = (5,5), strides = 1 , padding = 'same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(actFunction))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(filters = 30, kernel_size = (5,5), strides = 1 , padding = 'same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(actFunction))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Dropout(0.2)) \n",
    "model.add(Flatten())\n",
    "\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dense(intermediate_dim1, activation = actFunction))\n",
    "\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dense(intermediate_dim2, activation = actFunction))\n",
    "\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dense(intermediate_dim3, activation = actFunction))\n",
    "\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dense(intermediate_dim4, activation = 'linear'))\n",
    "\n",
    "model.add(Dense(output_dim, name = 'output'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "cd25fed0-6299-4f95-84e3-ba1fb2a984fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model\n",
    "optimizer = optimizers.Adam(learning_rate = 1e-5, beta_1 = 0.9, beta_2 = 0.999, amsgrad = False)\n",
    "model.compile(optimizer = optimizer, loss = 'mse', metrics=['mae','mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "756035ff-ea8d-48e4-813a-2528bcabaac4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0530 - mae: 0.1808 - mse: 0.0530\n",
      "Epoch 1: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 250ms/step - loss: 0.0530 - mae: 0.1808 - mse: 0.0530 - val_loss: 0.0669 - val_mae: 0.2041 - val_mse: 0.0669\n",
      "Epoch 2/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0530 - mae: 0.1823 - mse: 0.0530\n",
      "Epoch 2: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0530 - mae: 0.1823 - mse: 0.0530 - val_loss: 0.0684 - val_mae: 0.2066 - val_mse: 0.0684\n",
      "Epoch 3/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0493 - mae: 0.1753 - mse: 0.0493\n",
      "Epoch 3: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 239ms/step - loss: 0.0493 - mae: 0.1753 - mse: 0.0493 - val_loss: 0.0696 - val_mae: 0.2086 - val_mse: 0.0696\n",
      "Epoch 4/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0516 - mae: 0.1799 - mse: 0.0516\n",
      "Epoch 4: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0516 - mae: 0.1799 - mse: 0.0516 - val_loss: 0.0703 - val_mae: 0.2099 - val_mse: 0.0703\n",
      "Epoch 5/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0538 - mae: 0.1847 - mse: 0.0538\n",
      "Epoch 5: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 249ms/step - loss: 0.0538 - mae: 0.1847 - mse: 0.0538 - val_loss: 0.0705 - val_mae: 0.2102 - val_mse: 0.0705\n",
      "Epoch 6/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0508 - mae: 0.1800 - mse: 0.0508\n",
      "Epoch 6: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 272ms/step - loss: 0.0508 - mae: 0.1800 - mse: 0.0508 - val_loss: 0.0701 - val_mae: 0.2098 - val_mse: 0.0701\n",
      "Epoch 7/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0510 - mae: 0.1810 - mse: 0.0510\n",
      "Epoch 7: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 239ms/step - loss: 0.0510 - mae: 0.1810 - mse: 0.0510 - val_loss: 0.0689 - val_mae: 0.2076 - val_mse: 0.0689\n",
      "Epoch 8/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0551 - mae: 0.1876 - mse: 0.0551\n",
      "Epoch 8: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 288ms/step - loss: 0.0551 - mae: 0.1876 - mse: 0.0551 - val_loss: 0.0673 - val_mae: 0.2051 - val_mse: 0.0673\n",
      "Epoch 9/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0528 - mae: 0.1827 - mse: 0.0528\n",
      "Epoch 9: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0528 - mae: 0.1827 - mse: 0.0528 - val_loss: 0.0662 - val_mae: 0.2030 - val_mse: 0.0662\n",
      "Epoch 10/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0495 - mae: 0.1779 - mse: 0.0495\n",
      "Epoch 10: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 241ms/step - loss: 0.0495 - mae: 0.1779 - mse: 0.0495 - val_loss: 0.0672 - val_mae: 0.2049 - val_mse: 0.0672\n",
      "Epoch 11/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0549 - mae: 0.1863 - mse: 0.0549\n",
      "Epoch 11: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 247ms/step - loss: 0.0549 - mae: 0.1863 - mse: 0.0549 - val_loss: 0.0682 - val_mae: 0.2067 - val_mse: 0.0682\n",
      "Epoch 12/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0544 - mae: 0.1838 - mse: 0.0544\n",
      "Epoch 12: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0544 - mae: 0.1838 - mse: 0.0544 - val_loss: 0.0692 - val_mae: 0.2084 - val_mse: 0.0692\n",
      "Epoch 13/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0521 - mae: 0.1810 - mse: 0.0521\n",
      "Epoch 13: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0521 - mae: 0.1810 - mse: 0.0521 - val_loss: 0.0719 - val_mae: 0.2131 - val_mse: 0.0719\n",
      "Epoch 14/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0542 - mae: 0.1847 - mse: 0.0542\n",
      "Epoch 14: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0542 - mae: 0.1847 - mse: 0.0542 - val_loss: 0.0718 - val_mae: 0.2130 - val_mse: 0.0718\n",
      "Epoch 15/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0509 - mae: 0.1788 - mse: 0.0509\n",
      "Epoch 15: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0509 - mae: 0.1788 - mse: 0.0509 - val_loss: 0.0691 - val_mae: 0.2080 - val_mse: 0.0691\n",
      "Epoch 16/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0513 - mae: 0.1785 - mse: 0.0513\n",
      "Epoch 16: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 239ms/step - loss: 0.0513 - mae: 0.1785 - mse: 0.0513 - val_loss: 0.0682 - val_mae: 0.2064 - val_mse: 0.0682\n",
      "Epoch 17/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0510 - mae: 0.1785 - mse: 0.0510\n",
      "Epoch 17: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0510 - mae: 0.1785 - mse: 0.0510 - val_loss: 0.0678 - val_mae: 0.2056 - val_mse: 0.0678\n",
      "Epoch 18/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0510 - mae: 0.1780 - mse: 0.0510\n",
      "Epoch 18: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 246ms/step - loss: 0.0510 - mae: 0.1780 - mse: 0.0510 - val_loss: 0.0677 - val_mae: 0.2053 - val_mse: 0.0677\n",
      "Epoch 19/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0531 - mae: 0.1832 - mse: 0.0531\n",
      "Epoch 19: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 243ms/step - loss: 0.0531 - mae: 0.1832 - mse: 0.0531 - val_loss: 0.0673 - val_mae: 0.2047 - val_mse: 0.0673\n",
      "Epoch 20/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0505 - mae: 0.1785 - mse: 0.0505\n",
      "Epoch 20: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 239ms/step - loss: 0.0505 - mae: 0.1785 - mse: 0.0505 - val_loss: 0.0662 - val_mae: 0.2027 - val_mse: 0.0662\n",
      "Epoch 21/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0491 - mae: 0.1743 - mse: 0.0491\n",
      "Epoch 21: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 242ms/step - loss: 0.0491 - mae: 0.1743 - mse: 0.0491 - val_loss: 0.0659 - val_mae: 0.2019 - val_mse: 0.0659\n",
      "Epoch 22/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0524 - mae: 0.1812 - mse: 0.0524\n",
      "Epoch 22: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 277ms/step - loss: 0.0524 - mae: 0.1812 - mse: 0.0524 - val_loss: 0.0641 - val_mae: 0.1983 - val_mse: 0.0641\n",
      "Epoch 23/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0538 - mae: 0.1839 - mse: 0.0538\n",
      "Epoch 23: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0538 - mae: 0.1839 - mse: 0.0538 - val_loss: 0.0627 - val_mae: 0.1956 - val_mse: 0.0627\n",
      "Epoch 24/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0506 - mae: 0.1770 - mse: 0.0506\n",
      "Epoch 24: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0506 - mae: 0.1770 - mse: 0.0506 - val_loss: 0.0638 - val_mae: 0.1974 - val_mse: 0.0638\n",
      "Epoch 25/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0498 - mae: 0.1758 - mse: 0.0498\n",
      "Epoch 25: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 260ms/step - loss: 0.0498 - mae: 0.1758 - mse: 0.0498 - val_loss: 0.0664 - val_mae: 0.2020 - val_mse: 0.0664\n",
      "Epoch 26/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0520 - mae: 0.1803 - mse: 0.0520\n",
      "Epoch 26: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 252ms/step - loss: 0.0520 - mae: 0.1803 - mse: 0.0520 - val_loss: 0.0684 - val_mae: 0.2054 - val_mse: 0.0684\n",
      "Epoch 27/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0517 - mae: 0.1801 - mse: 0.0517\n",
      "Epoch 27: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 258ms/step - loss: 0.0517 - mae: 0.1801 - mse: 0.0517 - val_loss: 0.0697 - val_mae: 0.2081 - val_mse: 0.0697\n",
      "Epoch 28/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0536 - mae: 0.1827 - mse: 0.0536\n",
      "Epoch 28: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 255ms/step - loss: 0.0536 - mae: 0.1827 - mse: 0.0536 - val_loss: 0.0697 - val_mae: 0.2084 - val_mse: 0.0697\n",
      "Epoch 29/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0509 - mae: 0.1807 - mse: 0.0509\n",
      "Epoch 29: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 260ms/step - loss: 0.0509 - mae: 0.1807 - mse: 0.0509 - val_loss: 0.0699 - val_mae: 0.2091 - val_mse: 0.0699\n",
      "Epoch 30/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0538 - mae: 0.1827 - mse: 0.0538\n",
      "Epoch 30: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 265ms/step - loss: 0.0538 - mae: 0.1827 - mse: 0.0538 - val_loss: 0.0697 - val_mae: 0.2091 - val_mse: 0.0697\n",
      "Epoch 31/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0516 - mae: 0.1821 - mse: 0.0516\n",
      "Epoch 31: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 253ms/step - loss: 0.0516 - mae: 0.1821 - mse: 0.0516 - val_loss: 0.0688 - val_mae: 0.2075 - val_mse: 0.0688\n",
      "Epoch 32/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0538 - mae: 0.1832 - mse: 0.0538\n",
      "Epoch 32: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 253ms/step - loss: 0.0538 - mae: 0.1832 - mse: 0.0538 - val_loss: 0.0686 - val_mae: 0.2071 - val_mse: 0.0686\n",
      "Epoch 33/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0510 - mae: 0.1800 - mse: 0.0510\n",
      "Epoch 33: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 246ms/step - loss: 0.0510 - mae: 0.1800 - mse: 0.0510 - val_loss: 0.0680 - val_mae: 0.2059 - val_mse: 0.0680\n",
      "Epoch 34/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0521 - mae: 0.1809 - mse: 0.0521\n",
      "Epoch 34: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 244ms/step - loss: 0.0521 - mae: 0.1809 - mse: 0.0521 - val_loss: 0.0664 - val_mae: 0.2027 - val_mse: 0.0664\n",
      "Epoch 35/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0505 - mae: 0.1796 - mse: 0.0505\n",
      "Epoch 35: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 240ms/step - loss: 0.0505 - mae: 0.1796 - mse: 0.0505 - val_loss: 0.0641 - val_mae: 0.1983 - val_mse: 0.0641\n",
      "Epoch 36/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0524 - mae: 0.1806 - mse: 0.0524\n",
      "Epoch 36: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 247ms/step - loss: 0.0524 - mae: 0.1806 - mse: 0.0524 - val_loss: 0.0628 - val_mae: 0.1960 - val_mse: 0.0628\n",
      "Epoch 37/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0510 - mae: 0.1781 - mse: 0.0510\n",
      "Epoch 37: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 254ms/step - loss: 0.0510 - mae: 0.1781 - mse: 0.0510 - val_loss: 0.0621 - val_mae: 0.1948 - val_mse: 0.0621\n",
      "Epoch 38/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0514 - mae: 0.1797 - mse: 0.0514\n",
      "Epoch 38: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 263ms/step - loss: 0.0514 - mae: 0.1797 - mse: 0.0514 - val_loss: 0.0623 - val_mae: 0.1952 - val_mse: 0.0623\n",
      "Epoch 39/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0507 - mae: 0.1793 - mse: 0.0507\n",
      "Epoch 39: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 270ms/step - loss: 0.0507 - mae: 0.1793 - mse: 0.0507 - val_loss: 0.0632 - val_mae: 0.1967 - val_mse: 0.0632\n",
      "Epoch 40/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0487 - mae: 0.1736 - mse: 0.0487\n",
      "Epoch 40: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 281ms/step - loss: 0.0487 - mae: 0.1736 - mse: 0.0487 - val_loss: 0.0649 - val_mae: 0.1997 - val_mse: 0.0649\n",
      "Epoch 41/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0515 - mae: 0.1795 - mse: 0.0515\n",
      "Epoch 41: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 266ms/step - loss: 0.0515 - mae: 0.1795 - mse: 0.0515 - val_loss: 0.0669 - val_mae: 0.2032 - val_mse: 0.0669\n",
      "Epoch 42/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0490 - mae: 0.1756 - mse: 0.0490\n",
      "Epoch 42: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 258ms/step - loss: 0.0490 - mae: 0.1756 - mse: 0.0490 - val_loss: 0.0682 - val_mae: 0.2056 - val_mse: 0.0682\n",
      "Epoch 43/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0500 - mae: 0.1750 - mse: 0.0500\n",
      "Epoch 43: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 254ms/step - loss: 0.0500 - mae: 0.1750 - mse: 0.0500 - val_loss: 0.0691 - val_mae: 0.2074 - val_mse: 0.0691\n",
      "Epoch 44/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0508 - mae: 0.1781 - mse: 0.0508\n",
      "Epoch 44: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 250ms/step - loss: 0.0508 - mae: 0.1781 - mse: 0.0508 - val_loss: 0.0678 - val_mae: 0.2053 - val_mse: 0.0678\n",
      "Epoch 45/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0504 - mae: 0.1758 - mse: 0.0504\n",
      "Epoch 45: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 254ms/step - loss: 0.0504 - mae: 0.1758 - mse: 0.0504 - val_loss: 0.0661 - val_mae: 0.2023 - val_mse: 0.0661\n",
      "Epoch 46/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0489 - mae: 0.1770 - mse: 0.0489\n",
      "Epoch 46: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 250ms/step - loss: 0.0489 - mae: 0.1770 - mse: 0.0489 - val_loss: 0.0648 - val_mae: 0.2002 - val_mse: 0.0648\n",
      "Epoch 47/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0494 - mae: 0.1765 - mse: 0.0494\n",
      "Epoch 47: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 251ms/step - loss: 0.0494 - mae: 0.1765 - mse: 0.0494 - val_loss: 0.0640 - val_mae: 0.1988 - val_mse: 0.0640\n",
      "Epoch 48/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0515 - mae: 0.1796 - mse: 0.0515\n",
      "Epoch 48: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 250ms/step - loss: 0.0515 - mae: 0.1796 - mse: 0.0515 - val_loss: 0.0634 - val_mae: 0.1977 - val_mse: 0.0634\n",
      "Epoch 49/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0484 - mae: 0.1737 - mse: 0.0484\n",
      "Epoch 49: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 260ms/step - loss: 0.0484 - mae: 0.1737 - mse: 0.0484 - val_loss: 0.0626 - val_mae: 0.1963 - val_mse: 0.0626\n",
      "Epoch 50/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0519 - mae: 0.1813 - mse: 0.0519\n",
      "Epoch 50: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 253ms/step - loss: 0.0519 - mae: 0.1813 - mse: 0.0519 - val_loss: 0.0623 - val_mae: 0.1957 - val_mse: 0.0623\n",
      "Epoch 51/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0510 - mae: 0.1802 - mse: 0.0510\n",
      "Epoch 51: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 256ms/step - loss: 0.0510 - mae: 0.1802 - mse: 0.0510 - val_loss: 0.0618 - val_mae: 0.1949 - val_mse: 0.0618\n",
      "Epoch 52/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0530 - mae: 0.1839 - mse: 0.0530\n",
      "Epoch 52: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 302ms/step - loss: 0.0530 - mae: 0.1839 - mse: 0.0530 - val_loss: 0.0618 - val_mae: 0.1944 - val_mse: 0.0618\n",
      "Epoch 53/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0494 - mae: 0.1770 - mse: 0.0494\n",
      "Epoch 53: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 269ms/step - loss: 0.0494 - mae: 0.1770 - mse: 0.0494 - val_loss: 0.0623 - val_mae: 0.1949 - val_mse: 0.0623\n",
      "Epoch 54/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0528 - mae: 0.1785 - mse: 0.0528\n",
      "Epoch 54: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 267ms/step - loss: 0.0528 - mae: 0.1785 - mse: 0.0528 - val_loss: 0.0647 - val_mae: 0.1990 - val_mse: 0.0647\n",
      "Epoch 55/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0525 - mae: 0.1818 - mse: 0.0525\n",
      "Epoch 55: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 262ms/step - loss: 0.0525 - mae: 0.1818 - mse: 0.0525 - val_loss: 0.0672 - val_mae: 0.2036 - val_mse: 0.0672\n",
      "Epoch 56/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0492 - mae: 0.1768 - mse: 0.0492\n",
      "Epoch 56: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 254ms/step - loss: 0.0492 - mae: 0.1768 - mse: 0.0492 - val_loss: 0.0682 - val_mae: 0.2055 - val_mse: 0.0682\n",
      "Epoch 57/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0487 - mae: 0.1742 - mse: 0.0487\n",
      "Epoch 57: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 248ms/step - loss: 0.0487 - mae: 0.1742 - mse: 0.0487 - val_loss: 0.0666 - val_mae: 0.2025 - val_mse: 0.0666\n",
      "Epoch 58/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0488 - mae: 0.1763 - mse: 0.0488\n",
      "Epoch 58: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 240ms/step - loss: 0.0488 - mae: 0.1763 - mse: 0.0488 - val_loss: 0.0644 - val_mae: 0.1987 - val_mse: 0.0644\n",
      "Epoch 59/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0508 - mae: 0.1789 - mse: 0.0508\n",
      "Epoch 59: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0508 - mae: 0.1789 - mse: 0.0508 - val_loss: 0.0618 - val_mae: 0.1944 - val_mse: 0.0618\n",
      "Epoch 60/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0511 - mae: 0.1773 - mse: 0.0511\n",
      "Epoch 60: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 243ms/step - loss: 0.0511 - mae: 0.1773 - mse: 0.0511 - val_loss: 0.0603 - val_mae: 0.1919 - val_mse: 0.0603\n",
      "Epoch 61/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0529 - mae: 0.1809 - mse: 0.0529\n",
      "Epoch 61: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 264ms/step - loss: 0.0529 - mae: 0.1809 - mse: 0.0529 - val_loss: 0.0602 - val_mae: 0.1917 - val_mse: 0.0602\n",
      "Epoch 62/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0494 - mae: 0.1752 - mse: 0.0494\n",
      "Epoch 62: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 260ms/step - loss: 0.0494 - mae: 0.1752 - mse: 0.0494 - val_loss: 0.0606 - val_mae: 0.1923 - val_mse: 0.0606\n",
      "Epoch 63/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0497 - mae: 0.1770 - mse: 0.0497\n",
      "Epoch 63: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 253ms/step - loss: 0.0497 - mae: 0.1770 - mse: 0.0497 - val_loss: 0.0626 - val_mae: 0.1956 - val_mse: 0.0626\n",
      "Epoch 64/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0507 - mae: 0.1778 - mse: 0.0507\n",
      "Epoch 64: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 256ms/step - loss: 0.0507 - mae: 0.1778 - mse: 0.0507 - val_loss: 0.0632 - val_mae: 0.1967 - val_mse: 0.0632\n",
      "Epoch 65/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0498 - mae: 0.1773 - mse: 0.0498\n",
      "Epoch 65: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 244ms/step - loss: 0.0498 - mae: 0.1773 - mse: 0.0498 - val_loss: 0.0642 - val_mae: 0.1984 - val_mse: 0.0642\n",
      "Epoch 66/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0480 - mae: 0.1740 - mse: 0.0480\n",
      "Epoch 66: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 292ms/step - loss: 0.0480 - mae: 0.1740 - mse: 0.0480 - val_loss: 0.0642 - val_mae: 0.1985 - val_mse: 0.0642\n",
      "Epoch 67/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0513 - mae: 0.1767 - mse: 0.0513\n",
      "Epoch 67: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 243ms/step - loss: 0.0513 - mae: 0.1767 - mse: 0.0513 - val_loss: 0.0643 - val_mae: 0.1986 - val_mse: 0.0643\n",
      "Epoch 68/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0495 - mae: 0.1772 - mse: 0.0495\n",
      "Epoch 68: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 226ms/step - loss: 0.0495 - mae: 0.1772 - mse: 0.0495 - val_loss: 0.0643 - val_mae: 0.1983 - val_mse: 0.0643\n",
      "Epoch 69/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0508 - mae: 0.1795 - mse: 0.0508\n",
      "Epoch 69: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 242ms/step - loss: 0.0508 - mae: 0.1795 - mse: 0.0508 - val_loss: 0.0638 - val_mae: 0.1974 - val_mse: 0.0638\n",
      "Epoch 70/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0482 - mae: 0.1733 - mse: 0.0482\n",
      "Epoch 70: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 245ms/step - loss: 0.0482 - mae: 0.1733 - mse: 0.0482 - val_loss: 0.0624 - val_mae: 0.1950 - val_mse: 0.0624\n",
      "Epoch 71/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0489 - mae: 0.1746 - mse: 0.0489\n",
      "Epoch 71: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0489 - mae: 0.1746 - mse: 0.0489 - val_loss: 0.0612 - val_mae: 0.1930 - val_mse: 0.0612\n",
      "Epoch 72/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0489 - mae: 0.1756 - mse: 0.0489\n",
      "Epoch 72: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0489 - mae: 0.1756 - mse: 0.0489 - val_loss: 0.0605 - val_mae: 0.1919 - val_mse: 0.0605\n",
      "Epoch 73/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0498 - mae: 0.1772 - mse: 0.0498\n",
      "Epoch 73: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0498 - mae: 0.1772 - mse: 0.0498 - val_loss: 0.0603 - val_mae: 0.1916 - val_mse: 0.0603\n",
      "Epoch 74/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0483 - mae: 0.1754 - mse: 0.0483\n",
      "Epoch 74: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0483 - mae: 0.1754 - mse: 0.0483 - val_loss: 0.0595 - val_mae: 0.1903 - val_mse: 0.0595\n",
      "Epoch 75/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0489 - mae: 0.1754 - mse: 0.0489\n",
      "Epoch 75: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 243ms/step - loss: 0.0489 - mae: 0.1754 - mse: 0.0489 - val_loss: 0.0585 - val_mae: 0.1885 - val_mse: 0.0585\n",
      "Epoch 76/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0509 - mae: 0.1784 - mse: 0.0509\n",
      "Epoch 76: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0509 - mae: 0.1784 - mse: 0.0509 - val_loss: 0.0575 - val_mae: 0.1867 - val_mse: 0.0575\n",
      "Epoch 77/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0483 - mae: 0.1764 - mse: 0.0483\n",
      "Epoch 77: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0483 - mae: 0.1764 - mse: 0.0483 - val_loss: 0.0572 - val_mae: 0.1865 - val_mse: 0.0572\n",
      "Epoch 78/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0515 - mae: 0.1787 - mse: 0.0515\n",
      "Epoch 78: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0515 - mae: 0.1787 - mse: 0.0515 - val_loss: 0.0578 - val_mae: 0.1875 - val_mse: 0.0578\n",
      "Epoch 79/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0471 - mae: 0.1712 - mse: 0.0471\n",
      "Epoch 79: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0471 - mae: 0.1712 - mse: 0.0471 - val_loss: 0.0588 - val_mae: 0.1892 - val_mse: 0.0588\n",
      "Epoch 80/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0487 - mae: 0.1770 - mse: 0.0487\n",
      "Epoch 80: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0487 - mae: 0.1770 - mse: 0.0487 - val_loss: 0.0604 - val_mae: 0.1920 - val_mse: 0.0604\n",
      "Epoch 81/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0508 - mae: 0.1784 - mse: 0.0508\n",
      "Epoch 81: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0508 - mae: 0.1784 - mse: 0.0508 - val_loss: 0.0616 - val_mae: 0.1941 - val_mse: 0.0616\n",
      "Epoch 82/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0516 - mae: 0.1815 - mse: 0.0516\n",
      "Epoch 82: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0516 - mae: 0.1815 - mse: 0.0516 - val_loss: 0.0619 - val_mae: 0.1945 - val_mse: 0.0619\n",
      "Epoch 83/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0497 - mae: 0.1784 - mse: 0.0497\n",
      "Epoch 83: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 252ms/step - loss: 0.0497 - mae: 0.1784 - mse: 0.0497 - val_loss: 0.0622 - val_mae: 0.1951 - val_mse: 0.0622\n",
      "Epoch 84/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0506 - mae: 0.1792 - mse: 0.0506\n",
      "Epoch 84: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0506 - mae: 0.1792 - mse: 0.0506 - val_loss: 0.0634 - val_mae: 0.1972 - val_mse: 0.0634\n",
      "Epoch 85/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0486 - mae: 0.1739 - mse: 0.0486\n",
      "Epoch 85: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0486 - mae: 0.1739 - mse: 0.0486 - val_loss: 0.0651 - val_mae: 0.2001 - val_mse: 0.0651\n",
      "Epoch 86/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0491 - mae: 0.1744 - mse: 0.0491\n",
      "Epoch 86: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.0491 - mae: 0.1744 - mse: 0.0491 - val_loss: 0.0654 - val_mae: 0.2008 - val_mse: 0.0654\n",
      "Epoch 87/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0475 - mae: 0.1713 - mse: 0.0475\n",
      "Epoch 87: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0475 - mae: 0.1713 - mse: 0.0475 - val_loss: 0.0638 - val_mae: 0.1980 - val_mse: 0.0638\n",
      "Epoch 88/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0508 - mae: 0.1790 - mse: 0.0508\n",
      "Epoch 88: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 240ms/step - loss: 0.0508 - mae: 0.1790 - mse: 0.0508 - val_loss: 0.0624 - val_mae: 0.1956 - val_mse: 0.0624\n",
      "Epoch 89/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0507 - mae: 0.1787 - mse: 0.0507\n",
      "Epoch 89: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 225ms/step - loss: 0.0507 - mae: 0.1787 - mse: 0.0507 - val_loss: 0.0608 - val_mae: 0.1929 - val_mse: 0.0608\n",
      "Epoch 90/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0477 - mae: 0.1738 - mse: 0.0477\n",
      "Epoch 90: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0477 - mae: 0.1738 - mse: 0.0477 - val_loss: 0.0598 - val_mae: 0.1911 - val_mse: 0.0598\n",
      "Epoch 91/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0525 - mae: 0.1821 - mse: 0.0525\n",
      "Epoch 91: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 245ms/step - loss: 0.0525 - mae: 0.1821 - mse: 0.0525 - val_loss: 0.0605 - val_mae: 0.1921 - val_mse: 0.0605\n",
      "Epoch 92/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0483 - mae: 0.1752 - mse: 0.0483\n",
      "Epoch 92: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 241ms/step - loss: 0.0483 - mae: 0.1752 - mse: 0.0483 - val_loss: 0.0623 - val_mae: 0.1949 - val_mse: 0.0623\n",
      "Epoch 93/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0483 - mae: 0.1756 - mse: 0.0483\n",
      "Epoch 93: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 226ms/step - loss: 0.0483 - mae: 0.1756 - mse: 0.0483 - val_loss: 0.0644 - val_mae: 0.1985 - val_mse: 0.0644\n",
      "Epoch 94/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0512 - mae: 0.1828 - mse: 0.0512\n",
      "Epoch 94: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 240ms/step - loss: 0.0512 - mae: 0.1828 - mse: 0.0512 - val_loss: 0.0660 - val_mae: 0.2016 - val_mse: 0.0660\n",
      "Epoch 95/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0505 - mae: 0.1799 - mse: 0.0505\n",
      "Epoch 95: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0505 - mae: 0.1799 - mse: 0.0505 - val_loss: 0.0666 - val_mae: 0.2030 - val_mse: 0.0666\n",
      "Epoch 96/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0512 - mae: 0.1791 - mse: 0.0512\n",
      "Epoch 96: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0512 - mae: 0.1791 - mse: 0.0512 - val_loss: 0.0656 - val_mae: 0.2011 - val_mse: 0.0656\n",
      "Epoch 97/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0498 - mae: 0.1784 - mse: 0.0498\n",
      "Epoch 97: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 227ms/step - loss: 0.0498 - mae: 0.1784 - mse: 0.0498 - val_loss: 0.0643 - val_mae: 0.1989 - val_mse: 0.0643\n",
      "Epoch 98/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0469 - mae: 0.1716 - mse: 0.0469\n",
      "Epoch 98: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0469 - mae: 0.1716 - mse: 0.0469 - val_loss: 0.0628 - val_mae: 0.1962 - val_mse: 0.0628\n",
      "Epoch 99/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0470 - mae: 0.1737 - mse: 0.0470\n",
      "Epoch 99: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.0470 - mae: 0.1737 - mse: 0.0470 - val_loss: 0.0613 - val_mae: 0.1935 - val_mse: 0.0613\n",
      "Epoch 100/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0477 - mae: 0.1722 - mse: 0.0477\n",
      "Epoch 100: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 227ms/step - loss: 0.0477 - mae: 0.1722 - mse: 0.0477 - val_loss: 0.0603 - val_mae: 0.1917 - val_mse: 0.0603\n",
      "Epoch 101/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0493 - mae: 0.1753 - mse: 0.0493\n",
      "Epoch 101: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 239ms/step - loss: 0.0493 - mae: 0.1753 - mse: 0.0493 - val_loss: 0.0595 - val_mae: 0.1903 - val_mse: 0.0595\n",
      "Epoch 102/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0482 - mae: 0.1742 - mse: 0.0482\n",
      "Epoch 102: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 225ms/step - loss: 0.0482 - mae: 0.1742 - mse: 0.0482 - val_loss: 0.0587 - val_mae: 0.1890 - val_mse: 0.0587\n",
      "Epoch 103/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0474 - mae: 0.1730 - mse: 0.0474\n",
      "Epoch 103: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0474 - mae: 0.1730 - mse: 0.0474 - val_loss: 0.0579 - val_mae: 0.1874 - val_mse: 0.0579\n",
      "Epoch 104/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0484 - mae: 0.1733 - mse: 0.0484\n",
      "Epoch 104: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0484 - mae: 0.1733 - mse: 0.0484 - val_loss: 0.0578 - val_mae: 0.1873 - val_mse: 0.0578\n",
      "Epoch 105/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0523 - mae: 0.1822 - mse: 0.0523\n",
      "Epoch 105: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0523 - mae: 0.1822 - mse: 0.0523 - val_loss: 0.0583 - val_mae: 0.1880 - val_mse: 0.0583\n",
      "Epoch 106/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0483 - mae: 0.1725 - mse: 0.0483\n",
      "Epoch 106: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 239ms/step - loss: 0.0483 - mae: 0.1725 - mse: 0.0483 - val_loss: 0.0587 - val_mae: 0.1887 - val_mse: 0.0587\n",
      "Epoch 107/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0498 - mae: 0.1756 - mse: 0.0498\n",
      "Epoch 107: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 224ms/step - loss: 0.0498 - mae: 0.1756 - mse: 0.0498 - val_loss: 0.0584 - val_mae: 0.1883 - val_mse: 0.0584\n",
      "Epoch 108/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0492 - mae: 0.1771 - mse: 0.0492\n",
      "Epoch 108: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 239ms/step - loss: 0.0492 - mae: 0.1771 - mse: 0.0492 - val_loss: 0.0582 - val_mae: 0.1881 - val_mse: 0.0582\n",
      "Epoch 109/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0510 - mae: 0.1789 - mse: 0.0510\n",
      "Epoch 109: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 224ms/step - loss: 0.0510 - mae: 0.1789 - mse: 0.0510 - val_loss: 0.0584 - val_mae: 0.1884 - val_mse: 0.0584\n",
      "Epoch 110/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0496 - mae: 0.1752 - mse: 0.0496\n",
      "Epoch 110: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0496 - mae: 0.1752 - mse: 0.0496 - val_loss: 0.0585 - val_mae: 0.1885 - val_mse: 0.0585\n",
      "Epoch 111/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0481 - mae: 0.1747 - mse: 0.0481\n",
      "Epoch 111: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0481 - mae: 0.1747 - mse: 0.0481 - val_loss: 0.0591 - val_mae: 0.1895 - val_mse: 0.0591\n",
      "Epoch 112/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0472 - mae: 0.1724 - mse: 0.0472\n",
      "Epoch 112: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.0472 - mae: 0.1724 - mse: 0.0472 - val_loss: 0.0591 - val_mae: 0.1893 - val_mse: 0.0591\n",
      "Epoch 113/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0469 - mae: 0.1713 - mse: 0.0469\n",
      "Epoch 113: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0469 - mae: 0.1713 - mse: 0.0469 - val_loss: 0.0585 - val_mae: 0.1883 - val_mse: 0.0585\n",
      "Epoch 114/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0498 - mae: 0.1759 - mse: 0.0498\n",
      "Epoch 114: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0498 - mae: 0.1759 - mse: 0.0498 - val_loss: 0.0583 - val_mae: 0.1879 - val_mse: 0.0583\n",
      "Epoch 115/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0495 - mae: 0.1754 - mse: 0.0495\n",
      "Epoch 115: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 241ms/step - loss: 0.0495 - mae: 0.1754 - mse: 0.0495 - val_loss: 0.0584 - val_mae: 0.1882 - val_mse: 0.0584\n",
      "Epoch 116/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0486 - mae: 0.1747 - mse: 0.0486\n",
      "Epoch 116: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 227ms/step - loss: 0.0486 - mae: 0.1747 - mse: 0.0486 - val_loss: 0.0586 - val_mae: 0.1886 - val_mse: 0.0586\n",
      "Epoch 117/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0499 - mae: 0.1784 - mse: 0.0499\n",
      "Epoch 117: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0499 - mae: 0.1784 - mse: 0.0499 - val_loss: 0.0592 - val_mae: 0.1895 - val_mse: 0.0592\n",
      "Epoch 118/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0489 - mae: 0.1752 - mse: 0.0489\n",
      "Epoch 118: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 224ms/step - loss: 0.0489 - mae: 0.1752 - mse: 0.0489 - val_loss: 0.0612 - val_mae: 0.1931 - val_mse: 0.0612\n",
      "Epoch 119/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0481 - mae: 0.1747 - mse: 0.0481\n",
      "Epoch 119: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0481 - mae: 0.1747 - mse: 0.0481 - val_loss: 0.0622 - val_mae: 0.1949 - val_mse: 0.0622\n",
      "Epoch 120/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0513 - mae: 0.1807 - mse: 0.0513\n",
      "Epoch 120: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 240ms/step - loss: 0.0513 - mae: 0.1807 - mse: 0.0513 - val_loss: 0.0618 - val_mae: 0.1943 - val_mse: 0.0618\n",
      "Epoch 121/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0464 - mae: 0.1716 - mse: 0.0464\n",
      "Epoch 121: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 242ms/step - loss: 0.0464 - mae: 0.1716 - mse: 0.0464 - val_loss: 0.0615 - val_mae: 0.1937 - val_mse: 0.0615\n",
      "Epoch 122/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0476 - mae: 0.1717 - mse: 0.0476\n",
      "Epoch 122: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 245ms/step - loss: 0.0476 - mae: 0.1717 - mse: 0.0476 - val_loss: 0.0606 - val_mae: 0.1921 - val_mse: 0.0606\n",
      "Epoch 123/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0468 - mae: 0.1711 - mse: 0.0468\n",
      "Epoch 123: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0468 - mae: 0.1711 - mse: 0.0468 - val_loss: 0.0606 - val_mae: 0.1920 - val_mse: 0.0606\n",
      "Epoch 124/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0497 - mae: 0.1770 - mse: 0.0497\n",
      "Epoch 124: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 240ms/step - loss: 0.0497 - mae: 0.1770 - mse: 0.0497 - val_loss: 0.0616 - val_mae: 0.1938 - val_mse: 0.0616\n",
      "Epoch 125/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0471 - mae: 0.1735 - mse: 0.0471\n",
      "Epoch 125: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 241ms/step - loss: 0.0471 - mae: 0.1735 - mse: 0.0471 - val_loss: 0.0623 - val_mae: 0.1949 - val_mse: 0.0623\n",
      "Epoch 126/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0479 - mae: 0.1732 - mse: 0.0479\n",
      "Epoch 126: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 226ms/step - loss: 0.0479 - mae: 0.1732 - mse: 0.0479 - val_loss: 0.0632 - val_mae: 0.1967 - val_mse: 0.0632\n",
      "Epoch 127/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0487 - mae: 0.1762 - mse: 0.0487\n",
      "Epoch 127: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 268ms/step - loss: 0.0487 - mae: 0.1762 - mse: 0.0487 - val_loss: 0.0637 - val_mae: 0.1978 - val_mse: 0.0637\n",
      "Epoch 128/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0476 - mae: 0.1734 - mse: 0.0476\n",
      "Epoch 128: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0476 - mae: 0.1734 - mse: 0.0476 - val_loss: 0.0623 - val_mae: 0.1951 - val_mse: 0.0623\n",
      "Epoch 129/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0475 - mae: 0.1738 - mse: 0.0475\n",
      "Epoch 129: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 246ms/step - loss: 0.0475 - mae: 0.1738 - mse: 0.0475 - val_loss: 0.0607 - val_mae: 0.1921 - val_mse: 0.0607\n",
      "Epoch 130/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0479 - mae: 0.1734 - mse: 0.0479\n",
      "Epoch 130: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0479 - mae: 0.1734 - mse: 0.0479 - val_loss: 0.0602 - val_mae: 0.1911 - val_mse: 0.0602\n",
      "Epoch 131/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0490 - mae: 0.1757 - mse: 0.0490\n",
      "Epoch 131: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0490 - mae: 0.1757 - mse: 0.0490 - val_loss: 0.0599 - val_mae: 0.1905 - val_mse: 0.0599\n",
      "Epoch 132/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0489 - mae: 0.1757 - mse: 0.0489\n",
      "Epoch 132: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 227ms/step - loss: 0.0489 - mae: 0.1757 - mse: 0.0489 - val_loss: 0.0602 - val_mae: 0.1909 - val_mse: 0.0602\n",
      "Epoch 133/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0472 - mae: 0.1701 - mse: 0.0472\n",
      "Epoch 133: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0472 - mae: 0.1701 - mse: 0.0472 - val_loss: 0.0617 - val_mae: 0.1934 - val_mse: 0.0617\n",
      "Epoch 134/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0474 - mae: 0.1732 - mse: 0.0474\n",
      "Epoch 134: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0474 - mae: 0.1732 - mse: 0.0474 - val_loss: 0.0625 - val_mae: 0.1946 - val_mse: 0.0625\n",
      "Epoch 135/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0472 - mae: 0.1745 - mse: 0.0472\n",
      "Epoch 135: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 239ms/step - loss: 0.0472 - mae: 0.1745 - mse: 0.0472 - val_loss: 0.0626 - val_mae: 0.1947 - val_mse: 0.0626\n",
      "Epoch 136/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0485 - mae: 0.1734 - mse: 0.0485\n",
      "Epoch 136: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.0485 - mae: 0.1734 - mse: 0.0485 - val_loss: 0.0620 - val_mae: 0.1939 - val_mse: 0.0620\n",
      "Epoch 137/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0485 - mae: 0.1745 - mse: 0.0485\n",
      "Epoch 137: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 226ms/step - loss: 0.0485 - mae: 0.1745 - mse: 0.0485 - val_loss: 0.0613 - val_mae: 0.1927 - val_mse: 0.0613\n",
      "Epoch 138/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0473 - mae: 0.1723 - mse: 0.0473\n",
      "Epoch 138: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0473 - mae: 0.1723 - mse: 0.0473 - val_loss: 0.0606 - val_mae: 0.1917 - val_mse: 0.0606\n",
      "Epoch 139/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0463 - mae: 0.1696 - mse: 0.0463\n",
      "Epoch 139: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 244ms/step - loss: 0.0463 - mae: 0.1696 - mse: 0.0463 - val_loss: 0.0605 - val_mae: 0.1918 - val_mse: 0.0605\n",
      "Epoch 140/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0481 - mae: 0.1726 - mse: 0.0481\n",
      "Epoch 140: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 226ms/step - loss: 0.0481 - mae: 0.1726 - mse: 0.0481 - val_loss: 0.0616 - val_mae: 0.1936 - val_mse: 0.0616\n",
      "Epoch 141/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0474 - mae: 0.1712 - mse: 0.0474\n",
      "Epoch 141: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.0474 - mae: 0.1712 - mse: 0.0474 - val_loss: 0.0627 - val_mae: 0.1955 - val_mse: 0.0627\n",
      "Epoch 142/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0521 - mae: 0.1799 - mse: 0.0521\n",
      "Epoch 142: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0521 - mae: 0.1799 - mse: 0.0521 - val_loss: 0.0638 - val_mae: 0.1975 - val_mse: 0.0638\n",
      "Epoch 143/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0472 - mae: 0.1733 - mse: 0.0472\n",
      "Epoch 143: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 241ms/step - loss: 0.0472 - mae: 0.1733 - mse: 0.0472 - val_loss: 0.0643 - val_mae: 0.1986 - val_mse: 0.0643\n",
      "Epoch 144/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0479 - mae: 0.1718 - mse: 0.0479\n",
      "Epoch 144: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 224ms/step - loss: 0.0479 - mae: 0.1718 - mse: 0.0479 - val_loss: 0.0639 - val_mae: 0.1977 - val_mse: 0.0639\n",
      "Epoch 145/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0467 - mae: 0.1702 - mse: 0.0467\n",
      "Epoch 145: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 279ms/step - loss: 0.0467 - mae: 0.1702 - mse: 0.0467 - val_loss: 0.0622 - val_mae: 0.1948 - val_mse: 0.0622\n",
      "Epoch 146/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0486 - mae: 0.1741 - mse: 0.0486\n",
      "Epoch 146: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 225ms/step - loss: 0.0486 - mae: 0.1741 - mse: 0.0486 - val_loss: 0.0608 - val_mae: 0.1924 - val_mse: 0.0608\n",
      "Epoch 147/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0482 - mae: 0.1721 - mse: 0.0482\n",
      "Epoch 147: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.0482 - mae: 0.1721 - mse: 0.0482 - val_loss: 0.0598 - val_mae: 0.1907 - val_mse: 0.0598\n",
      "Epoch 148/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0451 - mae: 0.1687 - mse: 0.0451\n",
      "Epoch 148: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0451 - mae: 0.1687 - mse: 0.0451 - val_loss: 0.0599 - val_mae: 0.1908 - val_mse: 0.0599\n",
      "Epoch 149/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0474 - mae: 0.1733 - mse: 0.0474\n",
      "Epoch 149: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0474 - mae: 0.1733 - mse: 0.0474 - val_loss: 0.0599 - val_mae: 0.1909 - val_mse: 0.0599\n",
      "Epoch 150/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0485 - mae: 0.1734 - mse: 0.0485\n",
      "Epoch 150: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 243ms/step - loss: 0.0485 - mae: 0.1734 - mse: 0.0485 - val_loss: 0.0592 - val_mae: 0.1898 - val_mse: 0.0592\n",
      "Epoch 151/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0470 - mae: 0.1718 - mse: 0.0470\n",
      "Epoch 151: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 224ms/step - loss: 0.0470 - mae: 0.1718 - mse: 0.0470 - val_loss: 0.0585 - val_mae: 0.1887 - val_mse: 0.0585\n",
      "Epoch 152/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0488 - mae: 0.1748 - mse: 0.0488\n",
      "Epoch 152: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0488 - mae: 0.1748 - mse: 0.0488 - val_loss: 0.0584 - val_mae: 0.1885 - val_mse: 0.0584\n",
      "Epoch 153/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0500 - mae: 0.1760 - mse: 0.0500\n",
      "Epoch 153: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 241ms/step - loss: 0.0500 - mae: 0.1760 - mse: 0.0500 - val_loss: 0.0590 - val_mae: 0.1895 - val_mse: 0.0590\n",
      "Epoch 154/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0479 - mae: 0.1738 - mse: 0.0479\n",
      "Epoch 154: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0479 - mae: 0.1738 - mse: 0.0479 - val_loss: 0.0602 - val_mae: 0.1917 - val_mse: 0.0602\n",
      "Epoch 155/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0469 - mae: 0.1711 - mse: 0.0469\n",
      "Epoch 155: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0469 - mae: 0.1711 - mse: 0.0469 - val_loss: 0.0608 - val_mae: 0.1927 - val_mse: 0.0608\n",
      "Epoch 156/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0485 - mae: 0.1759 - mse: 0.0485\n",
      "Epoch 156: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0485 - mae: 0.1759 - mse: 0.0485 - val_loss: 0.0611 - val_mae: 0.1934 - val_mse: 0.0611\n",
      "Epoch 157/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0489 - mae: 0.1770 - mse: 0.0489\n",
      "Epoch 157: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0489 - mae: 0.1770 - mse: 0.0489 - val_loss: 0.0612 - val_mae: 0.1936 - val_mse: 0.0612\n",
      "Epoch 158/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0481 - mae: 0.1738 - mse: 0.0481\n",
      "Epoch 158: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 245ms/step - loss: 0.0481 - mae: 0.1738 - mse: 0.0481 - val_loss: 0.0610 - val_mae: 0.1934 - val_mse: 0.0610\n",
      "Epoch 159/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0467 - mae: 0.1713 - mse: 0.0467\n",
      "Epoch 159: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0467 - mae: 0.1713 - mse: 0.0467 - val_loss: 0.0608 - val_mae: 0.1929 - val_mse: 0.0608\n",
      "Epoch 160/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0481 - mae: 0.1721 - mse: 0.0481\n",
      "Epoch 160: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 239ms/step - loss: 0.0481 - mae: 0.1721 - mse: 0.0481 - val_loss: 0.0611 - val_mae: 0.1935 - val_mse: 0.0611\n",
      "Epoch 161/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0464 - mae: 0.1690 - mse: 0.0464\n",
      "Epoch 161: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 270ms/step - loss: 0.0464 - mae: 0.1690 - mse: 0.0464 - val_loss: 0.0617 - val_mae: 0.1946 - val_mse: 0.0617\n",
      "Epoch 162/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0475 - mae: 0.1713 - mse: 0.0475\n",
      "Epoch 162: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0475 - mae: 0.1713 - mse: 0.0475 - val_loss: 0.0622 - val_mae: 0.1953 - val_mse: 0.0622\n",
      "Epoch 163/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0447 - mae: 0.1684 - mse: 0.0447\n",
      "Epoch 163: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0447 - mae: 0.1684 - mse: 0.0447 - val_loss: 0.0614 - val_mae: 0.1938 - val_mse: 0.0614\n",
      "Epoch 164/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0478 - mae: 0.1745 - mse: 0.0478\n",
      "Epoch 164: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0478 - mae: 0.1745 - mse: 0.0478 - val_loss: 0.0613 - val_mae: 0.1935 - val_mse: 0.0613\n",
      "Epoch 165/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0479 - mae: 0.1745 - mse: 0.0479\n",
      "Epoch 165: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0479 - mae: 0.1745 - mse: 0.0479 - val_loss: 0.0609 - val_mae: 0.1928 - val_mse: 0.0609\n",
      "Epoch 166/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0485 - mae: 0.1758 - mse: 0.0485\n",
      "Epoch 166: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0485 - mae: 0.1758 - mse: 0.0485 - val_loss: 0.0618 - val_mae: 0.1943 - val_mse: 0.0618\n",
      "Epoch 167/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0457 - mae: 0.1687 - mse: 0.0457\n",
      "Epoch 167: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 240ms/step - loss: 0.0457 - mae: 0.1687 - mse: 0.0457 - val_loss: 0.0619 - val_mae: 0.1946 - val_mse: 0.0619\n",
      "Epoch 168/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0480 - mae: 0.1746 - mse: 0.0480\n",
      "Epoch 168: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 246ms/step - loss: 0.0480 - mae: 0.1746 - mse: 0.0480 - val_loss: 0.0613 - val_mae: 0.1936 - val_mse: 0.0613\n",
      "Epoch 169/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0475 - mae: 0.1723 - mse: 0.0475\n",
      "Epoch 169: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 243ms/step - loss: 0.0475 - mae: 0.1723 - mse: 0.0475 - val_loss: 0.0616 - val_mae: 0.1942 - val_mse: 0.0616\n",
      "Epoch 170/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0459 - mae: 0.1694 - mse: 0.0459\n",
      "Epoch 170: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 239ms/step - loss: 0.0459 - mae: 0.1694 - mse: 0.0459 - val_loss: 0.0611 - val_mae: 0.1934 - val_mse: 0.0611\n",
      "Epoch 171/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0468 - mae: 0.1717 - mse: 0.0468\n",
      "Epoch 171: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 246ms/step - loss: 0.0468 - mae: 0.1717 - mse: 0.0468 - val_loss: 0.0594 - val_mae: 0.1902 - val_mse: 0.0594\n",
      "Epoch 172/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0460 - mae: 0.1690 - mse: 0.0460\n",
      "Epoch 172: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 226ms/step - loss: 0.0460 - mae: 0.1690 - mse: 0.0460 - val_loss: 0.0585 - val_mae: 0.1886 - val_mse: 0.0585\n",
      "Epoch 173/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0490 - mae: 0.1747 - mse: 0.0490\n",
      "Epoch 173: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 241ms/step - loss: 0.0490 - mae: 0.1747 - mse: 0.0490 - val_loss: 0.0576 - val_mae: 0.1868 - val_mse: 0.0576\n",
      "Epoch 174/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0502 - mae: 0.1771 - mse: 0.0502\n",
      "Epoch 174: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0502 - mae: 0.1771 - mse: 0.0502 - val_loss: 0.0585 - val_mae: 0.1881 - val_mse: 0.0585\n",
      "Epoch 175/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0489 - mae: 0.1743 - mse: 0.0489\n",
      "Epoch 175: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0489 - mae: 0.1743 - mse: 0.0489 - val_loss: 0.0608 - val_mae: 0.1919 - val_mse: 0.0608\n",
      "Epoch 176/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0450 - mae: 0.1682 - mse: 0.0450\n",
      "Epoch 176: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0450 - mae: 0.1682 - mse: 0.0450 - val_loss: 0.0613 - val_mae: 0.1928 - val_mse: 0.0613\n",
      "Epoch 177/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0474 - mae: 0.1725 - mse: 0.0474\n",
      "Epoch 177: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 241ms/step - loss: 0.0474 - mae: 0.1725 - mse: 0.0474 - val_loss: 0.0621 - val_mae: 0.1944 - val_mse: 0.0621\n",
      "Epoch 178/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0476 - mae: 0.1727 - mse: 0.0476\n",
      "Epoch 178: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 215ms/step - loss: 0.0476 - mae: 0.1727 - mse: 0.0476 - val_loss: 0.0616 - val_mae: 0.1937 - val_mse: 0.0616\n",
      "Epoch 179/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0475 - mae: 0.1717 - mse: 0.0475\n",
      "Epoch 179: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 251ms/step - loss: 0.0475 - mae: 0.1717 - mse: 0.0475 - val_loss: 0.0613 - val_mae: 0.1934 - val_mse: 0.0613\n",
      "Epoch 180/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0488 - mae: 0.1742 - mse: 0.0488\n",
      "Epoch 180: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 249ms/step - loss: 0.0488 - mae: 0.1742 - mse: 0.0488 - val_loss: 0.0613 - val_mae: 0.1935 - val_mse: 0.0613\n",
      "Epoch 181/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0483 - mae: 0.1730 - mse: 0.0483\n",
      "Epoch 181: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 227ms/step - loss: 0.0483 - mae: 0.1730 - mse: 0.0483 - val_loss: 0.0613 - val_mae: 0.1937 - val_mse: 0.0613\n",
      "Epoch 182/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0472 - mae: 0.1711 - mse: 0.0472\n",
      "Epoch 182: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0472 - mae: 0.1711 - mse: 0.0472 - val_loss: 0.0617 - val_mae: 0.1943 - val_mse: 0.0617\n",
      "Epoch 183/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0480 - mae: 0.1760 - mse: 0.0480\n",
      "Epoch 183: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0480 - mae: 0.1760 - mse: 0.0480 - val_loss: 0.0616 - val_mae: 0.1940 - val_mse: 0.0616\n",
      "Epoch 184/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0494 - mae: 0.1756 - mse: 0.0494\n",
      "Epoch 184: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 242ms/step - loss: 0.0494 - mae: 0.1756 - mse: 0.0494 - val_loss: 0.0601 - val_mae: 0.1911 - val_mse: 0.0601\n",
      "Epoch 185/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0463 - mae: 0.1715 - mse: 0.0463\n",
      "Epoch 185: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 267ms/step - loss: 0.0463 - mae: 0.1715 - mse: 0.0463 - val_loss: 0.0598 - val_mae: 0.1904 - val_mse: 0.0598\n",
      "Epoch 186/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0466 - mae: 0.1713 - mse: 0.0466\n",
      "Epoch 186: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 295ms/step - loss: 0.0466 - mae: 0.1713 - mse: 0.0466 - val_loss: 0.0598 - val_mae: 0.1905 - val_mse: 0.0598\n",
      "Epoch 187/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0482 - mae: 0.1737 - mse: 0.0482\n",
      "Epoch 187: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 248ms/step - loss: 0.0482 - mae: 0.1737 - mse: 0.0482 - val_loss: 0.0606 - val_mae: 0.1921 - val_mse: 0.0606\n",
      "Epoch 188/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0478 - mae: 0.1737 - mse: 0.0478\n",
      "Epoch 188: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 278ms/step - loss: 0.0478 - mae: 0.1737 - mse: 0.0478 - val_loss: 0.0614 - val_mae: 0.1937 - val_mse: 0.0614\n",
      "Epoch 189/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0476 - mae: 0.1732 - mse: 0.0476\n",
      "Epoch 189: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 322ms/step - loss: 0.0476 - mae: 0.1732 - mse: 0.0476 - val_loss: 0.0623 - val_mae: 0.1957 - val_mse: 0.0623\n",
      "Epoch 190/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0502 - mae: 0.1776 - mse: 0.0502\n",
      "Epoch 190: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 289ms/step - loss: 0.0502 - mae: 0.1776 - mse: 0.0502 - val_loss: 0.0615 - val_mae: 0.1942 - val_mse: 0.0615\n",
      "Epoch 191/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0459 - mae: 0.1689 - mse: 0.0459\n",
      "Epoch 191: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 254ms/step - loss: 0.0459 - mae: 0.1689 - mse: 0.0459 - val_loss: 0.0605 - val_mae: 0.1924 - val_mse: 0.0605\n",
      "Epoch 192/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0469 - mae: 0.1740 - mse: 0.0469\n",
      "Epoch 192: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 242ms/step - loss: 0.0469 - mae: 0.1740 - mse: 0.0469 - val_loss: 0.0604 - val_mae: 0.1920 - val_mse: 0.0604\n",
      "Epoch 193/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0469 - mae: 0.1721 - mse: 0.0469\n",
      "Epoch 193: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 256ms/step - loss: 0.0469 - mae: 0.1721 - mse: 0.0469 - val_loss: 0.0614 - val_mae: 0.1936 - val_mse: 0.0614\n",
      "Epoch 194/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0474 - mae: 0.1706 - mse: 0.0474\n",
      "Epoch 194: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0474 - mae: 0.1706 - mse: 0.0474 - val_loss: 0.0635 - val_mae: 0.1973 - val_mse: 0.0635\n",
      "Epoch 195/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0478 - mae: 0.1744 - mse: 0.0478\n",
      "Epoch 195: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 252ms/step - loss: 0.0478 - mae: 0.1744 - mse: 0.0478 - val_loss: 0.0652 - val_mae: 0.2005 - val_mse: 0.0652\n",
      "Epoch 196/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0471 - mae: 0.1750 - mse: 0.0471\n",
      "Epoch 196: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0471 - mae: 0.1750 - mse: 0.0471 - val_loss: 0.0655 - val_mae: 0.2013 - val_mse: 0.0655\n",
      "Epoch 197/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0484 - mae: 0.1740 - mse: 0.0484\n",
      "Epoch 197: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0484 - mae: 0.1740 - mse: 0.0484 - val_loss: 0.0646 - val_mae: 0.1999 - val_mse: 0.0646\n",
      "Epoch 198/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0473 - mae: 0.1741 - mse: 0.0473\n",
      "Epoch 198: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 243ms/step - loss: 0.0473 - mae: 0.1741 - mse: 0.0473 - val_loss: 0.0630 - val_mae: 0.1971 - val_mse: 0.0630\n",
      "Epoch 199/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0476 - mae: 0.1745 - mse: 0.0476\n",
      "Epoch 199: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 240ms/step - loss: 0.0476 - mae: 0.1745 - mse: 0.0476 - val_loss: 0.0623 - val_mae: 0.1959 - val_mse: 0.0623\n",
      "Epoch 200/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0461 - mae: 0.1710 - mse: 0.0461\n",
      "Epoch 200: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0461 - mae: 0.1710 - mse: 0.0461 - val_loss: 0.0625 - val_mae: 0.1964 - val_mse: 0.0625\n",
      "Epoch 201/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0470 - mae: 0.1708 - mse: 0.0470\n",
      "Epoch 201: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0470 - mae: 0.1708 - mse: 0.0470 - val_loss: 0.0624 - val_mae: 0.1959 - val_mse: 0.0624\n",
      "Epoch 202/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0457 - mae: 0.1695 - mse: 0.0457\n",
      "Epoch 202: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 293ms/step - loss: 0.0457 - mae: 0.1695 - mse: 0.0457 - val_loss: 0.0616 - val_mae: 0.1942 - val_mse: 0.0616\n",
      "Epoch 203/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0485 - mae: 0.1750 - mse: 0.0485\n",
      "Epoch 203: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 243ms/step - loss: 0.0485 - mae: 0.1750 - mse: 0.0485 - val_loss: 0.0606 - val_mae: 0.1924 - val_mse: 0.0606\n",
      "Epoch 204/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0470 - mae: 0.1722 - mse: 0.0470\n",
      "Epoch 204: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 227ms/step - loss: 0.0470 - mae: 0.1722 - mse: 0.0470 - val_loss: 0.0601 - val_mae: 0.1913 - val_mse: 0.0601\n",
      "Epoch 205/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0462 - mae: 0.1706 - mse: 0.0462\n",
      "Epoch 205: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 247ms/step - loss: 0.0462 - mae: 0.1706 - mse: 0.0462 - val_loss: 0.0596 - val_mae: 0.1902 - val_mse: 0.0596\n",
      "Epoch 206/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0480 - mae: 0.1738 - mse: 0.0480\n",
      "Epoch 206: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 239ms/step - loss: 0.0480 - mae: 0.1738 - mse: 0.0480 - val_loss: 0.0589 - val_mae: 0.1890 - val_mse: 0.0589\n",
      "Epoch 207/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0461 - mae: 0.1698 - mse: 0.0461\n",
      "Epoch 207: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 245ms/step - loss: 0.0461 - mae: 0.1698 - mse: 0.0461 - val_loss: 0.0583 - val_mae: 0.1882 - val_mse: 0.0583\n",
      "Epoch 208/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0495 - mae: 0.1772 - mse: 0.0495\n",
      "Epoch 208: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.0495 - mae: 0.1772 - mse: 0.0495 - val_loss: 0.0586 - val_mae: 0.1889 - val_mse: 0.0586\n",
      "Epoch 209/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0482 - mae: 0.1754 - mse: 0.0482\n",
      "Epoch 209: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 241ms/step - loss: 0.0482 - mae: 0.1754 - mse: 0.0482 - val_loss: 0.0587 - val_mae: 0.1892 - val_mse: 0.0587\n",
      "Epoch 210/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0465 - mae: 0.1701 - mse: 0.0465\n",
      "Epoch 210: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 260ms/step - loss: 0.0465 - mae: 0.1701 - mse: 0.0465 - val_loss: 0.0581 - val_mae: 0.1882 - val_mse: 0.0581\n",
      "Epoch 211/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0458 - mae: 0.1682 - mse: 0.0458\n",
      "Epoch 211: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.0458 - mae: 0.1682 - mse: 0.0458 - val_loss: 0.0583 - val_mae: 0.1884 - val_mse: 0.0583\n",
      "Epoch 212/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0473 - mae: 0.1740 - mse: 0.0473\n",
      "Epoch 212: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0473 - mae: 0.1740 - mse: 0.0473 - val_loss: 0.0583 - val_mae: 0.1883 - val_mse: 0.0583\n",
      "Epoch 213/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0460 - mae: 0.1715 - mse: 0.0460\n",
      "Epoch 213: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0460 - mae: 0.1715 - mse: 0.0460 - val_loss: 0.0592 - val_mae: 0.1899 - val_mse: 0.0592\n",
      "Epoch 214/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0469 - mae: 0.1709 - mse: 0.0469\n",
      "Epoch 214: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 284ms/step - loss: 0.0469 - mae: 0.1709 - mse: 0.0469 - val_loss: 0.0601 - val_mae: 0.1915 - val_mse: 0.0601\n",
      "Epoch 215/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0449 - mae: 0.1684 - mse: 0.0449\n",
      "Epoch 215: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0449 - mae: 0.1684 - mse: 0.0449 - val_loss: 0.0614 - val_mae: 0.1937 - val_mse: 0.0614\n",
      "Epoch 216/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0463 - mae: 0.1735 - mse: 0.0463\n",
      "Epoch 216: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 227ms/step - loss: 0.0463 - mae: 0.1735 - mse: 0.0463 - val_loss: 0.0634 - val_mae: 0.1973 - val_mse: 0.0634\n",
      "Epoch 217/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0471 - mae: 0.1723 - mse: 0.0471\n",
      "Epoch 217: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0471 - mae: 0.1723 - mse: 0.0471 - val_loss: 0.0640 - val_mae: 0.1985 - val_mse: 0.0640\n",
      "Epoch 218/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0483 - mae: 0.1746 - mse: 0.0483\n",
      "Epoch 218: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 261ms/step - loss: 0.0483 - mae: 0.1746 - mse: 0.0483 - val_loss: 0.0627 - val_mae: 0.1964 - val_mse: 0.0627\n",
      "Epoch 219/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0462 - mae: 0.1711 - mse: 0.0462\n",
      "Epoch 219: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 285ms/step - loss: 0.0462 - mae: 0.1711 - mse: 0.0462 - val_loss: 0.0613 - val_mae: 0.1939 - val_mse: 0.0613\n",
      "Epoch 220/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0475 - mae: 0.1733 - mse: 0.0475\n",
      "Epoch 220: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0475 - mae: 0.1733 - mse: 0.0475 - val_loss: 0.0596 - val_mae: 0.1907 - val_mse: 0.0596\n",
      "Epoch 221/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0480 - mae: 0.1750 - mse: 0.0480\n",
      "Epoch 221: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0480 - mae: 0.1750 - mse: 0.0480 - val_loss: 0.0583 - val_mae: 0.1882 - val_mse: 0.0583\n",
      "Epoch 222/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0462 - mae: 0.1688 - mse: 0.0462\n",
      "Epoch 222: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 241ms/step - loss: 0.0462 - mae: 0.1688 - mse: 0.0462 - val_loss: 0.0580 - val_mae: 0.1876 - val_mse: 0.0580\n",
      "Epoch 223/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0451 - mae: 0.1660 - mse: 0.0451\n",
      "Epoch 223: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 256ms/step - loss: 0.0451 - mae: 0.1660 - mse: 0.0451 - val_loss: 0.0588 - val_mae: 0.1889 - val_mse: 0.0588\n",
      "Epoch 224/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0449 - mae: 0.1649 - mse: 0.0449\n",
      "Epoch 224: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 246ms/step - loss: 0.0449 - mae: 0.1649 - mse: 0.0449 - val_loss: 0.0599 - val_mae: 0.1910 - val_mse: 0.0599\n",
      "Epoch 225/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0461 - mae: 0.1698 - mse: 0.0461\n",
      "Epoch 225: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0461 - mae: 0.1698 - mse: 0.0461 - val_loss: 0.0607 - val_mae: 0.1927 - val_mse: 0.0607\n",
      "Epoch 226/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0471 - mae: 0.1735 - mse: 0.0471\n",
      "Epoch 226: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 311ms/step - loss: 0.0471 - mae: 0.1735 - mse: 0.0471 - val_loss: 0.0620 - val_mae: 0.1951 - val_mse: 0.0620\n",
      "Epoch 227/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0458 - mae: 0.1692 - mse: 0.0458\n",
      "Epoch 227: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.0458 - mae: 0.1692 - mse: 0.0458 - val_loss: 0.0633 - val_mae: 0.1975 - val_mse: 0.0633\n",
      "Epoch 228/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0461 - mae: 0.1722 - mse: 0.0461\n",
      "Epoch 228: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.0461 - mae: 0.1722 - mse: 0.0461 - val_loss: 0.0642 - val_mae: 0.1990 - val_mse: 0.0642\n",
      "Epoch 229/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0453 - mae: 0.1678 - mse: 0.0453\n",
      "Epoch 229: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 246ms/step - loss: 0.0453 - mae: 0.1678 - mse: 0.0453 - val_loss: 0.0657 - val_mae: 0.2020 - val_mse: 0.0657\n",
      "Epoch 230/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0483 - mae: 0.1745 - mse: 0.0483\n",
      "Epoch 230: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0483 - mae: 0.1745 - mse: 0.0483 - val_loss: 0.0664 - val_mae: 0.2033 - val_mse: 0.0664\n",
      "Epoch 231/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0482 - mae: 0.1738 - mse: 0.0482\n",
      "Epoch 231: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 244ms/step - loss: 0.0482 - mae: 0.1738 - mse: 0.0482 - val_loss: 0.0659 - val_mae: 0.2024 - val_mse: 0.0659\n",
      "Epoch 232/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0489 - mae: 0.1770 - mse: 0.0489\n",
      "Epoch 232: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.0489 - mae: 0.1770 - mse: 0.0489 - val_loss: 0.0653 - val_mae: 0.2013 - val_mse: 0.0653\n",
      "Epoch 233/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0466 - mae: 0.1709 - mse: 0.0466\n",
      "Epoch 233: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0466 - mae: 0.1709 - mse: 0.0466 - val_loss: 0.0653 - val_mae: 0.2015 - val_mse: 0.0653\n",
      "Epoch 234/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0472 - mae: 0.1713 - mse: 0.0472\n",
      "Epoch 234: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0472 - mae: 0.1713 - mse: 0.0472 - val_loss: 0.0652 - val_mae: 0.2015 - val_mse: 0.0652\n",
      "Epoch 235/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0450 - mae: 0.1695 - mse: 0.0450\n",
      "Epoch 235: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0450 - mae: 0.1695 - mse: 0.0450 - val_loss: 0.0654 - val_mae: 0.2019 - val_mse: 0.0654\n",
      "Epoch 236/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0471 - mae: 0.1735 - mse: 0.0471\n",
      "Epoch 236: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 224ms/step - loss: 0.0471 - mae: 0.1735 - mse: 0.0471 - val_loss: 0.0648 - val_mae: 0.2011 - val_mse: 0.0648\n",
      "Epoch 237/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0462 - mae: 0.1714 - mse: 0.0462\n",
      "Epoch 237: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 256ms/step - loss: 0.0462 - mae: 0.1714 - mse: 0.0462 - val_loss: 0.0636 - val_mae: 0.1987 - val_mse: 0.0636\n",
      "Epoch 238/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0446 - mae: 0.1670 - mse: 0.0446\n",
      "Epoch 238: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 226ms/step - loss: 0.0446 - mae: 0.1670 - mse: 0.0446 - val_loss: 0.0626 - val_mae: 0.1967 - val_mse: 0.0626\n",
      "Epoch 239/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0474 - mae: 0.1734 - mse: 0.0474\n",
      "Epoch 239: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.0474 - mae: 0.1734 - mse: 0.0474 - val_loss: 0.0625 - val_mae: 0.1964 - val_mse: 0.0625\n",
      "Epoch 240/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0461 - mae: 0.1685 - mse: 0.0461\n",
      "Epoch 240: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 246ms/step - loss: 0.0461 - mae: 0.1685 - mse: 0.0461 - val_loss: 0.0632 - val_mae: 0.1977 - val_mse: 0.0632\n",
      "Epoch 241/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0447 - mae: 0.1677 - mse: 0.0447\n",
      "Epoch 241: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 244ms/step - loss: 0.0447 - mae: 0.1677 - mse: 0.0447 - val_loss: 0.0650 - val_mae: 0.2011 - val_mse: 0.0650\n",
      "Epoch 242/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0472 - mae: 0.1718 - mse: 0.0472\n",
      "Epoch 242: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0472 - mae: 0.1718 - mse: 0.0472 - val_loss: 0.0674 - val_mae: 0.2056 - val_mse: 0.0674\n",
      "Epoch 243/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0447 - mae: 0.1696 - mse: 0.0447\n",
      "Epoch 243: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 248ms/step - loss: 0.0447 - mae: 0.1696 - mse: 0.0447 - val_loss: 0.0678 - val_mae: 0.2064 - val_mse: 0.0678\n",
      "Epoch 244/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0448 - mae: 0.1683 - mse: 0.0448\n",
      "Epoch 244: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0448 - mae: 0.1683 - mse: 0.0448 - val_loss: 0.0656 - val_mae: 0.2025 - val_mse: 0.0656\n",
      "Epoch 245/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0472 - mae: 0.1716 - mse: 0.0472\n",
      "Epoch 245: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0472 - mae: 0.1716 - mse: 0.0472 - val_loss: 0.0626 - val_mae: 0.1969 - val_mse: 0.0626\n",
      "Epoch 246/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0458 - mae: 0.1681 - mse: 0.0458\n",
      "Epoch 246: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.0458 - mae: 0.1681 - mse: 0.0458 - val_loss: 0.0610 - val_mae: 0.1938 - val_mse: 0.0610\n",
      "Epoch 247/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0463 - mae: 0.1712 - mse: 0.0463\n",
      "Epoch 247: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0463 - mae: 0.1712 - mse: 0.0463 - val_loss: 0.0597 - val_mae: 0.1913 - val_mse: 0.0597\n",
      "Epoch 248/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0471 - mae: 0.1723 - mse: 0.0471\n",
      "Epoch 248: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 227ms/step - loss: 0.0471 - mae: 0.1723 - mse: 0.0471 - val_loss: 0.0608 - val_mae: 0.1932 - val_mse: 0.0608\n",
      "Epoch 249/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0469 - mae: 0.1711 - mse: 0.0469\n",
      "Epoch 249: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 242ms/step - loss: 0.0469 - mae: 0.1711 - mse: 0.0469 - val_loss: 0.0629 - val_mae: 0.1970 - val_mse: 0.0629\n",
      "Epoch 250/250\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0457 - mae: 0.1683 - mse: 0.0457\n",
      "Epoch 250: val_loss did not improve from 0.05286\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 0.0457 - mae: 0.1683 - mse: 0.0457 - val_loss: 0.0657 - val_mae: 0.2021 - val_mse: 0.0657\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs     = 250\n",
    "history = model.fit(x_trainset, y_trainset,\n",
    "                   epochs = epochs,\n",
    "                   callbacks = [mc],\n",
    "                   validation_data = (x_valset, y_valset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "1788d100-9f66-4b14-835d-5136438f405f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f51ecd969d0>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'], label = 'Tr. Loss')\n",
    "plt.plot(history.history['val_loss'], label = 'Val. Loss')\n",
    "plt.ylim(0,1)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "da817465-472d-4425-adc7-2b6f7b535eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 29ms/step\n"
     ]
    }
   ],
   "source": [
    "y_predicted = model.predict(x_testset) * (max_y - min_y) + min_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "01f58734-d799-402d-a00b-e16406935ca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAD4CAYAAAANbUbJAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAxsElEQVR4nO3dd3xP1x/H8deRUHvGXjFi7wblZ49QqrQ21dgzVoui1KzaghpNBY2ttWLvUbO1gqgRBIkRM8hOvuf3R9I0SNqQcb9JPs/HIw/Jvef7/X7cfPPOybnnnqu01gghhEjeUhldgBBCiIQnYS+EECmAhL0QQqQAEvZCCJECSNgLIUQKYGl0AdGxsrLS1tbWRpchhBBJypkzZx5rrXNGt88sw97a2prTp08bXYYQQiQpSqnbMe0zq2EcpVQLpZSTr6+v0aUIIUSyYlZhr7XeqrXunSVLFqNLEUKIZMWswl4IIUTCkLAXQogUQMJeCCFSAAl7IYRIAcwq7GU2jhBCJAyzCnuZjSOESKlCQ0P59ddfWbhwYYI8v1mFvRBCpDTPnz9n5syZFC1WjHbt2vHDnB8xmUzx/jpmeQWtEEIkdx4eHsydO5ely5bh7+dHBusK5Pz8Sz5q1ARQ8f56EvZCCJFItNYcOnSIOXPmsG3bNlQqC9KXrks+2xa0aFCLHrWLYFs4G0pJ2AshRJITFBTEmjVrcHR0xM3NjTQZs5K5RntyVWtB5wYV6VazCIVypE/QGiTshRAigfj4+LBo0SIWLFzIIx8f0ue2JnvTgdjU/Jie9UrSrmpBMqdNnSi1mFXYK6VaAC2KFy9udClCCPHeLl68iKOjI6tWrSIoKIiMNlXJVW8gNevUo2ftYjQpmxtLi8SdH2NWYa+13gpstbW17WV0LUII8S601uzcuZPZs2ezf/9+LNN8QLqyDclZ9VM+rVuVHrWKUKVQNsPqM6uwF0KIpObv+fFTp07lwoULpM2ak6x17clT/RO61C2LfU1r8mdNZ3SZEvZCCPE+AgICWL58OTNmzODWrVtkz1+UHM2GUvJ/TehZtwRtbQuS8QPziVjzqUQIIZIAX19fFi1ahKOjIw8fPqRUhSrYdO5KaMEqDKhvg0OD4nxgaWF0mW+RsBdCiFh48OABjo6OLFq0iBcvXtCwUWNq927HH0F5KJ03MzPaVKR8AfNd6kXCXggh/sWNGzeYOXMmy5YtIyQkhLZt21K3bU+WXgFPv2AGNyqOQ/3ipLE079VnJOyFECIa58+fZ9q0aaxfvx5LS0u6detGr/6DWHE5iGl/3qN03sws61qVcvnNtzcflYS9EEJE0Fpz5MgRpk6dyq5du8iUKRPDhw9n8ODBXHiq6LPpEs/9gxnSyIb+9cy/Nx+VhL0QIsXTWrNjxw6+//57Tpw4Qa5cuZgyZQr9+vXDlDo9413dcXW7R5m8mfmle1XK5ksavfmozCrs5QpaIURiMplMbNy4kSlTpnDu3DkKFy7MggUL6NatG+nSpWPXpfuM2fwnvgEhfNW4BP3qFSN1Il/5Gl/Mqmq5eYkQIjGEhITg4uJC2bJladu2LX5+fixfvpzr16/Tv39//MNS4bD6LH1XniVPlrS4OtRiUEObJBv0YGY9eyGESEhBQUEsX76cadOmcevWLSpUqMC6deto3bo1Fhbhc+N3XLzP2M2XeBEYwjC7EvSpm3R781FJ2Ashkj0/Pz+cnJyYOXMm9+7do3r16sybN4/mzZtHrh1/5vYzfjxwnYNXH1E+fxZWta1OqTyZDa48/kjYCyGSLV9fXxYsWMCcOXN4/Pgx9evXx8XFhQYNGqCUCp99c+0RCw95cPLmU7KmT83Ij0vRs1aRRF+VMqFJ2Ashkp3Hjx8zd+5c5s+fj6+vL82aNePbb7+lZs2aAJhMml2X7rPw0A0uevuSJ3NaxjQvTcdqhchgRuvZxKfk+b8SQqRI9+/fZ+bMmSxevJiAgABat27N6NGjqVy5MgAhYSY2n/Nm0eEb3Hzkh3WO9ExrXZ5WlfOb5Xo28UnCXgiR5N27d49p06bx008/ERoaSqdOnRg1ahSlS5cGICA4jHV/3sHpyE3u+QZSOm9m5nesTLPyebFIFf/3ezVHEvZCiCTL29ubadOm4eTkRFhYGPb29owePZqiRYsC4BsQwooTniw75skTv2CqWmfj+8/LU69EzgS5qbc5k7AXQiQ53t7eTJ06lZ9//pmwsDC6du3K6NGjKVKkCACPXgbhfPQWK0/e5lVQKPVL5qR//eJUtc5ucOXGkbAXQiQZXl5ekSFvMpno1q0bo0ePxtraOnz/M39+OnyT9afvEhJmoln5vPSrVyxJLm8Q3yTshRBm7+7du0ydOpUlS5ZgMpno3r07o0aNigz5hy8C+fGAB2v/vANA6yoF6FO3GEWsMhhYtXmRsBdCmK07d+4wdepUnJ2d0VpHhnzhwoUBeOoXzKJDHricuE2YSdO+akEcGhQnbxbj7/lqbiTshRBm5/bt2/zwww8sXboUgB49ejBq1CgKFSoEhJ94df79Js5HbxEQEsZnlQswuKENhXKkN7Jss5YgYa+UKgp8C2TRWreJ2NYKaA5kBpy11nsS4rWFEEnXnTt3mDJlSmTI9+zZk5EjR0aGvH9wKMuOeeJ05Ca+ASE0r5CXoY1sKJ4rk5FlJwmxDnul1FLgE8BHa10uyvamwFzAAliitZ6qtb4J9FBK/fZ3O631ZmCzUiobMBOQsBdCAPDw4UO+//57Fi9ejFKKXr16MXLkSAoWLAhAYEgYq07dYdEhDx6/CqZhqVx8ZVdCTry+g3fp2S8HfgRc/t6glLIAFgCNAS/gT6WUq9b68r88z5iIxwghUrhnz54xc+ZMHB0dCQoKokePHowZMyYy5EPCTPx62ov5B65z3zeQ/xXPgZNdSaoUymZw5UlPrMNea31EKWX9xuZqgEdETx6l1FqgJfBW2KvwKximAju11mej2d8b6A1E/skmhEie/Pz8mDdvHtOnT8fX15eOHTsyfvx4bGxsAAgzabac98Zx33XuPPWnSqGszGpXkZrFrAyuPOmK65h9fuBulK+9gOpKqRzA90BlpdQorfUPwECgEZBFKVVca7046hNprZ0AJwBbW1sdx7qEEGYoKCgIJycnJk+ejI+PDy1atGDy5MlUqFABiFigzP0Bs/dew8PnFWXzhd/Uu17JlHfFa3xLkBO0WusnQN83ts0D5iXE6wkhzFtoaCgrVqxg/Pjx3Llzh3r16rF582Zq1KgR2ebqg5eM3HiBc3eeUzxXRhZ1rkKTsnlIlULWrklocQ17b6BglK8LRGx7L3IPWiGSF5PJxIYNGxg7dixXr16latWqODs707Bhw8ieelBoGAsO3mDRIQ8yp03NzLYV+axy/hSzQFliievq/H8CNkqpIkqpNEAHwPV9n0zuQStE8qC1ZufOndja2tKuXTssLCzYtGkTp06dolGjRq/dHar5vKPM23+dFhXysferurT5sIAEfQJ4l6mXa4B6gJVSygsYp7V2Vko5ALsJn3q5VGvtniCVCiGShN9//53Ro0dz9OhRihQpgouLC506dYq8xyuAX1AoM3Zf5ZcTnuTLko7l3apSr2QuA6tO/t5lNk7HGLbvAHbERzEyjCNE0nX58mWGDx/Ojh07yJs3LwsXLqRHjx6kSZPmtXaHrz1i9MaL3PMNwL6GNcOalCRjMr07lDkxqyOstd4KbLW1te1ldC1CiNjx8fFh3Lhx/Pzzz2TMmJFp06bh4OBA+vSvL13wzC+YSdsus/GcN8VzZeS3vjX4sHDKXXI4sZlV2Ashko7AwEAcHR2ZMmUK/v7+9OvXj3HjxmFl9fpceK01Wy/cZ4KrO74BIQxqUJwBDYon+9sAmhuzCnsZxhHC/GmtWbt2LaNGjeL27du0aNGC6dOnU6pUqbfa3vcNYMymS+y/4kPFgllZ1bo8pfJkNqBqEdfZOPFKZuMIYd6OHz9OjRo16NSpE9myZWP//v24urq+FfQmk2blyds0nn2E4zeeMKZ5aTb2qylBbyCz6tkLIczTzZs3+eabb/jtt9/Imzcvy5Yto0uXLq/NsIls++gVIzde5I9bT6lV3Iopn5WXpYfNgIS9ECJGz58/Z/LkycyfPx9LS0vGjx/PsGHDyJDh7TtAvQoKxfn3Wyw45EFay1RMb1OBth8WkGUOzIRZhb2M2QthHkJCQli8eDETJkzg6dOndO3alcmTJ5MvX7632gYEh7Hy5G0WHb7BU79gmpfPy7hPy5ArU1oDKhcxMauwl6mXQhhLa83WrVsZPnw4165do0GDBsyaNYtKlSq91TYoNIx1f97lxwMe+LwMoraNFV/blaRSwayJXrf4b2YV9kII47i7uzNo0CAOHDhAyZIlcXV15ZNPPnlrGCYkzMTGs17M2++B9/MAqllnZ37HylQvmsOgykVsSNgLkcL5+voyYcIE5s2bR+bMmZk/fz59+vQhderUr7ULM2m2ut3Dcd81PJ/4U7FgVqa2Lk+t4lYyLp8ESNgLkUKZTCZWrlzJiBEj8PHxoVevXnz//fdvXRRlMml2R6wxf93nFaXzZmbJl7Y0LJ1LQj4JMauwlxO0QiSOc+fO4eDgwPHjx6levTrbtm3D1tb2tTZaaw5e9WHWnmu433tBsZwZWNCpCh+XkzXmkyKzCns5QStEwnr69Cljxoxh8eLFWFlZsXTpUuzt7UmV6p/rK7XWHL/xhJl7rnLuznMKZU/P7HYVaVlJ1phPyswq7IUQCSMsLAxnZ2dGjx7Ns2fPGDhwIBMmTCBr1qyvtTt75xnTd13h5M2n5M2Slh8+L0+bDwuQ2sKsLrYX70HCXohk7uTJkzg4OHDmzBnq1KnD/PnzI+/5+renfsFM23mFdafvYpXxA8a1KEPHaoVIm1oWK0suJOyFSKYePnzIyJEjWb58Ofny5WP16tV06NDhtZOqJpPm1zN3mbrzCi8DQ+lTpyiDGtqQQdaXT3bM6jsqJ2iFiLvQ0FAWLlzId999h7+/PyNGjGDMmDFkypTptXZ/3X/BmM2XOHP7GVWtszG5VXlK5skUw7OKpE5prY2u4S22trb69OnTRpchRJJz+PBhHBwcuHTpEnZ2dsydO/etFSlfBYXiuPcay457kjmtJaOalaZNlQIywyYZUEqd0VrbRrfPrHr2Qoj38+DBA4YPH87KlSspXLgwmzZtomXLlq8N2Wit2XXpARO2XubBi0A6VivIiCalyJYhzb88s0guJOyFSMJCQ0NZtGgRY8aMITAwkG+//ZbRo0e/dUvAO0/8+c71EoeuPqJ03sws6FyFDwtnM6hqYQQJeyGSqJMnT9KvXz/Onz9P48aN+fHHHylRosRrbYJCw/jp8E0WHPTAMpViTPPSdK1pjaVMpUxxJOyFSGIeP37MyJEjcXZ2Jl++fKxfv542bdq8tXTBMY/HjN18iZuP/WhWPg9jPylD3izpDKpaGE3CXogkwmQy4ezszMiRI/H19eXrr79m3Lhxb82y8XkRyOTtf+Hqdo9C2dOzvFtV6pXMZVDVwlxI2AuRBJw9e5b+/ftz6tQpateuzYIFCyhfvvxrbQJDwlh16g6Oe68RFGpiUEMb+tcrJhdGCcDMwl7m2QvxuufPnzN27FgWLlyIlZUVv/zyC126dHltyCYoNIz1p71YcMCDBy8CqW1jxYRPy1I0Z0YDKxfmRubZC2GGtNasWrWKYcOG4ePjQ//+/Zk8efJra9mEhJnYcMaL+QfCbyLyYeFsfN24BDWK5ZClh1MomWcvRBLi7u7OgAEDOHz4MNWqVWP79u18+OGHkftDw0xsOufNvAPXufs0gIoFszLl8/LUsZGbiIiYSdgLYSb8/PyYMGECc+bMIVOmTPz000/07Nkzcvnhv+8UNXf/dW499qNc/sxM6FqW+iXlJiLiv0nYC2EGdu/eTd++ffH09KRbt25MmzaNnDlzAuGLle24dB/Hfdfx8HlFqTyZ+KnLh9iVyS0hL2JNwl4IA/n4+DB06FBWr15NyZIlOXToEHXr1gXCx+13uz/Ecd81rjx4iU2ujHKnKPHeJOyFMIDWmuXLlzNs2DBevnzJd999x6hRo0ibNi1aaw5c8WH23vDbARa1ysDcDpX4pEI+uVOUeG8S9kIksmvXrtG3b18OHjxIrVq1+OmnnyhTpgwQftXr9F1XcPPypVD29MxqW5GWlfLJ8gYiziTshUgkwcHBzJgxg0mTJpE2bdrXTsD6BoTw/fbLrD/tRf6s6ZjeugKfVckvtwMU8UbCXohEcOLECXr16oW7uztt27Zl7ty55M2bF4CDV3wYtfEiPi8D6VevGIMb2shVryLemVXYyxW0Irnx9fVl9OjRLFq0iAIFCrB161Y++eST8H3+IUzcdpkNZ70okTsjP3X5HxULZjW2YJFsmVXYa623AlttbW17GV2LEHG1adMmHBwcePDgAYMHD2bSpElkzBi+hMG+yw8ZvekiT/yCGdigOA4NivOBpfTmRcIxq7AXIjnw8vLCwcGBLVu2ULFiRTZv3kzVqlUBeO4fzIStl9l0zptSeTKxtGtVyuXPYnDFIiWQsBcinphMJhYtWsSoUaMIDQ1l+vTpDBkyhNSpUwOw2/0B3266xHP/YAY3tGFA/eKksZQTsCJxSNgLEQ+uXLlCz549OXbsGHZ2dixevJgiRYoA8NQvmPGu7ri63aN03sz80r0qZfNJb14kLgl7IeIgJCSE6dOnM3HiRDJmzIiLiwtffPFF5DIGOy/eZ+yWS/gGhDC0UQn61y8m0ymFISTshXhPp0+fpkePHly4cIF27doxb948cufODcCTV0F85+rO9gv3KZc/Myt6VKd03swGVyxSMgl7Id6Rv78/48ePZ9asWeTJk4fNmzfTsmXLyP3bL4T35l8GhjDMrgR96kpvXhhPwl6Id3Do0CF69eqFh4cHvXr1Yvr06ZE3FLn28CXTd11l318PqVAgCzPafETJPJn+/QmFSCQS9kLEgq+vLyNGjMDJyYmiRYuyf/9+GjRoAIDnYz8c911ji9s9MqSx5JumpehVu4isZyPMioS9EP/B1dWVfv368eDBA4YNG8aECRNInz493s8DmL//Or+e8SK1haJ3naL0qVOM7BnSGF2yEG+RsBciBj4+PgwaNIh169ZRvnz5yIujfF4EMm3LJdb8cReALh8Vpn/9YuTKlNbgioWImYS9EG/QWrNy5UqGDBnCq1evmDhxIt988w2vQuCHHX/xywlPQsM0bW0LMrBBcfJlTWd0yUL8Jwl7IaK4c+cOffr0YdeuXdSoUYMlS5aQv4gNPx66ifPRW/iHhPFZpfwMbmRD4RwZjC5XiFiL97BXShUFvgWyaK3bxLRNCHOitcbZ2ZmhQ4eitWbu3Ll07dmHFafu4vTrQXwDQmhePi9DGtlgk1tm2IikJ1bTBZRSS5VSPkqpS29sb6qUuqqU8lBKjQTQWt/UWveI2i66bUKYC29vb5o3b06vXr2wtbXl9NnzpK/8CfVnHWHG7qvYFs7GtoG1WNC5igS9SLJi27NfDvwIuPy9QSllASwAGgNewJ9KKVet9eX4LlKIhKC1ZtWqVQwcOJCgoCBmzp5Djmqf0vW3Wzx4EUit4lZ8ZVeCKoWyGV2qEHEWq7DXWh9RSlm/sbka4KG1vgmglFoLtATeK+yVUr2B3gCFChV6n6cQItZ8fHzo27cvmzZt4sOq1anXezwu3qnw3foXtoWzMad9JWoUy2F0mULEm7hc9ZEfuBvlay8gv1Iqh1JqMVBZKTUKILptb9JaO2mtbbXWtjlz5oxDWUL8uw0bNlC2bFm2bd9OjQ6DeNrgWzbeCKNWcSs29KvBr31rSNCLZCfeT9BqrZ8Aff9rmxCJ7enTpwxwcGDtmjVkLVgCq08n8LJgMXpWK8SXNQpTIFt6o0sUIsHEJey9gYJRvi4Qse29yT1oRUL5daMrvXr34sWzJ2Sp1Znyze3pUceG1h8WIOMHMgNZJH9xeZf/CdgopYoQHvIdgE5xKUbuQSvim9vNe3TtM5Dz+zaS2qowjUc5M7xTUxqUykWqVMro8oRINLEKe6XUGqAeYKWU8gLGaa2dlVIOwG7AAliqtXZPsEqFiCWtNcdvPGGy0zr2/jSBsJdPqNaqOz/NnkqlInI+SKRMsZ2N0zGG7TuAHfFVjAzjiLgwmTRb3LxZsMedP9b/yMuz28hZwJpfNv7Gxw3rGF2eEIYyq8FKGcYR7+uv+y8Yu/kSR48d4+Xuufg/9mbAwEFMn/oD6dPLiVchzCrshXhXr4JCcdx7jaW/X8f/xBp8jq6ncOHCLPv1IPXq1TO6PCHMhlmFvQzjiNjSWrPj4gMmbnPH6+ZVQvfPx8fzKj169GDOnDlkyiTLGggRlVmFvQzjiNi49diP77Zc4sg1H9Jf3cWjnUvIljUrW7Zs4dNPPzW6PCHMklmFvRD/JjAkjEWHbrDo8A3Uy0ekO7KQK+dP0apVK5ycnJArr4WImYS9SBIOXfVhnKs7no/9KPHiDCdXzUIBS5cupWvXriglc+aF+DdmFfYyZi/edO95AJO2XWbnpQcUSBeC9bkl7Nu7ndq1a+Pi4oK1tbXRJQqRJMRlIbR4p7XeqrXunSVLFqNLEQYLCTPhdOQGjWYf5sAVH5pm9sZjUV9OHt7L9OnTOXjwoAS9EO/ArHr2QgD86fmUMZsucfXhS+pYZyT0xC/85LKMChUqsGfPHipUqGB0iUIkORL2wmy8DAxh0rbLrD/tRf6s6RhSzsT8cd25desWI0aMYOLEiXzwwQdGlylEkmRWYS9j9inXmdtPGbLuPN7PAuhZsyC+x1bz9bczKFSoEIcPH6Z27dpGlyhEkiZj9sJQoWEmZu+9RtvFJwCY2iAbG8bZM3P6NLp27Yqbm5sEvRDxwKx69iJluf3Ej8Frz3P+7nM+q5yP/PeO8GWLb8icOTObN2+mZcuWRpcoRLIhYS8Sndaa3854Md7VHYtUislNCrFu9mgct23j448/ZtmyZeTOndvoMoVIViTsRaJ67h/M6E0X2XHxAdWLZOezXE8Y2smOp0+fMnfuXAYOHCgXSAmRACTsRaI57vGYr9a78fhVEF83LIrnbmc69p1F6dKl2bVrFxUrVjS6RCGSLbMKe5mNkzwFhYYxe881nH6/SRGrDMz+Xy4mfPUFZ8+epW/fvsyaNUvWnBcigZlV2Muql8mPh89LBq05z+X7L+hUrSAFH5+iY7MhpE2blk2bNtGqVSujSxQiRTCrqZci+dBas+KEJ83nHeXBi0DmtCyOx9rJ9O/Tm+rVq3PhwgUJeiESkVn17EXy8OhlEN9suMCBKz7ULZGT1vlf4NC+Mffv3+eHH35g+PDhWFhYGF2mECmKhL2IN1pr9v3lw6iNF3gRGMrYj0vguc+Fz3pNoUiRIhw7doxq1aoZXaYQKZKEvYgXf9x6ysw9V/nj1lNK5cnENLs8jB1iz4kTJ7C3t2f+/Plyq0AhDCRhL+LE7e5zZu29xpFrj8iZ6QMmtiyL9jjK541bALB69Wo6duxocJVCCLMKe5l6mXRcffCSWXuusufyQ7KlT83oZqVoWSY7I74egouLCzVq1GDVqlUUKVLE6FKFEJhZ2MvUS/N367Efc/ZeY+uFe2RMY8nQRiXoXsuaS+dOU7O6HZ6enowdO5bvvvsOS0uzensJkaLJT6OIFa9n/szf78FvZ71IY5GKvnWL0adOUTKmScWUKVOYOHEiBQoU4PDhw9SqVcvocoUQb5CwF//K50UgCw56sPqPOygUX9YoTL96xciVKS2enp588sUXHDt2jE6dOrFw4UJkeWohzJOEvYjWM79gFh++wS8nPAkN07S1LcjABsXJlzUdAKtWraJ///4ArFy5ks6dOxtZrhDiP0jYi9e8CAxhye+3WHr0Fn7BobSqlJ8hjWwonCMDAL6+vvTv35/Vq1fzv//9jxUrVshJWCGSAAl7EengVR+G/+rG41fBfFwuD181LoFN7n/mxh87dozOnTvj5eXFxIkTGTVqlJyEFSKJkJ9UQVBoGNN3XcX56C1K5cnEsq7VKF/gn7H30NBQJk2axOTJk7G2tubo0aN89NFHBlYshHhXEvYp3I1Hrxi05hzu917wZY3CjG5WmrSp/1m35saNG3zxxRecPHkSe3t75s2bR+bMmQ2sWAjxPiTsUyitNb+e8WLcFnc+SJ0Kpy4fYlc2z2v7V6xYwYABA7CwsGDt2rW0b9/ewIqFEHFhVmEvV9AmjheBIXy76RJb3e7xUdHsOLavTJ4saSP3P3v2jH79+rFu3Trq1KnDihUrKFSokIEVCyHiyqzWs9dab9Va95a52gnn7J1nNJv7Ozsu3md4k5Ks6vnRa0F/5MgRKlasyIYNG5gyZQoHDhyQoBciGTCrnr1IOGEmzeLDN5i99xp5s6RlfZ8afFg4W+T+oKAgxo4dy8yZMylevDjHjx+natWqBlYshIhPEvYpwAPfQIauO8+Jm0/4pEJepnxensxpU0fud3Nzo0uXLly8eJE+ffowc+ZMMmbMaGDFQoj4JmGfzO27/JDhv7kRGGJieusKtLUtgFIKgLCwMGbOnMnYsWPJkSMH27dvp1mzZgZXLIRICBL2yVRgSBhTd15h+XFPyuTNzPxOlSmW85/e+s2bN7G3t+fo0aO0bt2axYsXY2VlZWDFQoiEJGGfDF1/+JKBa85x5cFLuv+vCN98XJIPLMPnzmutcXZ2ZujQoVhYWLBixQo6d+4c2dsXQiRPEvbJiNaaNX/cZeI2dzKksWRZ16rUL5Urcv/Dhw/p2bMn27Zto0GDBixfvpyCBQsaWLEQIrFI2CcTPi8CGbnxIgeu+FCruBWz21UkV+Z/plRu3LiRPn368OrVKxwdHRk4cCCpUpnVzFshRAKSsE8GtrrdY+yWSwQEhzGuRRnsa1iTKlX4sIyvry+DBg3CxcWFKlWqsGLFCsqUKWNwxUKIxCZhn4Q98wtm7JZLbLtwn4oFszK7XcXXTsIeOnQIe3t7vL29GTt2LGPGjCFNmjQGViyEMIqEfRJ18IoPIzZc4JlfMMPsStC3bjEsLcKHZQIDAxk9ejRz5szBxsaGY8eOUb16dYMrFkIYScI+iXkVFMrkbZdZ++ddSubOxPJuVSmb75/lJc6dO8cXX3zB5cuX6d+/P9OnTydDhgwGViyEMAcS9knIyZtPGParG/eeB9C3bjGGNrZ5bUqlk5MTgwYNIkeOHOzcuZOmTZsaXLEQwlzEe9grpYoC3wJZtNZtIrZlABYCwcAhrfWq+H7d5CwwJIwZu6+y9NgtCmVPz699a/Bh4eyR+wMCAhgwYADLli2jSZMmrFq1ihw5chhYsRDC3MRq7p1SaqlSykcpdemN7U2VUleVUh5KqZEAWuubWusebzzF58BvWutewKfxUnkKccHrOc3n/Y7z0Vt8Ub0wOwfXfi3oPT09qVWrFsuWLWPMmDFs375dgl4I8ZbY9uyXAz8CLn9vUEpZAAuAxoAX8KdSylVrfTmaxxcALkZ8Hvbe1aYgIWEm5h/wYMFBD3Jm/IAVPapR2ybna2327NlDx44dCQ0NZcuWLXz6qfweFUJEL1Y9e631EeDpG5urAR4RPflgYC3QMoan8CI88GP9minZtYcv+WzhMebtv07LivnYPbTOa0FvMpmYMmUKTZs2JV++fJw+fVqCXgjxr+IyZp8fuBvlay+gulIqB/A9UFkpNUpr/QOwEfhRKdUc2BrdkymlegO9gRR7swyTSeN89BYz9lwl0weWLP7iQ5qWy/NaG19fX+zt7dmyZQsdO3bk559/ltk2Qoj/FO8naLXWT4C+b2zzA7r9x+OcACcAW1tbHd91mbv7vgF8vd6N4zeeYFcmN1M+L49Vxg9ea+Pu7s7nn3/OjRs3cHR0ZNCgQbKAmRAiVuIS9t5A1FW0CkRse28p9R60Oy/eZ+TGi4SEvb3m/N/Wr19P9+7dyZgxIwcOHKBOnToGVSuESIriMn7+J2CjlCqilEoDdABc41JMSrsHrV9QKCN+c6PfqrNY50jP9kG1aVe14GtBHxoaytdff0379u2pUKECZ8+elaAXQryzWPXslVJrgHqAlVLKCxintXZWSjkAuwELYKnW2j3BKk1mzt99zpC157j91J8B9YsxpFEJUlu8/rv34cOHtG/fnsOHDzNgwABmz54ta9sIId5LrMJea90xhu07gB3xVUxKGMb5+8bfc/ZeI1emD1jb6yOqF317XvzJkydp06YNT548wcXFhS5duhhQrRAiuTCraZDJfRjH65k/HZ1OMmP3VZqWy8POIXXeCnqtNYsWLaJOnTqkSZOGEydOSNALIeJM1sZJJK5u9/h200W0htntKvJZ5fxvnYT19/dnwIABLF++nGbNmrFy5UqyZctmUMVCiOREwj6BvQwMYdwWdzae86ZKoaw4tq9MoRzp32p35coV2rZti7u7O+PGjeO7776TO0kJIeKNWYV9chuzP3P7KUPWncf7WQCDG9owsEHxyDXno1q9ejW9e/cmXbp07Nq1Czs7OwOqFUIkZ2bVdUwuY/ahYSbm7L1G28Un0Bp+7VuDoY1LvBX0AQEB9OnTh86dO1OlShXOnz8vQS+ESBBm1bNPDu488WfIunOcvfOczyvnZ0LLsmRKm/qtdtevX6dt27a4ubkxcuRIJk2ahKWlfDuEEAlD0iUeXfL2pdPPJ9HA3A6VaFkpf7Tt1q9fT8+ePUmdOjXbt2+nWbNmiVuoECLFMathHKVUC6WUk6+vr9GlvLNrD1/SxfkUmdKmZseg2tEGfVBQEAMGDKB9+/aUK1eO8+fPS9ALIRKFWYV9Uh2z93zsR+clp0htkYpVPatTMPvbs21u3LhBzZo1WbhwIcOGDePw4cMULFgwmmcTQoj4J8M4ceT9PIDOS04RGmZifZ8aWFu9vdzwxo0b6datGxYWFri6utKiRQsDKhVCpGRm1bNPanxeBNL555O8CAxhRY/q2OTO9Nr+4OBghgwZQuvWrSlVqhRnz56VoBdCGMKswj4pjdk/8wvmC+dT+LwMYnm3qpTL//rQk6enJ7Vr12bu3LkMHjyY33//HWtra2OKFUKkeGYV9kllzP5FYAhfLv0Dzyf+LPnS9rUbgAO4urpSuXJlrl69yoYNG3B0dJTVKoUQhjKrsE8K/IND6b7sT648eMHiL6pQs7hV5L6wsDBGjBhBy5YtKVq0KGfPnuXzzz83sFohhAgnYf8OAkPC6O1yhrN3njG3Q2UalModuS8oKIgOHTowY8YM+vXrx7FjxyhatKiB1QohxD9kNk4shYSZGLDqLEc9HjOrbUWalc8bue/Fixd89tlnHDhwgFmzZvHVV18ZWKkQQrxNwj4WwkyaIevOs/+KD5NalaP1hwUi9z18+JBmzZpx4cIFucmIEMJsmVXYm+OqlyaT5psNF9h+4T6jm5Wiy0eFI/fdunULOzs7vL29cXV15eOPPzawUiGEiJlZjdmb22wcrTUTtrrz2xkvBje0oXedYpH73NzcqFmzJk+ePGH//v0S9EIIs2ZWYW9OtNZM23WVX07cpnedogxpZBO578iRI9SpUwdLS0uOHj1KjRo1DKxUCCH+m4R9DBYc9GDx4Rt0rl6IUR+XiryF4ObNm7GzsyNfvnwcP36cMmXKGFypEEL8Nwn7aDgfvcXMPdf4vHJ+JrUsFxn0S5YsoXXr1lSqVImjR4/KQmZCiCRDwv4Na/64w6Rtl/m4XB6mt6lAqlQKrTXff/89vXr1ws7Ojv3795MjRw6jSxVCiFgzq9k4RjKZNI77rjHvgAf1SuZkbofKWFqkwmQyMWTIEObPn0/nzp1ZtmwZqVO/fecpIYQwZ2YV9kZNvXwRGMLQteHz6NvZFmBiy3KksUxFcHAw9vb2rF27lqFDhzJz5kxSpZI/hoQQSY9ZJZcRUy9vPHpFqwXHOHztERNblmVa6wqkTW3Bq1evaNGiBWvXrmXq1KnMmjVLgl4IkWSZVc8+se3/6yFD1p4njWUqVvaszkdFw8fhHz16RPPmzTlz5gzOzs50797d4EqFECJuUmTYm0yaBQc9mL3vGmXzZeanLrbkz5oOgNu3b2NnZ8edO3fYtGkTn376qcHVCiFE3KW4sPcLCuXr9W7scn9Aq0r5mBoxbANw+fJlGjdujL+/P3v37qVWrVoGVyuEEPEjRYX97Sd+9HY5w3Wfl4xpXpoetYpEzqE/e/YsTZo0wcLCgiNHjlC+fHmDqxVCiPiTYsL+yLVHDFxzDqXApXt1atn8c9ORo0eP0rx5c7Jly8a+ffswp4XYhBAiPiT76SVaa5yO3KDrsj/ImyUtrgNqvRb0u3fvxs7Ojjx58vD7779L0AshkqVk3bMPCA7jmw0XcHW7R/PyeZnRtgLp0/zzX964cSMdOnSgTJky7Nmzh1y5chlYrRBCJJxkG/Zez/zp7XKGvx68YETTkvSrWyxyfB7gl19+oXv37lSvXp3t27eTLVs2A6sVQoiEZVZhH19X0J648YQBq88SEmZiadeq1C/5eo/9xx9/ZODAgTRs2JDNmzeTMWPGOL2eEEKYO7Mas4/rFbRaa5Yfu8UXzqfIniENrg613gr6H374gYEDB9KyZUu2bdsmQS+ESBHMqmcfV17PAvhh5xUalMrF7HYVyZT2nwXLtNaMGjWKadOmyYJmQogUJ1mFfcHs6dnU/3+UypOJVKn+GZ83mUwMGDCAxYsX07dvXxYsWCDr3AghUpRkl3hl8mV+LehDQ0Oxt7dn8eLFjBgxgoULF0rQCyFSnGTVs39TYGAgHTp0YMuWLUyZMoVRo0YZXZIQQhgi2Ya9n58frVq1Yt++fcyfPx8HBwejSxJCCMMky7B//vw5zZo149SpUyxfvhx7e3ujSxJCCEMlu7D38fHBzs6Oy5cvs379elq3bm10SUIIYbhkFfb379+nfv363Llzh61bt9KkSROjSxJCCLOQrKalZM2alVKlSrF7924JeiGEiCJZ9ezTpUvH5s2bjS5DCCHMTrLq2QshhIiehL0QQqQAiRL2SqkySqn1SqlFSqk2ifGaQggh/vHeYa+UWqqU8lFKXXpje1Ol1FWllIdSamTE5o+B+VrrfsCXcahXCCHEe4hLz3450DTqBqWUBbCA8HAvA3RUSpUBVgAdlFIzgBxxeE0hhBDv4b3DXmt9BHj6xuZqgIfW+qbWOhhYC7TUWvtorQcAI4HH0T2fUqq3Uuq0Uur0o0eP3rcsIYQQ0YjvMfv8wN0oX3sB+ZVS1kopJ8AFmBHdA7XWTlprW621bc6cOeO5LCGESNkSZZ691toT6J0YryWEEOJt8R323kDBKF8XiNj2Ts6cOfNYKXU7DnVYEcNwkZmQ+uJG6osbqS9uzLm+wjHtiO+w/xOwUUoVITzkOwCd3vVJtNZxGsdRSp3WWtvG5TkSktQXN1Jf3Eh9cWPu9cUkLlMv1wAngJJKKS+lVA+tdSjgAOwG/gLWa63d46dUIYQQ7+u9e/Za644xbN8B7HjvioQQQsS75LpcgpPRBfwHqS9upL64kfrixtzri5bSWhtdgxBCiASWXHv2QgghopCwF0KIFCDJhn0MC65F3f+BUmpdxP5TSinrRKytoFLqoFLqslLKXSk1OJo29ZRSvkqp8xEf3yVWfVFq8FRKXYx4/dPR7FdKqXkRx/CCUqpKItZWMsqxOa+UeqGUGvJGm0Q9htEt/qeUyq6U2quUuh7xb7YYHmsf0ea6Uso+EeuboZS6EvH926SUyhrDY//1vZCA9Y1XSnlH+R42i+Gx//rznoD1rYtSm6dS6nwMj03w4xdnWusk9wFYADeAokAawA0o80ab/sDiiM87AOsSsb68QJWIzzMB16Kprx6wzeDj6AlY/cv+ZsBOQAEfAacM/H4/AAobeQyBOkAV4FKUbdOBkRGfjwSmRfO47MDNiH+zRXyeLZHqswMsIz6fFl19sXkvJGB944Fhsfj+/+vPe0LV98b+WcB3Rh2/uH4k1Z59tAuuvdGmJfBLxOe/AQ2VUioxitNa39dan434/CXh1xzkT4zXjmctARcd7iSQVSmV14A6GgI3tNZxuao6znT0i/9FfZ/9ArSK5qFNgL1a66da62fAXt5YMTah6tNa79Hh178AnCT8qnZDxHD8YiM2P+9x9m/1RWRHO2BNfL9uYkmqYR/tgmsxtYl4s/tiwPLKEcNHlYFT0eyuoZRyU0rtVEqVTdzKANDAHqXUGaVUdGsXxeY4J4YOxPxDZvQxzK21vh/x+QMgdzRtzOU4dif8L7Xo/Nd7ISE5RAwzLY1hGMwcjl9t4KHW+noM+408frGSVMM+SVBKZQQ2AEO01i/e2H2W8GGJisB8YHMilwdQS2tdhfD7DwxQStUxoIZ/pZRKA3wK/BrNbnM4hpF0+N/zZjmXWSn1LRAKrIqhiVHvhUVAMaAScJ/woRJz1JF/79Wb/c9SUg372Cy4FtlGKWUJZAGeJEp14a+ZmvCgX6W13vjmfq31C631q4jPdwCplVJWiVVfxOt6R/zrA2wi/M/lqOJlYbs4+hg4q7V++OYOcziGwMO/h7Yi/vWJpo2hx1Ep1RX4BOgc8QvpLbF4LyQIrfVDrXWY1toE/BzD6xp9/CyBz4F1MbUx6vi9i6Qa9pELrkX0/DoArm+0cQX+nvXQBjgQ0xs9vkWM7zkDf2mtZ8fQJs/f5xCUUtUI/14k5i+jDEqpTH9/TviJvEtvNHMFvoyYlfMR4BtlyCKxxNijMvoYRoj6PrMHtkTTZjdgp5TKFjFMYRexLcEppZoCI4BPtdb+MbSJzXshoeqLeg7osxheNzY/7wmpEXBFa+0V3U4jj987MfoM8ft+ED5T5BrhZ+m/jdg2kfA3NUBawv/09wD+AIomYm21CP9z/gJwPuKjGdAX6BvRxgFwJ3xmwUmgZiIfv6IRr+0WUcffxzBqjYrw20zeAC4CtolcYwbCwztLlG2GHUPCf+ncB0IIHzfuQfh5oP3AdWAfkD2irS2wJMpju0e8Fz2AbolYnwfh491/vw//nqGWD9jxb++FRKpvRcR76wLhAZ73zfoivn7r5z0x6ovYvvzv91yUtol+/OL6IcslCCFECpBUh3GEEEK8Awl7IYRIASTshRAiBZCwF0KIFEDCXgghUgAJeyGESAEk7IUQIgX4P39H/0nZ5/DAAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = 4\n",
    "plt.plot(10**(y_predicted[i,:]))\n",
    "plt.plot(10**(y_testset[i,:] * (max_y - min_y) + min_y), c = 'black')\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "1e25d925-9e58-423c-99f4-645e717c76e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f51ed230bb0>]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.sqrt(np.sum((y_testset * (max_y - min_y) + min_y - y_predicted)**2, axis = 0) / len(y_testset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80170f62-ae06-46de-9a87-5460372520c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
